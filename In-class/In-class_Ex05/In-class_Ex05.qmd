---
title: "Emerging Hot Spot Analysis"
author: "Federico Jose Rodriguez"
date: "Sep 30 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
---

In this exercise, we perform emerging hotspot analysis on the Hunan GDPPC data.

# Importing and Loading packages

This exercise will make use of the following R packages:

-   **sf -** for importing, managing and processing vector-based geospatial data

-   **tidyverse -** collection of packages for performing data importation, wrangling and visualization

-   **tmap -** for plotting cartographic quality maps

-   **sfdep** - functions to create spatial weights, autocorrelation statistics for sf objects

-   **plotly** - for producing interactive visualizations

The code chunk below uses `p_load()` of **pacman** package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse, zoo, Kendall)
```

We also define a random seed value for repeatability of any simulation results.

```{r}
set.seed(1234)
```

# Data Import and Preparation

For this exercise, we will make use of the Hunan administrative map shapefile and economic indicators csv that we have used in the hands-on exercises.

The code chunk below uses `st_read()` of the **sf** package to load the Hunan shapefile into an R object.

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

We then use the code chunk below to load the csv file with the GDP per capita of Hunan province into R using `read_csv()`

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

We can inspect this last object with the code chunk below

```{r}
GDPPC
```

# Step 1: Create a Time Series Cube

The first step for EHSA is to create the space-time cube. We use `spacetime()` of **sfdep** in the codechunk below to perform this.

```{r}
GDPPC_st <- spacetime(GDPPC, hunan,
                      .loc_col = "County",
                      .time_col = "Year")
```

Note that the `.time_col` argument which specifies the time field should not be taking a column in full time format. It needs to be a discrete numeric values so converting it into an integer value.

We use `is_space_time_cube()` to verify that the new object is indeed a space-time cube object.

```{r}
is_spacetime_cube(GDPPC_st)
```

# Step 2: Calculate Gi\* Statistics

## Deriving the spatial weights

We use the code chunk below to identify the neighbors and to derive inverse distance weights. The `alpha` argument of `st_inverse_distance()` determines the level of distance decay.

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(
    st_contiguity(geometry)),
    wt=st_inverse_distance(nb,
                  geometry,
                  scale = 1,
                  alpha = 1),
    .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

## Computing Gi\*

We use the following chunk to calculate the local Gi\* for each location. We do this using `local_gstar_perm()` of **sfdep** package. We then use `unnest()` to unnest the `gi_star` column of the newly created data frame.

```{r}
gi_stars <- GDPPC_nb %>%
  group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt)) %>%
  tidyr::unnest(gi_star)
```

# Step 3: Evaluate Hotspots and Coldspots using Mann Kendall Test

The Mann Kendall test is used to check for signs of monotonicity.

## Performing Mann-Kendall Test on Gi of one location

The code chunk below evaluates Changsha county for trends

```{r}
cbg <- gi_stars %>%
  ungroup() %>%
  filter(County == "Changsha") %>%
  select(County, Year, gi_star)
```

We then plot the result using **ggplot2** package

```{r}
ggplot(data = cbg,
       aes(x = Year,
           y = gi_star)) +
  geom_line() +
  theme_light()
```

We can convert this into an interactive plot by passing the chart into `ggplotly()`

```{r}
p <- ggplot(data = cbg,
       aes(x = Year,
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

We can conduct the Mann-Kendall test on Changsha by using MannKendall() from the Kendall package in the code chunk below

-   $H_0$ - No monotonic trend on the GDPPC value of Changsha

-   $H_1$ - Monotonic trend is present on the GDPPC value of Changsha

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

The output gives the tau value, the significance level (sl) or p-value.

## Mann-Kendall Test Dataframe

The code chunk below runs the Mann-Kendall test on all counties in Hunan.

```{r}
ehsa <- gi_stars %>%
  group_by(County) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
head(ehsa)
```

Not all the counties are showing statistically significant result (i.e., `sl` \< 0.05)

We can examine some of the counties to observe how their Gi\* values change overtime.

The code chunk below

```{r}
countycheck <- gi_stars %>%
  ungroup() %>%
  filter(County == "Changsha") %>%
  select(County, Year, gi_star)

p <- ggplot(data = countycheck,
       aes(x = Year,
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

# Step 4: Performing Emerging Hotspot Analysis

We perform EHSA using the code below.

```{r}
ehsa <- emerging_hotspot_analysis(
  x = GDPPC_st,
  .var = "GDPPC",
  k = 1,
  nsim = 99
)
```

We can visualize the different classifications below

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

## Visualising EHSA

To visualize, we first hoing the ehsa results with the sf dataframe

```{r}
hunan_ehsa <- hunan %>%
  left_join(ehsa,
            by = join_by(County == location))
```

We can then use tmap package to create a filled map based on the classification filtered for statistically significant results.

```{r}
ehsa_sig <- hunan_ehsa %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(hunan_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
  tm_shape(ehsa_sig) +
  tm_fill("classification") +
  tm_borders(alpha = 0.4)
```
