---
title: "Geographic Segmentation with Spatially Constrained Clustering Techniques"
author: "Federico Jose Rodriguez"
date: "Sep 27 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
---

In this hands-on exercise, we apply hierarchical cluster analysis and spatially constrained cluster analysis to delineate homogeneous regions based on geographically referenced data.

This exercise is based on Chapter 12 of Dr Kam's online book which can be accessed [here](https://r4gdsa.netlify.app/ "R for Geospatial Data Science and Analytics by Dr Kam").

# Getting Started

## Analytical Question

In the development of spatial policy and for business, it is often important to segregate homogenous regions using multivariate data. We apply techniques in the study of Shan State in Myanmar by using various indicators.

## Data Sources

Data for this exercise are based on information for Myanmar and for its Shan state:

-   Myanmar township boundary data in ESRI shapefile format (polygon)

-   Shan state ICT indicators for 2014 contained in a csv file

## Installing and launching R packages

This exercise will make use of thirteen R packages:

-   **sf, rgdal, spdep -** for spatial data handling

-   **tidyverse -** collection of packages for performing data importation, wrangling and visualization

-   **tmap -** for plotting cartographic quality maps

-   **coorplot, ggpubr, heatmaply** - packages for multivariate data visualization and analysis

-   **cluster, ClustGeo** - packages for performing cluster analysis

The code chunk below uses `p_load()` of **pacman** package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

We also define a random seed value for repeatability where of any randmoized results.

```{r}
set.seed(1234)
```

# Data Import and Preparation

## Data Loading - Shan state boundary

The code chunk below uses `st_read()` of the **sf** package to load the Myanmar township boundary shapefile into an R object. The code chunk includes a pipeline to already filter to the Shan state and include only the relevant columns.

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))
```

We can inspect the contents of shan_sf using the code chunk below

```{r}
shan_sf
```

The sf dataframe conforms to the tidy framework. Given this, we can also use `glimpse()` to reveal the fields' data types.

```{r}
glimpse(shan_sf)
```

## Data Loading - Shan state 2014 indicators (aspatial)

The code chunk below uses `read_csv()` to load the contents of the csv file into an object `ict`

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
```

We can use `head()` to check the first 6 elements of the object,

```{r}
head(ict)
```

and `summary()` to display summary statistics of the numeric columns.

```{r}
summary(ict)
```

The dataset contains 11 fields with 55 observations. The numeric fields give the total number of households in each township, and the number of households with the corresponding technology or appliance. (e.g., television, internet connection, etc)

## Deriving new indicator variables

Using the numeric fields directly will be highly biased as it depends on the number of households in the township. (i.e., townships with higher total households are likely to have higher values for all other columns) To overcome this problem, we can derive the penetration rates (PR) of each of the items by computing the number of households with that item per 1000 households. We accomplish this using `mutate()` from dplyr package in the code below.

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

We can use `summary()` again to display summary statistics on the 6 new columns.

```{r}
summary(ict_derived[c(12:17)])
```

## Joining spatial and aspatial data

For later map preparations, we need to combine the two datasets (geospatial `shan_sf`, aspatial `ict_derived`) into a single object. We do this using the `left_join()` function of the **dplyr** package. Both datasets have a common field `TS_PCODE` which will be treated as the unique identifier or joining key.

```{r}
shan_sf <- left_join(shan_sf, 
                     ict_derived, by=c("TS_PCODE"="TS_PCODE"))
  
write_rds(shan_sf, "data/rds/shan_sf.rds")
```

The code includes creation of a new rds file so we can use the following code in the future to read this joined dataset without performing all the steps above.

```{r}
shan_sf <- read_rds("data/rds/shan_sf.rds")
```

# Exploratory Data Analysis (EDA)

## EDA using statistical graphics

We can use histograms to visualize the overall distribution of data valuesâ€“ e.g., the shape or skewness. The code chunk below produces on for the field `RADIO_PR`.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20,  color="black", fill="light blue") +
  xlab("Radio Penetration Rate, per K-HH") +
  ylab("No. of Townships")
```

We can also use boxplots for identifying the median, quartiles, and outliers in the data.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")+
  xlab("Radio Penetration Rate, per K-HH")
```

We can create multiple histograms side by side by creating objects for each variable's histogram, and then laying them out in a grid with `ggarange()` of the **ggpubr** package.

::: panel-tabset
#### Creation of Histogram objects

```{r}
radio <- ggplot(data=ict_derived, aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20,color="black", fill="light blue") +
  xlab("Radio PR") +
  ylab("No. of Townships")

tv <- ggplot(data=ict_derived, aes(x= `TV_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  xlab("TV PR") +
  ylab("No. of Townships")

llphone <- ggplot(data=ict_derived, aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  xlab("Landline Phone PR") +
  ylab("No. of Townships")

mphone <- ggplot(data=ict_derived, aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  xlab("Mobile Phone PR") +
  ylab("No. of Townships")

computer <- ggplot(data=ict_derived, aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  xlab("Computer PR") +
  ylab("No. of Townships")

internet <- ggplot(data=ict_derived, aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  xlab("Internet PR") +
  ylab("No. of Townships")
```

#### Grid display of multiple histograms

```{r}
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, nrow = 2)
```
:::

## EDA using choropleth map

The code chunk below prepares a choropleth map of the Shan state and the Radio penetration rate using `qtm()`

```{r}
qtm(shan_sf, "RADIO_PR")
```

The above map is based on the derived penetration rate. We can use choropleth maps to go back to the earliest statement that using the raw variables are likely to be biased on the number of households. We can use the code chunk below to look at them side by side. We use the approach of passing multiple arguments instead of using `tmap_arrange()`

```{r}
tm_shape(shan_sf) + 
  tm_fill(col = c("TT_HOUSEHOLDS", "RADIO"),
          n = 5,style = "jenks", 
          title = c("Total households","Number Radio")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(legend.position = c("right", "top"), bg.color = "grey90")
```

The above map shows that townships with high number of households with radios, also are towns with the high number of households. We can produce a second map to see if the penetration rate and the total number of households are correlated.

```{r}
tm_shape(shan_sf) + 
  tm_fill(col = c("TT_HOUSEHOLDS", "RADIO_PR"),
          n = 5,style = "jenks", 
          title = c("Total households","Radio Penetration")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(legend.position = c("right", "top"), bg.color = "grey90")
```

The second pair of maps shows no strong correlation between townships having high number of households and having high radio penetration rates.

Finally, we can show the six derived variables visually using a similar approach in the code chunk below. The viewer needs to be mindful of the data classes. While the darker the shading means a higher value for that derived variable, the range of values are different between pairs of variables.

```{r}
tm_shape(shan_sf) + 
  tm_fill(col = c("RADIO_PR", "TV_PR", "LLPHONE_PR",
                  "MPHONE_PR", "COMPUTER_PR", "INTERNET_PR"),
          n = 5,style = "jenks") + 
  tm_borders(alpha = 0.5) +
  tm_layout(legend.position = c("right", "top"), bg.color = "grey90")
```

# Correlation Analysis

Before we perform cluster analysis, it is important to check that the cluster variables are not highly correlated.

In the code chunk below, we use `corrplot.mixed()` from the **corrplot** package to visualize the correlation between vairables

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

The plot above shows that `COMPUTER_PR` and `INTERNET_PR` are highly correlated (coefficient of 0.87, shown as a very dark blue oval) This suggests that only one of these variables should be used in cluster analysis.

# Hierarchical Cluster Analysis

In this section, we perform hierarchical cluster analysis which is done in a few steps steps

## Selecting and extracting cluster variables

The code chunk below will be used to extract the clustering variables from the `shan_sf` dataframe. We have chosen to include `COMPUTER_PR` rather than `INTERNET_PR` for the cluster analysis

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

The next step is to change the row names or indices to the township names rather than the row numbers

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

We see that the row numbers have been replaced with the township names, however, the township names are now duplicated. We solve this by using the code chunk below

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

## Data standardisation

Multiple variables will usually have different range of values. If we use them as is for cluster analysis, then the clustering will be biased towards variables with larger values. It is useful to standardise the clustering variables to reduce the risk of this occuring.

### Min-max standardisation

The code chunk below uses `normalize()` of **heatmaply** package to standardise the clustering variables using min-max method. We then use `summary()` to show that the ranges of each variable have transformed to \[0,1\]

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

### Z-score standardisation

We can perform z-score standardisation by using `scale()` of **Base R**. We use `describe()` of **psych** package to display some statistics of the standardised columns. These show that each of the variables have been transformed to have a mean of 1 and a standard deviation of 1

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

### Visualising the standardised clustering variables

Aside from viewing the statistics of the standardised variables, it is also good practice to visualise their distribution graphically.

The code chunk below produces histograms to show the `RADIO_PR` field without and with standardisation

```{r}
r <- ggplot(data=ict_derived, aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

Alternatively, we can view these as density plots using the code below.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

## Computing the proximity matrix

There are many packages in R that provide functions to compute for the distance matrix. We will use `dist()` for our case.

This function supports six distance calculations: **euclidean (*default*), maximum, manhattan, canberra, binary and minkowski**. The code chunk below is used to compute the proximity matrix using the *euclidean* method.

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
```

The code chunk below displays the content of `proxmat` for inspection

```{r}
proxmat
```

## Computing hierarchical clustering

There are several packages in R that can perform hierarchical clustering. In this exercise, we use `hclust()` of **R stats**.

`hlcust()` employs agglomeration method to compute clusters. Eight clustering algorithms are supported: (1) ward.D, (2) ward.D2, (3) single, (4) complete, (5) average(UPGMA), (6) mcquitty(WPGMA), (7) median(WPGMC), and (8) centroid (UPGMC)

The code chunk below performs hierarchical clustering using ward.D method. The output is stored in an object of class hclust which describes the tree produced by the clustering process.

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

Once ran, we can plot the resulting object as tree by using `plot()`

```{r}
plot(hclust_ward, cex = 0.6)
```

## Selecting the optimal clustering algorithm

A challenge in performing hierarchical clustering is identifying strong clustering structures. This can be solved by using `agnes()` of the **cluster** package. The function acts like `hclust()`, but can also get the agglomerative coefficientsâ€“ or the measure of the strength of the clustering structure. (with a value of 1 indicating a strong structure)

The code chunk below computes the agglomerative coefficient of all algorithms.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

The output above shows that Ward's method provides the best coefficient, and therefore the strongest cluster, among the four methods assessed. We will then focus on this method in succeeding analyses.

## Determining optimal clusters

Another challenge in cluster analysis is determining the number of clusters to retain. For this, there are three commonly used methods to determine the number of clusters:

1.  Elbow method

2.  Average Silhouette method

3.  Gap Statistic Method

### Gap statistic method

The gap statistic compares intra-cluster variation for different values of k with their expected values under null reference data distribution. The optimal cluster will be the one that maximizes the gap statisticâ€“ meaning that the optimal cluster is the farthest from representing a random distribution of points.

We use the code chunk below to compute the gap statistic using `clusGap()` of cluster package. One of the arguments, `FUN`, use the `hcut` function which comes from **factoextra** package indicating that hierarchical clustering is used.

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

We can then visualize the plot by using `fviz_gap_stat()` of **factoextra** package.

```{r}
fviz_gap_stat(gap_stat)
```

While the chart above shows that k=1 cluster(s) gives the highest gap statistic, it is not logical. Aside from k=1, we see that k=6 clusters gives the largest statistic and would be the best number of clusters to pick.

In addition to the above, the **NbClust** package provides 30 indices for determining the optimal number of clusters.

## Interpreting the dendograms

Each leaf in the dendogram represents one observation. (townships in our example) Moving up the dendogram, leaves are combined into similar ones using branches. The heights of the fusion indicates the dissimilarity between the two observations, i.e., the higher the height the larger the difference between the two observations. The horizontal axis does not provide any information on the similarity or dissimilarity of pairs of observations.

The dendogram can be redrawn with a border around selected (number of) clusters by using `rect.hclust()` of **R stat**. The `border` argument specifies the border colors for the rectangles.

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

## Visually-driven hierarchical clustering analysis

In this section, we use **heatmaply** package to perform visually driven hierarchical clustering analysis. With this package, we can build interactive or static cluster heatmaps.

### Transforming dataframe into a matrix

To create a heatmap, the data needs to be in a matrix. We convert the data frame to this format using the code below

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

### Plotting interactive cluster heatmap using `heatmaply()`

The code chunk below uses `heatmaply()` of **heatmap** package to produce an interactive cluster heatmap

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

## Mapping the clusters formed

We can use `cutree()` for **R base** to derive a 6-cluster model. The code below outputs a list object.

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

In order to visualize the clusters, the list first needs to be appended to the `shan_sf` simple feature object.

This is accomplished in the following code chunk in three steps:

1.  The object is converted to a matrix
2.  `cbind()` is used to append the matrix object onto `shan_sf` as a new sf object
3.  `rename()` is then used to rename the appended field `as.matrix.groups` into `CLUSTER`

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

We then use `qtm()` of **tmap** package to produce a quick map of Shan state with the clusters

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

The plot shows that the resulting clusters are quite fragmented. This is a limitation of performing non-spatial clustering algorithm.

# Spatially Constrained Clustering: SKATER Approach

In this section we use `skater()` of **spdep** package to derive spatially constrained clusters.

## Converting into SpatialPolygons DataFrame

The `skater()` function can only support sp objects, so conversion into the appropriate type is required

The code chunk performs the conversion using `as_Spatial()` of **sf** package.

```{r}
shan_sp <- as_Spatial(shan_sf)
```

```{r}
class(shan_sp)
```

## Computing Neighbor list

We then use `poly2nd()` of **spdep** package to generate the neighbor list from the polygon dataframe.

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

We can visualize the neighbor lists using the chunks below. We first plot the boundaries based on `shan_sf`. We follow this with the neighbor list object `shan.nd` and use the shape centroids to represent nodes for the graph representation. The `add=TRUE` argument specifies plotting the network on top of the plot of the boundaries.

```{r}
coords <- st_coordinates(
  st_centroid(st_geometry(shan_sf)))
```

```{r}
plot(st_geometry(shan_sf), 
     border=grey(.5))
plot(shan.nb,
     coords, 
     col="blue", 
     add=TRUE)
```

## Computing minimum spanning tree

### Calculating edge costs

We then use `nbcosts()` of **spdep** package to compute the cost of each edge. The cost will be the "distance" between nodes.

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

The function computes the dissimilarity between pairs of neighbors across the value of the five variables.

We then incorporate these costs into a weights object which is equivalent to converting the neighbor list into a list weights object by specifying `lcosts` as weights. We achieve this by using nb2listw() of spdep package in the code chunk below. We specify `style="B"` to make sure the cost values are not row-standardised.

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

### Computing the minimum spanning tree

We use `mstree()` of **spdep** package to compute the minimal spanning tree.

```{r}
shan.mst <- mstree(shan.w)
```

```{r}
dim(shan.mst)
```

Note that the resulting minimum spanning tree has a dimension n-1 (55-1=54) as it consists of n-1 links to connect the nodes that allow traversal from any any pair of nodes.

We can display the content of the spanning tree using `head()` where we see the first six links and their respective weight

```{r}
head(shan.mst)
```

The `plot` method for MST includes a way to show the observation (numbers/index) in addition to the connecting edge. The resulting plot will be similar to the neighbor list's except there will only be at most two edges connecting to any node

```{r}
plot(st_geometry(shan_sf), 
                 border=gray(.5))
plot.mst(shan.mst, 
         coords, 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

## Computing spatially constrained clusters using SKATER method

We can use `skater()` of **spdep** package to compute spatially constrained clusters as in the code chink below

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

The function requires three mandatory arguments:

1.  The first two columes of the MST (the edge, or pair of nodes)
2.  the data matrix
3.  the number of cuts which is equal to the number of clusters minus 1

The resulting object is of class skater. We can examine the contents using the code chunk below

```{r}
str(clust6)
```

The first field `groups` contains the label of the cluster membership for that observation. The next object contains details of the different clusters which include the nodes and edges in that cluster.

We can check the cluster assignments using the following code chunk

```{r}
ccs6 <- clust6$groups
ccs6
```

We can check how many observations are in each cluster using the `table()` function.

```{r}
table(ccs6)
```

We can also plot the pruned tree showing the six clusters. Note that two of the clusters have only one township each so they will not produce a colored edge in the plot (Group 5 with node 23, group 6 with node 3)

```{r}
plot(st_geometry(shan_sf), 
     border=gray(.5))
plot(clust6, 
     coords, 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink", "black"),
     cex.circles=0.005, 
     add=TRUE)
```

## Visualizing the clusters in a choropleth map

The code chunk below can be used to plot the clusters derived from SKATER method using tmap package

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

For easier comparison, we can put the clusters generated using hierarchical clustering and spatially constrained hierarchical clustering side by side.

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```

# Spatially Constrained Clustering: ClustGeo Method

In this section, we use functions from the **ClustGeo** package to perform non-spatially constrained and spatially constrained hierarchical cluster analysis

## Ward-like hierarchical clustering: ClustGeo

The package contains a function `hclustgeo()` to perform Ward-like hierarchical clustering similar to `hclust()`

The function only requires the dissimilarity matrix to perform non-spatially constrained clustering as in the code chunk below. Note that the dissimilarity matrix needs to be of class dist, an object generated by `dist()`

```{r}
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

## Mapping the clusters formed

We can plot the generated clusters in a shaded map using the code chunks below

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))
```

```{r}
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

```{r}
qtm(shan_sf_ngeo_cluster, "CLUSTER")
```

As it is not spatially-constrained, the resulting clusters are quite fragmented

## Spatially-constrained hierarchical clustering

Before performing spatially-constrained hierarchical clustering, we use `st_distance()` of **sf** package to generate a spatial distance matrix. We use the code chunk below which also include `as.dist()` to convert the dataframe into a matrix.

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
```

Next, we determine a suitable value for the mixing parameter alpha using `choicealpha()`

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```

Based on the charts above, we select alpha = 0.3 as the input to `hclustgeo()`

```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.3)
```

We then use `cutree()` to derive the cluster object

```{r}
groups <- as.factor(cutree(clustG, k=6))
```

We then join the group list back into the `shan_sf` polygon feature dataframe using the code chunk below

```{r}
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

We can then use `qtm()` to map the spatially constrained clusters.

```{r}
qtm(shan_sf_Gcluster, "CLUSTER")
```

# Visual Interpretation of Clusters

## Visualising individual clustering variable

The code chunk below reveals the distribution of a clustering variable (`RADIO_PR`) based on one of the clustering approaches (non-spatially constrained ClustGeo)

```{r}
ggplot(data = shan_sf_ngeo_cluster,
       aes(x = CLUSTER, y = RADIO_PR)) +
  geom_boxplot()
```

The boxplot reveals that cluster 3 (in this method) has the highest mean radio penetration rate while clusters 4,5, and 6 have the lowest

## Multi-variate visualisation

Parallel coordinate plots can be used to reveal insights on clustering variables by clusters effectively, We can perform this using `ggparcoord()` of **GGally** package.

```{r}
ggparcoord(data = shan_sf_ngeo_cluster, 
           columns = c(17:21), 
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster") +
  facet_grid(~ CLUSTER) + 
  theme(axis.text.x = element_text(angle = 30))
```

The `scale` argument of `ggparcorr()` provides several options to scale the variables:

-   std - univariately, scaled as Z value

-   robust - univariately, subtract median and divide by mean absolute deviation

-   uniminmax - univariately, scale to \[0-1\]

-   globalminmax - no scaling is done, range is defined by global min and max values

-   center - use uniminmax to standardize vertical height, then center at a specified value (`scaleSummary` parameter)

-   centerObs - use uniminmax to standardize vertical height, then center at a specified observation's value (`centerObsID`parameter)

We can also compare the clusters by computing for statistics that will complement the visual interpretation. The code chunk below uses `group_by()` and `summarise()` of **dplyr** to derive the mean values for the clustering variables by cluster

```{r}
shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(CLUSTER) %>%
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```
