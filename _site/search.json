[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "",
    "text": "Welcome to my website for my work in the course ISSS 626 - Geospatial Analytics and Applications during the August term of 2024 under Dr Kam Tin Seong.\nThe course covers theory and methods of geospatial analysis and the tools in R to implement such analyses."
  },
  {
    "objectID": "index.html#hands-on-exercises",
    "href": "index.html#hands-on-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nHands-on exercises are assigned before each lesson to give us hands-on experience in using R on the geospatial analysis methods and functions we learned. We are given step-by-step instructions and explanations (through Dr Kam’s online book) which complement our pre-class reading, and prepares us for the in-class exercises.\n\nGeospatial Data Wrangling with R (for Session 1, 26 Aug ’24)\nChoropleth Mapping with R (for Session 1, 26 Aug ’24)\n1st Order Spatial Point Patterns Analysis Methods (for Session 2, 2 Sept ’24)\n2nd Order Spatial Point Patterns Analysis Methods (for Session 2, 2 Sept ’24)\nNetwork Constrained Spatial Point Pattern Analysis (for Session 3, 9 Sept 2024)\nSpatial Weights and Applications (for Session 4, 16 Sept 2024)\nGlobal Measures of Spatial Autocorrelation (for Session 5, 23 Sept 2024)\nLocal Measures of Spatial Autocorrelation (for Session 5, 23 Sept 2024)\nGeographical Segmentation with Spatially Constrained Clustering Techniques (for Session 6, 30 Sept 2024)\nGeographically Weighted Regression Models (for Session 7, 14 Oct 2024)\nGeographically Weighted Predictive Models (for Session 8, 21 Oct 2024)"
  },
  {
    "objectID": "index.html#in-class-exercises",
    "href": "index.html#in-class-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nIn each session, we go through a hands-on exercise to revise the readings for the lesson. The objective of these is to further reinforce the concepts and tools learned in the readings and in the take-home exercise. These will also go further into the analysis and interpretation of results.\n\nIntroduction to Geospatial Analytics (26 Aug ’24)\nSpatial Point Pattern Analysis - Data Load for Take-home Exercise 1 (2 Sept ’24)\nAdvanced Spatial Point Patterns Analysis (9 Sept ’24)\nSpatial Autocorrelation (23 Sept’24)\nEmerging Hot Spot Analysis (30 Sept ’24)\nGeographically Weighted Regression (14 Oct 2024)"
  },
  {
    "objectID": "index.html#take-home-exercises",
    "href": "index.html#take-home-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "Take-Home Exercises",
    "text": "Take-Home Exercises\nTake-home exercises are where we students are able to apply the methods and techniques learned in class in real-world cases– using real-world data and aim to generate real insights.\nFor each of these, we are given a specific problem, objectives and a base data set. Each student will work on the problem independently, guided only by the previous exercises and lessons.\n\nGeospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region\nDiscovering the Impact of COVID-19 on Thai tourism economy"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "",
    "text": "In this exercise, we apply spatial point pattern analysis to analyse the distribution of road traffic accidents in the Bangkok Metropolitan Region. We demonstrate how kernel density estimation can be used to visualize hotspots in a network constrained and non-network constrained context. We use different approaches to display charts side by side, especially to compare hotspots across different time dimensions or different conditions. Finally, we demonstrate how K- and G-functionss can be used to support any claims on the randomness, clustering or dispersion of a spatial point distribution."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.1-background",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.1-background",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.1 Background",
    "text": "A.1 Background\nRoad traffic accidents account for 1.19million deaths and up to 50 million non-fatal injuries according to a report by the WHO last year.\nThe same report identifies major risk groups: low- and middle-income countries, (esp in Africa and Europe) the working population, and males. It also identifies some key risk factors which include human error, speeding, driving under the influence of alcohol, distracted driving, unsafe road infrastructure, unsafe vehicles, and law enforcement. Most of the factors identified are behavioral in nature but do not discount that other factors may also contribute to a higher risk of occurrence.\nWithin Southeast Asia, Thailand has ranked the highest in terms of incidence of road traffic accidents with an average number of of 20,000 deaths a year or 56 a day. The country has also seen an increase in the number of accidents from 2014 to 2021. A large 19% of these accidents occurred in national highways, and the chances of encountering an accident-prone zone was found to be 66%."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.2-objectives",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.2-objectives",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nThis study aims to take a deeper look into the road accidents in Thailand, focusing on the Bangkok Metropolitan Region (BMR) which contains the capital Bangkok, and five neighboring provinces. (Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakan, Samut Sakhon)\nAs most literature has focused on behavioral and environmental factors, the study will focus on identifying spatiotemporal factors influencing the occurrence of road accidents in BMR. At the minimum, the study deliverables include the following:\n\nVisualization of spatiotemporal dynamics of road traffic accidents in BMR\nDetailed spatial analysis of road traffic accidents in BMR\nDetailed spatiotemporal analysis of road traffic accidents in BMR\n\nThe appropriate technique must be used for these deliverables and all the analysis and visualizations will be carried out using R."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.3-data-sources",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.3-data-sources",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe study makes use of the following datasets which are publicly available online.\n\n\n\nDataset Short Name\nDescription\nDatasource\n\n\n\n\nTHRA\nThailand road accident data from 2019 to 2022\nKaggle\n\n\nTHOSM\nThailand roads open street map in shapefile format\nHDX\n\n\nTHSAB\nThailand - Subnational Administrative Boudaries shapefile\nHDX"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.4-importing-and-launching-r-packages",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.4-importing-and-launching-r-packages",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, four R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nsf - package for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - package with functions for plotting cartographic quality maps\nsPNetwork - provides functions for performing SPPA methods like KDE and K-function on a network. The package can also be used to build spatial matrices to conduct traditional spatial analyses with spatial weights based on reticular distances\nspatstat - package for plotting, EDA and simulation of spatial data\n\n\n\n\npacman::p_load(sf, spNetwork, tmap, tidyverse, spatstat)\n\n\n\n\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.1-thailand-subnational-administrative-boundary-shapefile",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.1-thailand-subnational-administrative-boundary-shapefile",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.1 Thailand Subnational Administrative Boundary, Shapefile",
    "text": "B.1 Thailand Subnational Administrative Boundary, Shapefile\nWe load the Thailand subnational administrative boundary shapefile into an R dataframe using st_read() from the sf package. The source provides the geospatial data in varying levels as indicated by their suffix: country (0), province (1), district (2), and sub-district. (3) For focusing on the BMR, which covers Bangkok and neighboring provinces, province is the most likely level of detail we will need so we will use the code chunk below to load the appropriate layer first.\n\nthsab_prov &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nWe examine the loaded data to confirm the load has been done properly and to get some initial observations of the data.\n\nCalling ObjectChecking crs information with st_crs()\n\n\n\nthsab_prov\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   Shape_Leng Shape_Area                  ADM1_EN       ADM1_TH ADM1_PCODE\n1    2.417227 0.13133873                  Bangkok  กรุงเทพมหานคร       TH10\n2    1.695100 0.07926199             Samut Prakan    สมุทรปราการ       TH11\n3    1.251111 0.05323766               Nonthaburi         นนทบุรี       TH12\n4    1.884945 0.12698345             Pathum Thani        ปทุมธานี       TH13\n5    3.041716 0.21393797 Phra Nakhon Si Ayutthaya พระนครศรีอยุธยา       TH14\n6    1.739908 0.07920961                Ang Thong        อ่างทอง       TH15\n7    5.693342 0.54578838                 Lop Buri          ลพบุรี       TH16\n8    1.778326 0.06872655                Sing Buri         สิงห์บุรี       TH17\n9    2.896316 0.20907828                 Chai Nat         ชัยนาท       TH18\n10   4.766446 0.29208711                 Saraburi         สระบุรี       TH19\n   ADM1_REF ADM1ALT1EN ADM1ALT2EN ADM1ALT1TH ADM1ALT2TH  ADM0_EN   ADM0_TH\n1      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n2      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n3      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n4      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n5      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n6      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n7      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n8      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n9      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n10     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n   ADM0_PCODE       date    validOn    validTo                       geometry\n1          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.6139 13...\n2          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.7306 13...\n3          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3415 14...\n4          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.8916 14...\n5          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.5131 14...\n6          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3332 14...\n7          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((101.3453 15...\n8          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3691 15...\n9          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.1199 15...\n10         TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((101.3994 15...\n\n\n\n\n\nst_crs(thsab_prov)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\nThe output confirms that we have a multipolygon sf object with 77 rows and 17 columns. There is a column named ADM1_EN which appears to contain the province names needed to define the BMR boundaries. It also shows that the dataset is using a coordinate reference system rather than a projected reference system.\nFirst, we reload the data to use a projected reference system and apply the correct reference system with EPSG code of 32647 using st_transform(). This transformation can be confirmed with st_crs() The tmap package is then used to visualize the object to see if it properly depicts the boundaries of Thailand and its provinces.\n\nLoad Object and Transform CRS informationChecking crs information with st_crs()Plot of thsab_prov using tmap\n\n\n\nthsab_prov &lt;- st_read(dsn=\"data/geospatial\",\n                          layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\nst_crs(thsab_prov)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n\ntm_shape(thsab_prov) +\n  tm_polygons(\"grey\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.2-filtering-thsab-for-the-bangkok-metropolitan-region",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.2-filtering-thsab-for-the-bangkok-metropolitan-region",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.2 Filtering THSAB for the Bangkok Metropolitan Region",
    "text": "B.2 Filtering THSAB for the Bangkok Metropolitan Region\nBefore further analyzing the data, we will limit the scope to only consider the Bangkok Metropolitan Region or BMR. This would encompass Bangkok, Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakan, Samut Sakhon. While it is good to get insights outside of BMR, it is out of the study scope and it is best to focus on the objectives.\nThe code chunk below checks if all the provinces in the BMR appear as is under the ADM1_EN column of thsab_prov\n\nfilter(thsab_prov, ADM1_EN %in% c(\"Bangkok\", \"Nonthaburi\",\"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\"))$ADM1_EN\n\n[1] \"Bangkok\"       \"Samut Prakan\"  \"Nonthaburi\"    \"Pathum Thani\" \n[5] \"Nakhon Pathom\" \"Samut Sakhon\" \n\n\nWith the previous code returning all 6 provinces, we have confirmation that the provinces are all present and spelled as is in the data source. We create a new object bmr_boundary to contain only the provinces in BMR. We also take this opportunity to only keep the relevant columns in the dataset using the select() function of dplyr package.\n\nCreate BMR boundary object using filter()Plot of bmr_boundary using tmap\n\n\n\nbmr_boundary &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647) %&gt;%\n  filter(ADM1_EN %in% c(\"Bangkok\", \"Nonthaburi\",\"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\")) %&gt;% dplyr::select(Shape_Leng, Shape_Area, ADM1_EN, geometry)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\ntm_shape(bmr_boundary) +\n  tm_polygons(\"grey\")\n\n\n\n\n\n\n\n\n\n\n\nThe code below creates a second object which is just a union of all the provinces. (i.e., borders between provinces are lost) This is done using the st_union() function.\n\nbmr_full = st_union(bmr_boundary)\n\n\ntm_shape(bmr_full) +\n  tm_polygons(\"grey\")\n\n\n\n\n\n\n\n\nThe code below keeps the final boundary objects into files to make loading more convenient for later analyses.\n\nwrite_rds(bmr_boundary, \"data/rds/bmr_boundary.rds\")\nwrite_rds(bmr_full, \"data/rds/bmr_full.rds\")\n\nThe code below then reloads the same objects into R:\n\nbmr_boundary = read_rds(\"data/rds/bmr_boundary.rds\")\nbmr_full = read_rds(\"data/rds/bmr_full.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.3-road-accident-data-aspatial-csv-file",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.3-road-accident-data-aspatial-csv-file",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.3 Road Accident Data, Aspatial, csv-file",
    "text": "B.3 Road Accident Data, Aspatial, csv-file\nThe road accident data is contained in a csv file. We use the code block in the first tab below to load it into the thra object with some necessary transformations that we identified upon inspecting the raw file. The second tab gives an explanation of the different nested functions used in the code\n\nCode to import and transform road accident dataExplanation of the code lines / functions used\n\n\n\nbmracc &lt;- read_csv(\"data/aspatial/thai_road_accident_2019_2022.csv\")  %&gt;%\n  filter(!is.na(longitude) & longitude != \"\", \n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;%\n  st_transform(crs = 32647)\n\nbmracc &lt;- filter(bmracc, geometry %in% st_intersection(bmr_full, bmracc)) %&gt;%\n  mutate(Year = year(incident_datetime)) %&gt;%\n  mutate(MonthNum = month(incident_datetime)) %&gt;%\n  mutate(Month = month(incident_datetime, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(DayOfWeek = wday(incident_datetime, label = TRUE, abbr = TRUE))\n\n\n\n\nread_csv() used to import a csv file into an R object\nfilter(!is.na(longitude) & longitude != \"\", !is.na(latitude) & latitude != \"\") used to exclude any records where the longitude or latitude information is missing\nst_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) used to convert the dataframe into an sf object using a reference system (WGS84) based on the coordinates\nst_transform(crs = 32647) used to apply the correct EPSG code to the sf object\nfilter(bmracc, geometry %in% st_intersection(bmr_full, bmracc)) used to leave only records which fall within the BMR boundaries\nmutate(...) these lines are used to add additional columns to quickly reference the year, month and day of the week that each accident occured as these dimensions allow for some temporal analyses\n\n\n\n\nCalling the new object shows that it has 12,989 rows across 20 fields.\n\nbmracc\n\nWe use the code chunks below to check the data and visualize the data on the BMR boundary map.\n\ntm_shape(bmr_boundary) +\n  tm_polygons(col = \"grey\") +\n  tm_shape(bmracc) +\n  tm_dots(col = \"red\", size = 0.01, alpha = 0.5)\n\n\n\n\n\n\n\n\nThe code chunk below writes the resulting accident dataset into an rds file for convenient loading.\n\nwrite_rds(bmracc, \"data/rds/bmracc.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.4-thailand-roads-open-streetmap-shapefile",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.4-thailand-roads-open-streetmap-shapefile",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.4 Thailand Roads Open StreetMap, Shapefile",
    "text": "B.4 Thailand Roads Open StreetMap, Shapefile\nThe second geospatial object is the street map shapefile. We will use the object network to contain the final road network for the study.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"hotosm_tha_roads_lines_shp\")\n\nRunning the above code confirms that the dataset is in multilinestring sf format and that it contains 2.8M records across 15 variables. It also shows that there is no CRS applied to the dataset.\nBased on these, the following steps need to be done: apply the right CRS/EPSG code of 32647 or the same as bmr_full, and, filter the network to only include BMR.\nThe code below does the first step of applying a reference system and updating the EPSG code to 32647 using st_set_crs() and st_crs() from the sf package.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"hotosm_tha_roads_lines_shp\") %&gt;%\n  st_make_valid() %&gt;% st_set_crs(4326) %&gt;% st_transform(crs = st_crs(bmr_full))\n\nThe code below then finds the network within BMR by using st_intersection() to find the overlap between the full road network and the BMR boundary. We also include write_rds() in the chunk to store this object into an rds file for easy future loading.\n\nnetwork &lt;- st_intersection(network, bmr_full)\nwrite_rds(network, \"data/rds/network.rds\")\n\nCalling the object name allows us to inspect the contents.\n\nnetwork\n\nThe size of the object has now been reduced to 585K features from the original 2.8M. This still appears a very large number if we want to visualize the data, so we need to inspect if there are any opportunities to reduce the dataset by excluding any irrelevant records.\nThe data includes a column named highway which gives information on the the type or classification of the road.\n\nggplot(network, aes(x = reorder(highway, table(highway)[highway]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Roads by Highway type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"Highway Type\", y = \"Number of Roads\")\n\nThe resulting chart shows that roads with highway type of residential and service make up 522K of the 585K roads in the dataset. We refer to the OpenStreetMap wiki page to see the definition of the different types of highways in the Thailand map and see that these two highway types are access roads for residences or specific buildings. For our objective, we should be able to limit to roads where accidents (are expected to frequently) happen, and only to roads that should be accessible by vehicles. Going through the definition of the highway types, we see that the following 6 types could be out of scope for our study:\n\nresidential - road within a residential area that gives public access to one or multiple residences\nservice - minor road that gives access to buildings or places outside a residential area (e.g., to a religious site, an attraction, part of an estate)\nfootway - pathways designed for pedestrian access\ntrack - road whose only function is to provide access to surrounding land, and is most of the time unpaved\npath - multi-purpose path intended for non-motor vehicles\nsteps\n\nWe can then use the following code chunk which uses the filter() function to remove these classifications from the current network object. We call the object name in the succeeding code chunk to check the new dataset.\n\nbmr_network &lt;- bmr_network %&gt;% \n  filter(!(highway %in% c(\"residential\", \"service\", \"footway\", \"track\", \"path\", \"steps\")))\n\n\nbmr_network\n\nWhile this looks good, it looks like the object is being identified as a GEOMETRY rather than a LINESTRING object. We can use the code below to correct it.\n\nbmr_network &lt;- st_cast(bmr_network, \"LINESTRING\")\n\n\nbmr_network\n\nThe new road network object is now reduced to 34K records or roads which is a 94% reduction in the number of records. We will use some visual inspection to see if this reduction in records will affect our analysis. The two code chunks below plot the road network within the boundaries, while the second plots the three objects together. We use the tmap function to create these maps.\n\nBMR filtered road network onlyBMR filtered road network with road accident dataset\n\n\n\ntm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\") +\n  tm_shape(bmracc) +\n  tm_dots(col = \"red\", alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nFrom these two maps, we see that:\n\nwhile we have filtered 90% of the original records, the resulting map still appears dense, especially in some central areas; and,\nthe road accident locations appear to fall along the network\n\nBased on these, we will go ahead with this version of the network for our analysis.\nThe following code writes the resulting network into an rds file for more convenient loading in the future.\n\nwrite_rds(bmr_network, \"data/rds/bmr_network.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.5-resolving-duplicate-points",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.5-resolving-duplicate-points",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.5 Resolving duplicate points",
    "text": "B.5 Resolving duplicate points\nIn this section, we perform some additional transformations to perform the required analyses.\nFirst, we check the event or accident dataset to see if there are any duplicated data points or locations as the methods require that the points are unique. The code below checks if any duplicate points exist. Note that we specify the column in the argument as we are double-checking duplicate locations rather than completely duplicate records.\n\nany(duplicated(bmracc$geometry))\n\n[1] TRUE\n\n\nAs the code returned TRUE, it confirms the presence of duplicate points, we use st_jitter() from the sf package to introduce some jitter to each point and ensure that points do not lie on the same location. Without any additional arguments, the function uses a default factor 0.002 of the bounding box diagonal as the bounds for the amount of jitter introduced. In the code below, we define an amount of 0.01 instead.\n\nbmracc_jitt &lt;- st_jitter(bmracc, 0.01)\n\nRerunning the check using duplicated() shows that there are no duplicate points anymore.\n\nany(duplicated(bmracc_jitt$geometry))\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#c.1-categories-of-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#c.1-categories-of-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "C.1 Categories of accidents",
    "text": "C.1 Categories of accidents\nWe first would like to understand the different labels we can use from the BMR road accident dataset bmracc The following columns appear to be able to give some insight about the nature of the accident:\n\nvehicle_type\npresumed_cause\naccident_type\nnumber_of_vehicles_involved\nnumber_of_fatalities\nweather_condition\n\nWe will try to be brief in analysing these variables as the main intent is to understand which ones will add the most value to the spatial analysis needed to address the main study objectives.\nNote that while we use bmracc rather than the modified bmracc_jitt in the codes below, the result will be the same as we do not concern ourselves with the geometry innformation yet.\n\nC.1.1 Vehicle Type\nThis variable is intended to give the type of vehicle involved in the accident. We use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(vehicle_type, table(vehicle_type)[vehicle_type]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Vehicle type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nThe output shows that out of 12,989 accidents, 35% are with private or passenger cars, 27% are with pickup trucks and 13% are with motorcycles. These three make up 75% of all the recorded accidents while the remaining 11 types make up the balance 25%\n\n\nC.1.2 Presumed Cause\nThis variable is intended to give the presumed cause of the accident. We again use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(presumed_cause, table(presumed_cause)[presumed_cause]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Presumed Cause\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that 10,146 or 75% of the accidents are presumed to be caused by speeding. The next largest named presumed cause only accounts for 5% of the overall data.\n\n\nC.1.3 Accident Type\nThis variable is intended to give the type or nature of the accident. We again use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(accident_type, table(accident_type)[accident_type]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that “rear-end collisions” and “rollover/fallen on straight road” are the leading causes recorded and account for 83% of the accidents.\n\n\nC.1.4 Number of Fatalities\nThis variable is intended to give the number of fatalities resulting from the accident. We again use the code block below to understand the categories under this variable using a simple histogram created through ggplot().\n\nggplot(bmracc, aes(x = number_of_fatalities)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"darkgrey\") +\n  scale_x_continuous(breaks = seq(min(bmracc$number_of_fatalities), max(bmracc$number_of_fatalities), by = 1)) +\n  labs(title = \"Accidents by Number of Fatalities\", x = \"Number of Fatalities\", y = \"Number of Accidents\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.5)\n\n\n\n\n\n\n\n\nThe plot shows that 94% of the recorded accidents are non-fatal. Only 719 were fatal. Although this is a small number, it might be worth looking at the location of such fatal accidents later. We can use the code below to introduce a new column fatal into the data for more convenient filtering later.\n\nbmracc$fatal &lt;- bmracc$number_of_fatalities &gt; 0\n\n\n\nC.1.5 Weather Condition\nThis variable is intended to indicate the weather condition when the accident was recorded. We first use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(weather_condition, table(weather_condition)[weather_condition]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Weather Condition\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that 90% of the accidents occurred during “clear” weather. Online sources suggest that Bangkok experieces a long rainy season and has 153 rainy days per year, so 10% for the occurrence of accidents appears low. The sources also say that the wettest month is September.\nWe can use the code chunk below to plot the number of accidents that were not recorded on clear weather (i.e., rainy) by month.\n\nggplot(filter(bmracc, !weather_condition == \"clear\"), aes(x = reorder(Month, table(Month)[Month]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Month during Non-clear Weather\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe plot does align with the expectation that September is the wettest month. The very low number of accidents during rainy weather is still questionable though so we will watch out for this if we will use this variable later."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.1.-converting-objects-into-spatstats-formats",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.1.-converting-objects-into-spatstats-formats",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.1. Converting Objects into spatstat’s formats",
    "text": "D.1. Converting Objects into spatstat’s formats\nThe events need to be converted into spatstat’s ppp object using as.ppp()\n\nbmracc_ppp &lt;- as.ppp(st_geometry(bmracc_jitt))\n\nWe then prepare an owin object to define the boundaries using the as.owin() function.\n\nbmr_owin &lt;- as.owin(bmr_full)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.2-kde-for-all-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.2-kde-for-all-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.2 KDE for all Accidents",
    "text": "D.2 KDE for all Accidents\nWe first combine the accidents (all years, all types) into the owin using the following code chunk\n\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n\nWe can then compute for the kde using the density() function. We use the four common methods for automatic bandwidth selection and examine them in a grid using the code chunk below.\n\npar(mfrow=c(2,2))\nplot(density(bmracc_for_kde, sigma=bw.diggle,\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Diggle Method\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Scott's Rule\")\nplot(density(bmracc_for_kde, sigma=bw.CvL(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Cronie & Van Lieshout Criterion\")\nplot(density(bmracc_for_kde, sigma=bw.ppl(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Likelihood Cross Validation\")\n\n\n\n\n\n\n\n\nAmong the four methods, it looks like Scott’s rule (using sigma=bw.scott()) is identifying hot spots unlike the others. There is a hotspot in the southwest and southeast of Bangkok. There also appears to be a high density strip (or maybe a major highway) stretching up northwards.\nWe will use this bandwidth selection method for our succeeding analysis.\n\nD.3 KDE for accidents across years\nWe then want to see if the hotspots move across the years. To do this, we effectively need to compute for the kde across years and see if there are any visible signs of shifts in the hotspots.\nFirst, let us try to understand the distribution of accidents by year. This will help us understand if the numberical values of the density will move because of change in the absolute number of accidents.\nWe use ggplot() in the code chunk below to achieve this.\n\nggplot(bmracc, aes(x = reorder(Year, table(Year)[Year]))) +\n  geom_bar() +\n  ggtitle(\"Number of Accidents by Year\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), vjust = +2) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 12),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that there were significantly more accidents in 2022 compared to the previous years. The other years are within 10% of each other. We should expect that 2022 KDE may have higher numerical values compared to the other years.\nWe can then use the following code to generate four different kde’s, one for each year.\n\npar(mfrow=c(2,2))\nfor (i in c(2019, 2020, 2021, 2022)) {\n  bmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, Year == i)))\n  bmracc_for_kde = bmracc_ppp[bmr_owin]\n  bmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n  plot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = paste(\"KDE for year\",i))\n}\n\n\n\n\n\n\n\n\nWe see that there appears to be a shift between 2020 and 2021. Before 2020, there appeared to be two separate promininet hotspots for accidents– in the central and southeastern portion of the region. However, after 2020, it seems that the accidents are more frequent in the southeastern part, and in a much wider area."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.3-kde-across-months",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.3-kde-across-months",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.3 KDE across Months",
    "text": "D.3 KDE across Months\nWe can apply a similar approach of analysing by month using the code chunk below. For now, we are aggregating accidents by month across all years, so the insights will apply to the whole period and not any particular year.\n\npar(mfrow=c(3,4))\nfor (i in 1:12) {\n  bmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, MonthNum == i)))\n  bmracc_for_kde = bmracc_ppp[bmr_owin]\n  bmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n  plot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = paste(\"KDE for month\",i))\n}\n\n\n\n\n\n\n\n\nThe output reveals no drastic shift in hotspots (using kde) across months. There are months where the intensities and relative intensities differ, but it appears like the hotspots remain in the same areas."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.4-kde-for-clear-vs-rainy-days",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.4-kde-for-clear-vs-rainy-days",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.4 KDE for Clear vs “Rainy” Days",
    "text": "D.4 KDE for Clear vs “Rainy” Days\nThe final analysis we want to perform before moving to network-constrained analysis is on clear vs non-clear days. This is indicated in the field called weather_condition in the accident dataset.\nTo produce the kde visualization, we can use the code chunk below.\n\npar(mfrow=c(1,2))\n\nbmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, weather_condition == \"clear\")))\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n             edge=TRUE,kernel=\"gaussian\"), main = \"KDE for Clear Days\")\n\nbmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, !(weather_condition == \"clear\"))))\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n             edge=TRUE,kernel=\"gaussian\"), main = \"KDE for Rainy Days\")\n\n\n\n\n\n\n\n\nThe output are very similar too some of the charts generated earlier. The hotspot in the center of BMR appears to dissipate during rainy days. (relative to the one in the southeast portion of the region."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.1-preparation-of-data-for-network-constrained-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.1-preparation-of-data-for-network-constrained-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.1 Preparation of Data for Network Constrained Analysis",
    "text": "E.1 Preparation of Data for Network Constrained Analysis\nBefore we are able to perform network-constrained NKDE, we first need to define sample points along the road network, and to do that, we can use the midpoint of the lixels of the network.\n\nE.1.1 Preparing the lixels\nTo lixelize a network, the minimum and (maximum) length of lixels need to be defined. A logical distance needs to be chosen for a given study. In our case, we might no have enough information to understand what road segment length is relevant to group accidents into. However, we can start with understanding the road lengths in the network.\nWe can use the code block below to show the distribution of the road length values using summary() to give the quartiles, and quantile() to give a wider range of view.\n\nsummary(st_length(bmr_network))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n    0.143    31.581   124.691   413.002   433.312 24745.584 \n\nquantile(st_length(bmr_network), probs = seq(.1, .9, by = .1))\n\nUnits: [m]\n       10%        20%        30%        40%        50%        60%        70% \n  13.78247   24.36581   41.45909   71.64269  124.69070  207.11305  337.56988 \n       80%        90% \n 556.85935 1074.26696 \n\n\nThe output shows that there is a very wide range of values. There is also a surprisingly large number of roads (&gt;40%) that are less than 100m– which seem to be too short for typical roads. We can first choose a min distance of 200m which would allow for at least 40% of roads to not be split. As for the maximum length, let us first set it to 600m so only a little over 20% of the roads will be split into smaller segments.\nWe implement this using lixelize_lines() in the code chunk below.\n\nlixels &lt;- lixelize_lines(bmr_network$geometry, \n                         600, \n                         mindist = 200)\n\n\n\nE.1.2 Generating sample points\nThe next step is to define sample points along the network which will be the points where the KDE function will be computed on.\nWe can create sample points on the lixel centers using lines_center() in the code below.\n\nsamples &lt;- lines_center(lixels)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.2-all-years-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.2-all-years-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.2 All Years, All Accidents, Initial Analysis",
    "text": "E.2 All Years, All Accidents, Initial Analysis\nLet’s start by looking at the highest levels. We can compute for the nkde for all accidents in the dataset (2019-2022) using the code chunk below. This uses the accidents with the jitter applied.This code uses a bandwidth of 300m, anduses “quartic” for the kernel function, and uses simple calculation method for the KDE\n\ndensities &lt;- nkde(bmr_network, \n                  events = bmracc_jitt,\n                  w = rep(1, nrow(bmracc_jitt)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe import the densities into the lixel and sample object using the code below. We use a multiple of 1000 to convert the figures from accidents per square meter to accidents per square kilometer\n\nsamples$density_all &lt;- densities*1000000\nlixels$density_all &lt;- densities*1000000\n\nWe use the code below to produce a map with just the calculated densities.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe resulting map surprisingly does not show a lot of high density road segments, which is not aligned to the earlier map with the locations of the accidents. We can check if the jitter has caused displacement of the locations and hidden high density road segments by rerunning the below code chunk which uses the accident locations without jitter applied.\n\ndensities &lt;- nkde(bmr_network, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nTo see if using the original event points will address our concern, we repeat the code chunk below to create a static map.\n\nsamples$density_all &lt;- densities*1000000\nlixels$density_all &lt;- densities*1000000\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nIt looks like the use of the original accident data does little to reveal dense locations. If we examine the interactive map and zoom in, we see one possible reason for the problem. Major roads are being split into multiple semi-parallel roads. These might denote different directions on the same highway, service roads, etc. These might cause accidents on the same “parent” road to be split across their parts.\nWe try to solve this problem by recreating our network while merging such roads into one."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.3-transformation-step---merging-of-parallel-roads",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.3-transformation-step---merging-of-parallel-roads",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.3 Transformation step - merging of parallel roads",
    "text": "E.3 Transformation step - merging of parallel roads\nOne way to merge the roads, is to first use st_buffer() to create a width dimension on the roads. We would prefer to use the actual width of the roads, plus the islands here, but there is no way to do this accurately and this would vary from road to road. (e.g., some roads could have one lane, while others could have four or more lanes) For our case, we will use a width of 2 x 15m, which is based on a ~3m lane width estimate, which means we are buffering up to the width of five lanes or five cars on each side.\nWe use the code chunk below to produce a buffered network.\n\nbmr_network_buffered &lt;- st_buffer(bmr_network, dist = 15)\nbmr_network_dissolved &lt;- st_union(bmr_network_buffered)\n\nThe next step is to convert or cast this into a linestring object, but before that we would want to make sure that the geometries are simple enough so the casting is executed properly. To do this, we fist use st_simplify() which simplifies objects by reducing vertices.\n\nbmr_network_simplified &lt;- st_simplify(bmr_network_dissolved, dTolerance = 1)\n\nWith the network simplified, we can then use st_cast() to convert the geometries back into linestrings. Note that we use two calls since we cannot cast polygons directly into linestrings.\n\nbmr_network_v2 &lt;- st_cast(bmr_network_simplified, \"MULTILINESTRING\")\nbmr_network_v2 &lt;- st_cast(bmr_network_v2, \"LINESTRING\")\n\nNote that the resulting object was a list rather than a dataframe, we can use st_as_sf() to ensure that it is in an sf dataframe format.\n\nbmr_network_v2 &lt;- st_as_sf(bmr_network_v2)\n\nWe can examine the original and new network side by side using tmap_arrange() in the code below to see that the new network has worked sufficiently.\n\norig_network &lt;- tm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\") +\n  tm_layout(title = \"Original Road Network\")\n\nnew_network &lt;- tm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network_v2) +\n  tm_lines(col = \"black\")+\n  tm_layout(title = \"Simplified Road Network\")\n\n\ntmap_arrange(orig_network, new_network, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nThe transformation seems to have worked, and we see by counting the objects using length() st_geometry() in the code below, that the number of roads has been reduced dramatically from 34K to 4.2K– even with the very similar high level map.\n\nlength(st_geometry(bmr_network))\n\n[1] 34056\n\nlength(st_geometry(bmr_network_v2))\n\n[1] 4162\n\n\nLet us examine the road lengths in the updated network using summary() and quantile() in the code chunk below.\n\nsummary(st_length(bmr_network_v2))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n      4.0     326.1    1935.9    4529.2    5826.1 1263640.1 \n\nquantile(st_length(bmr_network_v2), probs = seq(.1, .9, by = .1))\n\nUnits: [m]\n       10%        20%        30%        40%        50%        60%        70% \n  111.8646   236.2789   474.0794  1023.5185  1935.9245  3231.7745  4846.3553 \n       80%        90% \n 7074.5520 10733.3337 \n\n\nThe simplified network now has longer road segments with the median being close to 2km in length.\nWe can then repeat the preparation of data from the creation of the lixels to the creation of the sample points. we will use exactly a similar code using lixelize_lines() as in the earlier sections. Given the distribution of the lengths, we decide to use longer lixel lengths with this new network. We choose 1km and 2km for the parameters.\n\nlixels_v2 &lt;- lixelize_lines(st_geometry(bmr_network_v2), \n                         2000, \n                         mindist = 1000)\nsamples_v2 &lt;- lines_center(lixels_v2) \n\nBefore we move, let us remove the intermediate objects from memory using rm()\n\nrm(bmr_network_buffered)\n\nWarning in rm(bmr_network_buffered): object 'bmr_network_buffered' not found\n\nrm(bmr_network_dissolved)\n\nWarning in rm(bmr_network_dissolved): object 'bmr_network_dissolved' not found\n\nrm(bmr_network_simplified)\n\nWarning in rm(bmr_network_simplified): object 'bmr_network_simplified' not\nfound"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-all-years-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-all-years-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.4 All Years, All Accidents, Initial Analysis",
    "text": "E.4 All Years, All Accidents, Initial Analysis\nWe now rerun the highest level KDE with the updated network to see if we are getting more insightful output.\nWe rerun nkde() to compute the network constrained KDE on the new network and using the new sample points.\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe again transfer these densities into the lixels and sample dataframes using the code chunk below\n\nsamples_v2$density_all &lt;- densities*1000000\nlixels_v2$density_all &lt;- densities*1000000\n\nNext, we can create a static map to show the computed KDEs visually using tmap package in the code chunk below.\n\ntm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\")\n\n\n\n\n\n\n\n\nWIP"
  },
  {
    "objectID": "In-class/In-class_Ex01/data/geospatial/MPSZ-2019.html",
    "href": "In-class/In-class_Ex01/data/geospatial/MPSZ-2019.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-distribution-of-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-distribution-of-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.4 Distribution of All Accidents, Initial Analysis",
    "text": "E.4 Distribution of All Accidents, Initial Analysis\nWe now rerun the highest level KDE with the updated network to see if we are getting more insightful output.\nWe rerun nkde() to compute the network constrained KDE on the new network and using the new sample points.\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe again transfer these densities into the lixels and sample dataframes using the code chunk below\n\nsamples_v2$density_all &lt;- densities*1000000\nlixels_v2$density_all &lt;- densities*1000000\n\nNext, we can create a static map to show the computed KDEs visually using tmap package in the code chunk below.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWhile the network is simplified, it looks like we still get a very homoegenous, yellow chart. This might be due to the bottom-most bin including zeros. We can inspect the number of zeros and the range of the nonzero kde’s using summary() in the code chunk below.\n\nprint(\"Distribution of all Densities\")\n\n[1] \"Distribution of all Densities\"\n\nsummary(lixels_v2$density_all)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   1.623   0.000 214.461 \n\nprint(\"Distribution of non-zero Densities\")\n\n[1] \"Distribution of non-zero Densities\"\n\nsummary(filter(lixels_v2, density_all &gt; 0)$density_all)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n  0.00004   2.83624   4.71730  11.10733  11.22094 214.46127 \n\n\nIt looks like non-zero vlues are sparse. Less than 25% of the lixles have non-zero values. We can either use a custom palette or add a layer to grey out the zero denisty lixels. We use the latter in the code chunk below. We also add some additional elements like the provinces, and modify the formatting, in order to make the chart more information-rich and readable.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 2, title.col = \"Per sq km\") +\n  tm_shape(filter(lixels_v2, density_all == 0)) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe plot now shows clear road segments where there is higher density. It reveals the most dense segments lie within Bangkok. Pathum Thani and Samut Sakhon also show some high density segments. Meanwhile, Nakohn Pathom appears to have the least accident dense road segments."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-year",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-year",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.5 Distribution of Accidents by Year",
    "text": "E.5 Distribution of Accidents by Year\nWe then look into the distribution of accidents across years to see if there is a change or shift that has occured. To do this, we need to generate the nkde for each year using the code below. The code writes a new column for the lixels and the samples dataframes for each year’s KDE values.\n\nfor (i in 2019:2022) {\n   densities &lt;- nkde(bmr_network_v2, \n                  events = filter(bmracc, Year == i),\n                  w = rep(1, nrow(filter(bmracc, Year == i))),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n  lixels_v2[[paste(\"density_\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n  samples_v2[[paste(\"density_\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n  }\n\nWe can then use the code block below to generate the map of the four different years using tmap package.\n\ncolumns_to_map &lt;- c(\"density_2019\", \"density_2020\", \"density_2021\", \"density_2022\")\nyearly = list()\nfor (col in columns_to_map)\n{\n  yearly[[col]] &lt;- tm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=col, palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_shape(lixels_v2[lixels_v2[[col]] == 0, ]) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = paste(\"Year -\",substr(col,9,12)),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n}\n\n\ntmap_arrange(yearly[[1]], yearly[[2]], yearly[[3]], yearly[[4]], ncol = 2)\n\n\n\n\n\n\n\n\nThe output shows no significant change in the location of the hotspots across years. Before we look at another dimension, let us try to test for complete spatial randomness on the most recent year."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-test-for-csr---2022-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-test-for-csr---2022-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.6 Test for CSR - 2022 Accidents",
    "text": "E.6 Test for CSR - 2022 Accidents\nIt clearly looks like accidents are not randomly or homogeneously distributed in the network. We can verify this using tests for CSR (Complete Spatial Randomness) using the K- or G-functions.\nFor our CSR test, the test hypotheses will be:\n\n\\(H_0\\) - Road accidents in 2022 are randomly distributed along the BMR road network\n\\(H_1\\) - Road accidents in 2022 are not randomly distributed along the BMR road network\n\nThe code chunk below runs these two functions for testing CSR using kfunctions() from the spNetwork package. We specify a range of 0m (start) and 2km (end) to evaluate the function. We also specify 50 Monte Carlo simulations (nsim + 1) to draw the envelope. A confidence interval (1 - conf_int) of 95%, and intervals of 200m for the steps and the donut width. We also use an agg argument to allow consolidation of events. (as the function cannot work with duplicate points)\n\nkfun_bmracc &lt;- kfunctions(bmr_network_v2, \n                             filter(bmracc_jitt, Year == 2022),\n                             start = 0, \n                             end = 2000, \n                             step = 200, \n                             width = 200, \n                             nsim = 49, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05,\n                             agg = 100)\n\nWe can output the K-function by calling on the plotk field, and the G-function by calling on the plotg field of the resulting object\n\nkfun_bmracc$plotg\n\n\n\n\n\n\n\nkfun_bmracc$plotk\n\n\n\n\n\n\n\n\nThe envelop depicts a 95% confidence level CSR interval for each function. Both functions do not support th hypothesis of CSR except for very short intervals. (where the blue lines fall within the envelope) The K-function supports the view on clustering from a distance of around 300m-1.9km, while the G-function supports this from around 250-650m. The G-function supports a view on regular distribution beyond 750m. While these differ in the details, both tests do not support CSR for the distribution of accidents in 2022."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-month",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-month",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.5 Distribution of Accidents by Month",
    "text": "E.5 Distribution of Accidents by Month\nWe can then look into the distribution of accidents across months. Seasonal events including holidays, climate, etc can be linked to the months, so it is good to see if there are months that deviate from most of the others.\nTo do this, we need to generate the nkde for each month similar to the approach for the yearly analysis.\n\nfor (i in 1:12) {\n   densities &lt;- nkde(bmr_network_v2, \n                  events = filter(bmracc, MonthNum == i),\n                  w = rep(1, nrow(filter(bmracc, MonthNum == i))),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n   if (i &gt; 9){\n     lixels_v2[[paste(\"density_M\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n     samples_v2[[paste(\"density_M\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n   }\n   else{\n     lixels_v2[[paste(\"density_M0\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n     samples_v2[[paste(\"density_M0\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n   }\n  \n  }\n\nWe can then use the code block below to generate the map of the different months using tmap package.\n\ncolumns_to_map &lt;- c(\"density_M01\", \"density_M02\", \"density_M03\",\n                    \"density_M04\",\"density_M05\",\"density_M06\",\n                    \"density_M07\", \"density_M08\", \"density_M09\",\n                    \"density_M10\",\"density_M11\",\"density_M12\")\nmonthly = list()\nfor (col in columns_to_map)\n{\n  monthly[[col]] &lt;- tm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=col, palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_shape(lixels_v2[lixels_v2[[col]] == 0, ]) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = paste(\"Month -\",substr(col,10,11)),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n}\n\nTo be able to view the maps clearly, we display them individually in the tabs below.\n\nJanuaryFebruaryMarchAprilMayJuneJulyAugustSeptemberOctoberNovemberDecember\n\n\n\nmonthly[[1]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[2]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[3]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[4]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[5]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[6]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[7]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[8]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[9]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[10]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[11]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[12]]\n\n\n\n\n\n\n\n\n\n\n\nThe output shows a few hotspots arising on specific months. We mention the ones where there are highly dense segments outside Bangkok:\n\nPathum Thani - January, March, June\nSamut Sakhon - January"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-fatal-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-fatal-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.6 Fatal Accidents",
    "text": "E.6 Fatal Accidents\nAs mentioned in the earlier section, only a small 6% or 719 of the total number of accidents were fatal. While small, this is still a large number for the affected families. We expect that such accidents would have also caused more disruption compared to most of the non-fatal ones.\n\nE.6.1 Fatal Accidents by Year and by Province, Non-Network Constrained\nIf we look at the distribution of these accidents across years using the chart below, the annual number has ranged from 153-203, and 2020 and 2021 had 33% more accidents than the other years.\n\nggplot(filter(bmracc, number_of_fatalities &gt; 0), aes(x = reorder(Year, table(Year)[Year]))) +\n  geom_bar() +\n  ggtitle(\"Number of Fatal Accidents by Year\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), vjust = +2) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 12),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nWe can then visualize the location of these using the tmap package. The code below displays a map of the location of the fatal accidents in BMR between 2019 and 2022.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(bmr_network_v2)+\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = \"BMR Fatal Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nIt looks like fatal accidents are more frequent outside Bangkok. With the exception of Nakhon Pathom, it looks like there is a larger number of accidents, in number and in density, happening in the four other provinces.\nIt is hard to judge whether or not there are more accidents in a province using the graph above because of the presence of close or overlapping dots. One approach is to use the functions from sf package to count the accidents or events that fall within each province and then also compute for a density by computing the areas of each province.\nWe first compute for the number of fatal accidents that occur in each province by using st_intersects() function from sf package. We produce this for the total number of accidents and the accidents for each year by using the code chunk below. We create a copy of the bmr_boundary object to store these values.\n\nbmr_with_fatacc &lt;- bmr_boundary %&gt;%\n  mutate('FatAcc19-22' = lengths(st_intersects(bmr_boundary,filter(bmracc, number_of_fatalities &gt; 0)))) %&gt;%\n  mutate('FatAcc19' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2019) )))) %&gt;%\n  mutate('FatAcc20' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2020) )))) %&gt;%\n  mutate('FatAcc21' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2021) )))) %&gt;%\n  mutate('FatAcc22' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2022) ))))\n\nWe then compute for the area of each province using st_area() in the code chunk below.\n\nbmr_with_fatacc$Area &lt;- st_area(bmr_with_fatacc)\n\nFinally, we can compute for the density of accidents in each province by taking the ration of the last two measures we computed. Note that we are multiplying each by 1 million to convert the units from per meter to per square kilometer.\n\nbmr_with_fatacc$FatDensAll &lt;- bmr_with_fatacc$`FatAcc19-22` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens19 &lt;- bmr_with_fatacc$`FatAcc19` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens20 &lt;- bmr_with_fatacc$`FatAcc20` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens21 &lt;- bmr_with_fatacc$`FatAcc21` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens22 &lt;- bmr_with_fatacc$`FatAcc22` / bmr_with_fatacc$Area * 1000000\n\nWe can now compare the occurence of fatal accidents across provinces visually. First, we can produce a choropleth map for the number of accidents and the density of accidents side by side using the code below. This is done by passing a list of arguments for the different tmap elements. In the code below, we use this on the color of the polygons, the label and the chart title.\n\ntm_shape(bmr_with_fatacc)+\n  tm_polygons(col = c('FatAcc19-22', 'FatDensAll'), style = \"equal\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = c(\"Number\", \"Per Sq Km\"))+\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = c(\"Fatal Road Accidents 19-22\",\"Fatal Road Accidents Density\"),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe plot shows that Pathum Thani in the northeast and Samut Prakan in the southeast have the highest number of accidents. However, if we normalize by the area, Nonthaburi in the center and Samut Prakan, still, have the highest density of fatal accidents.\nWe can use the same approach to look at the density across the different years.\n\ntm_shape(bmr_with_fatacc)+\n  tm_polygons(col = c('FatDens19', 'FatDens20', 'FatDens21', 'FatDens22'), style = \"equal\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Per Sq Km\")+\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = c(\"2019 Fatal Accidents\",\"2020 Fatal Accidents\",\"2021 Fatal Accidents\",\"2022 Fatal Accidents\"),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nAt a province-level, Nonthaburi has been consistently the most dense with regards to fatal road accidents. In the meantime, Samut Prakan appears to have increased its density and risen in rank between 2020 and 2021.\nWe will not attempt to perform kde on this set of accidents and go straight to an nkde which considers the road network.’\n\n\nE.6.2 Fatal Accidents, Network Constrained\nThe final analyses we will perform is on the distribution of the fatal accidents along the road network. For this, we will also focus only on one year– 2022, as we recognize a shift in the hotspots over the years, at least across provinces\nTo facilitate the analysis, we create a subset of the accident dataset to only consider fatal accidents and the latest year.\n\nbmr_fatacc_2022 &lt;- filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2022) )\n\n\nbmr_fatacc_2022\n\nSimple feature collection with 159 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 610274.5 ymin: 1489002 xmax: 706582.6 ymax: 1570658\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 159 × 22\n   acc_code incident_datetime   report_datetime     province_th  province_en  \n *    &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;        &lt;chr&gt;        \n 1  5484851 2022-01-01 02:30:00 2022-01-02 02:43:00 สมุทรปราการ   Samut Prakan \n 2  5493686 2022-01-01 05:30:00 2022-01-03 10:23:00 นนทบุรี        Nonthaburi   \n 3  5493725 2022-01-01 14:00:00 2022-01-03 10:28:00 นครปฐม       Nakhon Pathom\n 4  6950193 2022-01-03 21:00:00 2022-09-21 10:53:00 นนทบุรี        Nonthaburi   \n 5  5837858 2022-01-03 23:34:00 2022-03-03 14:19:00 กรุงเทพมหานคร Bangkok      \n 6  6567040 2022-01-04 21:19:00 2022-01-05 11:27:00 สมุทรปราการ   Samut Prakan \n 7  6567055 2022-01-09 22:00:00 2022-01-11 10:56:00 ปทุมธานี       Pathum Thani \n 8  5861456 2022-01-11 18:00:00 2022-03-07 11:28:00 สมุทรปราการ   Samut Prakan \n 9  5579080 2022-01-15 00:10:00 2022-01-15 09:11:00 กรุงเทพมหานคร Bangkok      \n10  5843213 2022-01-16 11:30:00 2022-03-04 10:48:00 ปทุมธานี       Pathum Thani \n# ℹ 149 more rows\n# ℹ 17 more variables: agency &lt;chr&gt;, route &lt;chr&gt;, vehicle_type &lt;chr&gt;,\n#   presumed_cause &lt;chr&gt;, accident_type &lt;chr&gt;,\n#   number_of_vehicles_involved &lt;dbl&gt;, number_of_fatalities &lt;dbl&gt;,\n#   number_of_injuries &lt;dbl&gt;, weather_condition &lt;chr&gt;, road_description &lt;chr&gt;,\n#   slope_description &lt;chr&gt;, geometry &lt;POINT [m]&gt;, Year &lt;dbl&gt;, MonthNum &lt;dbl&gt;,\n#   Month &lt;ord&gt;, DayOfWeek &lt;ord&gt;, fatal &lt;lgl&gt;\n\n\nThere are 159 fatal accidents in the BMR in 2022, and in the new dataset– consistent with the summary in the previous section.\nWe then use the following code chunk to compute for the network-constrained KDE of the 2022 fatal accidents in our network. Note that in the code, we create a duplicate lixels and samples object to keep the original one unchanged. We take this opportunity to widen the computation range by increasing the bandwidth or bw parameter and the max_depth parameter.\n\nsamples_22 &lt;- samples_v2\nlixels_22 &lt;- lixels_v2\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmr_fatacc_2022,\n                  w = rep(1, nrow(bmr_fatacc_2022)),\n                  samples = samples_22,\n                  kernel_name = \"quartic\",\n                  bw = 10000, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 20,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nsamples_22$density_22 &lt;- densities*1000000\nlixels_22$density_22 &lt;- densities*1000000\n\nWe can then use the following codeblock to visualize the nKDE using tmap package.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(lixels_22)+\n  tm_lines(col=\"density_22\", palette = \"-inferno\", lwd = 2, title.col = \"Per sq km\") +\n  tm_shape(filter(lixels_22, density_22 == 0)) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = \"BMR Fatal Road Accident Density - 2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we learn how to compute and interpret global measures of spatial autocorrelation (GMSA) using the spdep package.\nThis exercise is based on Chapter 9 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#analytical-question",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Analytical Question",
    "text": "Analytical Question\nOne of the main development objective in spatial policy is for local governments and planners to ensure that there is equal distribution of development in the province. We then need to apply the appropriate spatial methods to verify if there is indeed even distribution of wealth geographically. If there is uneven distribution, then the next step is to identify if and where clusters are happening.\nWe continue studying the Hunan Province in China and focus on GDP per capita as the key indicator of development."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-sources",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-sources",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on the Hunan county coming from two files:\n\nHunan county boundary layer in ESRI shapefile format\nHunan local development indicators for 2012 stored in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#installing-and-launching-r-packages",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, tmap, and spdep.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspdep - functions to create spatial weights, autocorrelation statistics\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\nWe also define a random seed value for repeatability of any simulation results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-loading",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-loading",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Hunan shapefile into an R object.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe then use the code chunk below to load the csv file with the indicators into R using read_csv()\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-preparation",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-preparation",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-the-development-indicator",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-the-development-indicator",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package. We present these uas two maps using classes of equal intervals and equal quantiles.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#computing-contiguity-spatial-weights",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#computing-contiguity-spatial-weights",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nPrior to computing GMSA’s, we need t construct spatial weights of the study area. Spatial weights are used to define the neighborhood relationship between units. (i.e., neighbors or adjacent units)\nThe code chunk below uses poly2nb() of the spdep package to compute contiguity weight matrices for the study area. The function builds a neighbor list based on regions with shared boundaries. The queen argument takes TRUE (default) or FALSE as options. This instructs the function if Queen criteria should be used in defining neighbors. For the code below, we use the Queen criteria to build the contiguity matrix\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output shows that there are 88 units in the hunan dataset, The most connected unit has 11 neighbors and two units have only one neighbor."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#row-standardised-weights-matrix",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#row-standardised-weights-matrix",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Row-standardised weights matrix",
    "text": "Row-standardised weights matrix\nThe next step is assigning weights to each neighbor. For our case, we assign equal weight (using style=\"W\") to each neighboring polygon. This assigns the fraction 1/n, where n is the number of neighbors a unit has, as the weight of each unit’s neighbor. The drawback of this approach is that polygons in the edge of the study area will base their value on a smaller number of neighbors. This means that we may be potentially over- or under-estimating the true nature of spatial autocorrelation. The alternative more robust style=\"B\" can address this.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-test",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-test",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Moran’s I test",
    "text": "Moran’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of the spdep package\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nThe p-value does not support CSR for the GDPPC, while a positive statistic indicates signs of clustering. If the statistic value were below 0, or negative, then it would indicate signs of dispersion."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-morans-i",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-morans-i",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Monte Carlo Simulation for Moran’s I",
    "text": "Monte Carlo Simulation for Moran’s I\nWe use the code chunk below to perform permutation test for the statistic by using moran.mc() of spdep. The nsim argument is set so that 1000 simulations will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-morans-i",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-morans-i",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of Monte Carlo Simulation Results (Moran’s I)",
    "text": "Visualization of Monte Carlo Simulation Results (Moran’s I)\nIt is good practice to analyse and visualize the simulation results in more detail. We can do this by checking the values and distribution of the statistic numerically and graphically.\nWe can use the code chunk below to show individual statistics of the simulated value.\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\npaste(\"Standard Dev:\", var(bperm$res[1:999]))\n\n[1] \"Standard Dev: 0.00437157393477615\"\n\n\nWe can visualize graphically using hist() and abline() from R Graphics.\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-test",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-test",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Geary’s C test",
    "text": "Geary’s C test\nThe code chunk below uses geary.test() to perform Geary’s C test for spatial autocorrelation.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nGeary’s C test uses a different interpretation compared to Moran’s I. A statistic value less than one, as in the case above, indicates signs of clustering, while a value of greater than one indicates dispersion. The very low p-value means that any hypothesis of compete spatial randomness (with α &gt; 0.015%) is not supported by the observed data."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-gearys-c",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-gearys-c",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Monte Carlo Simulation for Geary’s C",
    "text": "Monte Carlo Simulation for Geary’s C\nWe use the code chunk below to perform permutation test for the statistic by using geary.mc() of spdep. The nsim argument is set so that 1000 simulations will be performed.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-gearys-c",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-gearys-c",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of Monte Carlo Simulation Results (Geary’s C)",
    "text": "Visualization of Monte Carlo Simulation Results (Geary’s C)\nIt is good practice to analyse and visualize the simulation results in more detail. We can do this by checking the values and distribution of the statistic numerically and graphically.\nWe can use the code chunk below to show individual statistics of the simulated value.\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\npaste(\"Standard Dev:\", var(bperm$res[1:999]))\n\n[1] \"Standard Dev: 0.00743649278244122\"\n\n\nWe can visualize graphically using hist() and abline() from R Graphics.\n\nhist(bperm$res,\n     freq=TRUE,\n     breaks=20,\n     xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-correlogram-and-plot",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-correlogram-and-plot",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Moran’s I Correlogram and Plot",
    "text": "Moran’s I Correlogram and Plot\nThe code chunk below uses sp.correlogram() of spdep package to compute a 6-lag (order=6) spatial correlogram of GDPPC using Moran’s I. (method=\"I\") We then plot() to produce the visualization.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\nAside from the output, we can also display the full content of the analysis using the code below. This lets us see the result for each lag in more detail.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-correlogram-and-plot",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-correlogram-and-plot",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Geary’s C Correlogram and Plot",
    "text": "Geary’s C Correlogram and Plot\nThe code chunk below uses sp.correlogram() of spdep package to compute a 6-lag (order=6) spatial correlogram of GDPPC using Geary’s C. (method=\"C\") We then plot() to produce the visualization.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\nWe can also examine the results in more detail using the code chunk below\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we learn how to compute and interpret local measures of spatial autocorrelation or local indicators of spatial association (LISA) using the spdep package.\nThis exercise is based on Chapter 10 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#analytical-question",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Analytical Question",
    "text": "Analytical Question\nOne of the main development objective in spatial policy is for local governments and planners to ensure that there is equal distribution of development in the province. We then need to apply the appropriate spatial methods to verify if there is indeed even distribution of wealth geographically. If there is uneven distribution, then the next step is to identify if and where clusters are happening.\nWe continue studying the Hunan Province in China and focus on GDP per capita as the key indicator of development."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-sources",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-sources",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on the Hunan county coming from two files:\n\nHunan county boundary layer in ESRI shapefile format\nHunan local development indicators for 2012 stored in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#installing-and-launching-r-packages",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, tmap, and spdep.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspdep - functions to create spatial weights, autocorrelation statistics\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\nWe also define a random seed value for repeatability of any simulation results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-loading",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-loading",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Hunan shapefile into an R object.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe then use the code chunk below to load the csv file with the indicators into R using read_csv()\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-preparation",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-preparation",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#visualization-of-the-development-indicator",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#visualization-of-the-development-indicator",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package. We present these uas two maps using classes of equal intervals and equal quantiles.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-contiguity-spatial-weights",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-contiguity-spatial-weights",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nPrior to computing LISA’s, we need t construct spatial weights of the study area. Spatial weights are used to define the neighborhood relationship between units. (i.e., neighbors or adjacent units)\nThe code chunk below uses poly2nb() of the spdep package to compute contiguity weight matrices for the study area. The function builds a neighbor list based on regions with shared boundaries. The queen argument takes TRUE (default) or FALSE as options. This instructs the function if Queen criteria should be used in defining neighbors. For the code below, we use the Queen criteria to build the contiguity matrix\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output shows that there are 88 units in the hunan dataset, The most connected unit has 11 neighbors and two units have only one neighbor."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#row-standardised-weights-matrix",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#row-standardised-weights-matrix",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Row-standardised weights matrix",
    "text": "Row-standardised weights matrix\nThe next step is assigning weights to each neighbor. For our case, we assign equal weight (using style=\"W\") to each neighboring polygon. This assigns the fraction 1/n, where n is the number of neighbors a unit has, as the weight of each unit’s neighbor. The drawback of this approach is that polygons in the edge of the study area will base their value on a smaller number of neighbors. This means that we may be potentially over- or under-estimating the true nature of spatial autocorrelation. The alternative more robust style=\"B\" can address this.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-local-morans-i",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-local-morans-i",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Moran’s I",
    "text": "Computing Local Moran’s I\nWe use localmoran() of the spdep package to compute for the local Moran’s I statistic. The function computes for a set of Ii values based on a set of zi values and a listw object which provides the neighbor weight information.\nThe code chunk below computes for the local Moran’s I of the GDPPC variable.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nThe function returns a matrix with the following columns:\n\nIi - the local Moran’s statistic\nE.Ii - the expected value of the statistic under randomisation hypothesis\nV.Ii - the variance of the statistic under randomisation hypothesis\nZ.Ii - the standard deviate of the statistic\nPr(z != E(Ii)) - the p-value of the local Moran statistic\n\nThe code chunk below displays the content of the local Moran matrix by using printCoefmat()\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-the-local-morans-i",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-the-local-morans-i",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Mapping the local Moran’s I",
    "text": "Mapping the local Moran’s I\nBefore mapping, we append the local Moran’s I dataframe to the hunan SpatialPolygonDataFrame using the code chunk below.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nThe code chunk below uses the tmap package to plot the local Moran’s I statistic values.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe map shows evidence of positive and negative Ii values. It is good to consider the p-values for these regions. We use the code chunk below to plot the p-values\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nFor better interpretation, we should consider having these two maps side by side like in the code below.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-moran-scatterplot",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-moran-scatterplot",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting the Moran Scatterplot",
    "text": "Plotting the Moran Scatterplot\nThe Moran scatterplot illustrates the relationship between the value of a chosen attribute against the average of that value across neighbors. The code chunk below uses moran.plot() of spdep package to produce the Moran scatterplot of GDPPC\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\nThe chart is split into quadrants based on the region’s GDPPC and their neighbors’ average or their lagged GDPPC."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-moran-scatterplot-with-standardised-variables",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-moran-scatterplot-with-standardised-variables",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting Moran scatterplot with standardised variables",
    "text": "Plotting Moran scatterplot with standardised variables\nWe can use scale() to center and scale the variable as in the code chunk below. The final function in the code, as.vector(), ensures that we get a vector out of this transformation, that we can then map into our target dataframe.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nWe can then rerun the scatterplot with standardised variables using the code chunk below\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#preparing-lisa-map-classes",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#preparing-lisa-map-classes",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Preparing LISA map classes",
    "text": "Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nWe then derive the spatially lagged variable of interest, GDPPC, and center it around its mean by using the code below.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nWe then center the local Moran’s statistics around the mean\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nWe set a 5% statistical significance level for the local Moran\n\nsignif &lt;- 0.05       \n\nThe code chunk below defines the four different quadrants or categories\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, the code chunk below places the non-significant Moran in category 0 (zero)\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nThe previous steps can be rewritten into a single code chunk below\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-lisa-map",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-lisa-map",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting the LISA map",
    "text": "Plotting the LISA map\nThe code chunk below builds the LISA map using tmap package\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nFor better and more effective interpretation, we can again plot the LISA map and the original GDPPC values side by side using the code chunk below\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#getis-and-ords-g-statistics",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#getis-and-ords-g-statistics",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Getis and Ord’s G-statistics",
    "text": "Getis and Ord’s G-statistics\nThe Getis and Ord’s G-statistics looks at neighbors based on a defined proximity to identify high (hot spots) or low (cold spots) value clusters.\nThe analysis consists of three steps:\n\nDeriving a spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#deriving-distance-based-weight-matrix",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#deriving-distance-based-weight-matrix",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Deriving distance-based weight matrix",
    "text": "Deriving distance-based weight matrix\nFor Getis-Ord, we need to define neighbors based on distance, which can be done by using fixed distance weights or adaptive distance.\n\nDeriving the Centroid\nWe need to define the centroids of each polygon. This consists of multiple steps as we cannot directly use st_centroid() directly on our object for our problem.\nWe first get the longitude values and then map the st_centroid() function on them.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for the latitude values using the code chunk below\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nWith the centroid for the longitude and latitude calculated, we can bind them into a single object using cbind()\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\nDetermining the cut-off distance\nWe then determine the upper limit for the distance bands using the following steps\n\nCreate a matrix with indices of points belonging to k-nearest neighbors using knearneigh() of the spdep package\nConvert the matrix into a neighbors list of class nb by using knn2nb()\nReturn the length of neighbor relationship edges by using nbdists()\nRemove the list structure of the returned object using unlist()\n\nThe code chunk below executes these steps\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\nComputing fixed-distance weight matrix\nWe then compute the distance matrix using dnearneigh() in the code chunk below\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nWe then convert th nb object into a spatial weights object using nb2listw() in the chunk below\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\nComputing adaptive distance matrix\nFixed distance weight matrices will result to units in densely packed areas having more neighbors than less densely packed areas.\nWe can control the numbers of neighbors directly using knn, either accepting asymmetric neighbors or imposing symmetry using the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nWe then convert the nb object into a spatial weights object using the code below\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-fixed-distance",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-fixed-distance",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Gi statistics using fixed distance",
    "text": "Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or G* values.\nThe Gi statistics are represented as Z-scores. Greater values represent grester clustering intensity while the sign indicates the high (positive) or low (negative) clusters.\nWe then join the Gi values with the corresponding units in hunan using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-gi-values-with-fixed-distance-weights",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-gi-values-with-fixed-distance-weights",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Mapping Gi values with fixed distance weights",
    "text": "Mapping Gi values with fixed distance weights\nThe code below maps the Gi values using a fixed distance weight matrix\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap_fix &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap_fix, asp=1, ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-adaptive-distance",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-adaptive-distance",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Gi statistics using adaptive distance",
    "text": "Gi statistics using adaptive distance\nThe code chunk below computes the Gi values for GDPPC by suing an adaptive distance matrix\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\nWe then visualize this (also beside the original GDPPC values) using the code below.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap_ad &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap_ad, \n             asp=1, \n             ncol=2)\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWe show the fixed and adaptive maps side-by-side using the chunk below\n\ntmap_arrange(Gimap_fix,Gimap_ad, asp=1,ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "",
    "text": "We look into the performance of the provinces across Thailand across various tourism indicators like revenue, number of tourists and occupancy rate and see if there is any spatial and spatiotemporal relationship present. We use various techniques to verify spatial randomness, and monotonicity, and where these are violated, identify clusters, outliers and hot or coldspots that can lead to targetted policies to address the problems or learn from the success of such provinces."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.1-background",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.1-background",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.1 Background",
    "text": "A.1 Background\nTourism is a major industry in Thailand as it made up to 20% of their gross domestic product pre-pandemic. However, like the rest of the world, the industry has taken a hit with COVID-19 in 2020, and has slowly been recovering since 2021. Recent reports are stating that Thailand is already, but still, at 80% of its peak level in 2019.\nWhile we speak about the industry in general, the state of tourism within Thailand, and their recovery status are not the same. For example, tourism revenues have been focused on Bangkok, Phuket and Chonburi pre-pandemic.\nWe are interested in understanding the state of tourism across Thailand with regards to its spatial distribution and time and space distribution– both in absolutes and in terms of the trend with respect to the pandemic."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.2-objectives",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.2-objectives",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nFor this study, we want to understand the state of tourism in Thailand at a provincial level, and answer the following questions:\n\nAre the key tourism indicators in Thailand (at a province level) independent from space and from space and time?\nIf tourism or any tourism indicators are not independent, what are the clusters, outliers and emerging hotspots and coldspots?\n\nWe will use the appropriate packages in R in order to perform the different analysis (spatial and otherwise) to support our answers to the above questions."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.3-data-sources",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.3-data-sources",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe following data sources are used for this analysis:\n\nThailand Domestic Tourism Statistics from Kaggle covering the years 2019-2023 and are at province and month level across 8 indicators:\n\nno_tourist_all - total number of domestic tourists\nno_tourist_foreign - number of foreign tourists\nno_tourist_occupied - number of hotel rooms occupied\nno_tourist_thai - number of Thai tourists\noccupancy_rate - the percentage of occupied travel accommodations (hotel rooms)\nrevenue_all - total tourism revenue, in M-THB (appears as net profit in the raw data)\nrevenue_foreign - revenue generated by foreign tourists, in M-THB (appears as net profit in the raw data)\nrevenue_thai - revenue generated by Thai tourists, in M-THB (appears as net profit in the raw data)\n\nThailand-Subnational Administrative Boundaries from Human Data Exchange in shapefile format"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.4-importing-and-launching-r-packages",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.4-importing-and-launching-r-packages",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, the following R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nsf - package for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - package with functions for plotting cartographic quality maps\nsfdep - for handling spatial data\ncoorplot, ggpubr, heatmaply, factoextra - packages for multivariate data visualization and analysis\ncluster, ClustGeo, NbClust - packages for performing cluster analysis\n\n\n\n\npacman::p_load(sf, tmap, spdep, sfdep, tidyverse,\n               ggpubr, heatmaply, factoextra,\n               NbClust, cluster, ClustGeo)\n\n\n\n\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n\nset.seed(1234)"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this exercise, we are introduced to the sfdep package which is a wrapper on spdep and enables us to work directly with sf objects. It is also written in such a way to fully take advantage of the tidyverse framework."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#data-preparation",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#data-preparation",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualization-of-the-development-indicator",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualization-of-the-development-indicator",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package.\n\ntm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Hunan GDP per capita\")"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-1-computing-deriving-queens-contiguity-weights",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-1-computing-deriving-queens-contiguity-weights",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 1: Computing Deriving Queen’s Contiguity Weights",
    "text": "Step 1: Computing Deriving Queen’s Contiguity Weights\nWe use the code chunk below to compute for the contiguity weight matrix using Queen’s criterion.\n\nwm_q &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style=\"W\"),\n         .before=1)\n\nThe st_weights() function allows three arguments:\n\nnb -\nstyle -\nallow_zero -"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-2a-performing-global-morans-i-test",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-2a-performing-global-morans-i-test",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 2a: Performing Global Moran’s I Test",
    "text": "Step 2a: Performing Global Moran’s I Test\nThe Global Moran’s I test can be performed using global_moran_test() of the sfdep package.\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nAt α=0.05, the test shows that we reject a null hypothesis that the GDPPC values are randomly distributed. As the test statistic is above 0, then the data is showing signs of clustering."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-2b-performing-global-morans-i-permutation-test",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-2b-performing-global-morans-i-permutation-test",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 2b: Performing Global Moran’s I Permutation Test",
    "text": "Step 2b: Performing Global Moran’s I Permutation Test\nMonte Carlo simulation on the (Global Moran’s I) statistic is performed using global_moran_perm() of the sfdep package. The code chunk below performs 100 simulations (nsim + 1)\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nWe get consistent result with the one-time run, but with a lower p-value. (and higher confidence)"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-morans-i",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-morans-i",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Moran’s I",
    "text": "Computing Local Moran’s I\nWe compute for the local Moran’s I statistic for each unit by using local_moran() of sfdep package. The unnest() function expands the elements of list local_moran as separate columns in the lisa object.\n\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe can examine the columns of lisa using the code chunk below.\n\nglimpse(lisa)\n\nRows: 88\nColumns: 21\n$ ii           &lt;dbl&gt; -1.468468e-03, 2.587817e-02, -1.198765e-02, 1.022468e-03,…\n$ eii          &lt;dbl&gt; 0.0017692414, 0.0064149158, -0.0374068734, -0.0000348833,…\n$ var_ii       &lt;dbl&gt; 4.179959e-04, 1.051040e-02, 1.020555e-01, 4.367565e-06, 1…\n$ z_ii         &lt;dbl&gt; -0.15836231, 0.18984794, 0.07956903, 0.50594053, 0.448752…\n$ p_ii         &lt;dbl&gt; 0.874171311, 0.849428289, 0.936580031, 0.612898396, 0.653…\n$ p_ii_sim     &lt;dbl&gt; 0.82, 0.96, 0.76, 0.64, 0.50, 0.82, 0.08, 0.08, 0.02, 0.2…\n$ p_folded_sim &lt;dbl&gt; 0.41, 0.48, 0.38, 0.32, 0.25, 0.41, 0.04, 0.04, 0.01, 0.1…\n$ skewness     &lt;dbl&gt; -0.8122108, -1.0905447, 0.8239085, 1.0401038, 1.6357304, …\n$ kurtosis     &lt;dbl&gt; 0.651875433, 1.889177462, 0.046095140, 1.613439800, 3.960…\n$ mean         &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ median       &lt;fct&gt; High-High, High-High, High-High, High-High, High-High, Hi…\n$ pysal        &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ nb           &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1,…\n$ wt           &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0…\n$ NAME_2       &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"C…\n$ ID_3         &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2…\n$ NAME_3       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ ENGTYPE_3    &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"C…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7…\n$ geometry     &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.228…\n\n\nThe local_moran() function generated 12 columns– which are the first twelve in the lisa dataframe. Key columns are:\n\nii - local Moran i statistic\np_ii_sim - p value from simulation\nFor the clustering / outlier classification, there are three options in different columns: mean, median, pysal."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-local-moran-is",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-local-moran-is",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualising Local Moran I’s",
    "text": "Visualising Local Moran I’s\nThe code chunk below prepares a choropleth map of the statistic in the ii and the p_ii_sim field\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(c(\"ii\", \"p_ii_sim\"), title = c(\"Local Moran's I\",\"P Value\")) +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"Local Moran's I and P-values\")\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#lisa-map",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#lisa-map",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "LISA map",
    "text": "LISA map\nA LISA map is a categorical map showing outliers and clusters.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-gi-statistics",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-gi-statistics",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Gi* Statistics",
    "text": "Computing Local Gi* Statistics\nThe code below computes the weight matrix using inverse distance.\n\nwm_idw &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before=1)\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wts = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nWe then compute the local Gi* by using the code below.\n\nHCSA &lt;- wm_idw %&gt;%\n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0416 Low     0.0114 0.00000641  0.0493 9.61e-1  0.7          0.35    0.875\n 2 -0.333  Low     0.0106 0.00000384 -0.0941 9.25e-1  1            0.5     0.661\n 3  0.281  High    0.0126 0.00000751 -0.151  8.80e-1  0.9          0.45    0.640\n 4  0.411  High    0.0118 0.00000922  0.264  7.92e-1  0.6          0.3     0.853\n 5  0.387  High    0.0115 0.00000956  0.339  7.34e-1  0.62         0.31    1.07 \n 6 -0.368  High    0.0118 0.00000591 -0.583  5.60e-1  0.72         0.36    0.594\n 7  3.56   High    0.0151 0.00000731  2.61   9.01e-3  0.06         0.03    1.09 \n 8  2.52   High    0.0136 0.00000614  1.49   1.35e-1  0.2          0.1     1.12 \n 9  4.56   High    0.0144 0.00000584  3.53   4.17e-4  0.04         0.02    1.23 \n10  1.16   Low     0.0104 0.00000370  1.82   6.86e-2  0.12         0.06    0.416\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-gi",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-gi",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualising Gi*",
    "text": "Visualising Gi*\nThe code chunk\n\ntm_shape(HCSA) +\n  tm_polygons()+\ntm_shape(filter(HCSA,p_sim &lt; 0.05)) +\n  tm_polygons(c(\"cluster\",\"p_sim\"), title=c(\"Cluster\",\"P-Value\"))"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "",
    "text": "In this hands-on exercise, we apply hierarchical cluster analysis and spatially constrained cluster analysis to delineate homogeneous regions based on geographically referenced data.\nThis exercise is based on Chapter 12 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#analytical-question",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Analytical Question",
    "text": "Analytical Question\nIn the development of spatial policy and for business, it is often important to segregate homogenous regions using multivariate data. We apply techniques in the study of Shan State in Myanmar by using various indicators."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-sources",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-sources",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on information for Myanmar and for its Shan state:\n\nMyanmar township boundary data in ESRI shapefile format (polygon)\nShan state ICT indicators for 2014 contained in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#installing-and-launching-r-packages",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of thirteen R packages:\n\nsf, rgdal, spdep - for spatial data handling\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\ncoorplot, ggpubr, heatmaply - packages for multivariate data visualization and analysis\ncluster, ClustGeo - packages for performing cluster analysis\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\nWe also define a random seed value for repeatability where of any randmoized results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Myanmar township boundary shapefile into an R object. The code chunk includes a pipeline to already filter to the Shan state and include only the relevant columns.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex09\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-boundary",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-boundary",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Data Loading - Shan state boundary",
    "text": "Data Loading - Shan state boundary\nThe code chunk below uses st_read() of the sf package to load the Myanmar township boundary shapefile into an R object. The code chunk includes a pipeline to already filter to the Shan state and include only the relevant columns.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex09\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nWe can inspect the contents of shan_sf using the code chunk below\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nThe sf dataframe conforms to the tidy framework. Given this, we can also use glimpse() to reveal the fields’ data types.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-2014-indicators-aspatial",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-2014-indicators-aspatial",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Data Loading - Shan state 2014 indicators (aspatial)",
    "text": "Data Loading - Shan state 2014 indicators (aspatial)\nThe code chunk below uses read_csv() to load the contents of the csv file into an object ict\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\nRows: 55 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): District Pcode, District Name, Township Pcode, Township Name\ndbl (7): Total households, Radio, Television, Land line phone, Mobile phone,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can use head() to check the first 6 elements of the object,\n\nhead(ict)\n\n# A tibble: 6 × 11\n  `District Pcode` `District Name` `Township Pcode` `Township Name`\n  &lt;chr&gt;            &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;          \n1 MMR014D001       Taunggyi        MMR014001        Taunggyi       \n2 MMR014D001       Taunggyi        MMR014002        Nyaungshwe     \n3 MMR014D001       Taunggyi        MMR014003        Hopong         \n4 MMR014D001       Taunggyi        MMR014004        Hsihseng       \n5 MMR014D001       Taunggyi        MMR014005        Kalaw          \n6 MMR014D001       Taunggyi        MMR014006        Pindaya        \n# ℹ 7 more variables: `Total households` &lt;dbl&gt;, Radio &lt;dbl&gt;, Television &lt;dbl&gt;,\n#   `Land line phone` &lt;dbl&gt;, `Mobile phone` &lt;dbl&gt;, Computer &lt;dbl&gt;,\n#   `Internet at home` &lt;dbl&gt;\n\n\nand summary() to display summary statistics of the numeric columns.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThe dataset contains 11 fields with 55 observations. The numeric fields give the total number of households in each township, and the number of households with the corresponding technology or appliance. (e.g., television, internet connection, etc)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#deriving-new-indicator-variables",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#deriving-new-indicator-variables",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Deriving new indicator variables",
    "text": "Deriving new indicator variables\nUsing the numeric fields directly will be highly biased as it depends on the number of households in the township. (i.e., townships with higher total households are likely to have higher values for all other columns) To overcome this problem, we can derive the penetration rates (PR) of each of the items by computing the number of households with that item per 1000 households. We accomplish this using mutate() from dplyr package in the code below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nWe can use summary() again to display summary statistics on the 6 new columns.\n\nsummary(ict_derived[c(12:17)])\n\n    RADIO_PR          TV_PR         LLPHONE_PR       MPHONE_PR     \n Min.   : 21.05   Min.   :116.0   Min.   :  2.78   Min.   : 36.42  \n 1st Qu.:138.95   1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14  \n Median :210.95   Median :517.2   Median : 37.59   Median :305.27  \n Mean   :215.68   Mean   :509.5   Mean   : 51.09   Mean   :314.05  \n 3rd Qu.:268.07   3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43  \n Max.   :484.52   Max.   :842.5   Max.   :181.49   Max.   :735.43  \n  COMPUTER_PR      INTERNET_PR     \n Min.   : 3.278   Min.   :  1.041  \n 1st Qu.:11.832   1st Qu.:  8.617  \n Median :18.970   Median : 22.829  \n Mean   :24.393   Mean   : 30.644  \n 3rd Qu.:29.897   3rd Qu.: 41.281  \n Max.   :92.402   Max.   :117.985"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#joining-spatial-and-aspatial-data",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#joining-spatial-and-aspatial-data",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Joining spatial and aspatial data",
    "text": "Joining spatial and aspatial data\nFor later map preparations, we need to combine the two datasets (geospatial shan_sf, aspatial ict_derived) into a single object. We do this using the left_join() function of the dplyr package. Both datasets have a common field TS_PCODE which will be treated as the unique identifier or joining key.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")\n\nThe code includes creation of a new rds file so we can use the following code in the future to read this joined dataset without performing all the steps above.\n\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-statistical-graphics",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-statistical-graphics",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "EDA using statistical graphics",
    "text": "EDA using statistical graphics\nWe can use histograms to visualize the overall distribution of data values– e.g., the shape or skewness. The code chunk below produces on for the field RADIO_PR.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20,  color=\"black\", fill=\"light blue\") +\n  xlab(\"Radio Penetration Rate, per K-HH\") +\n  ylab(\"No. of Townships\")\n\n\n\n\n\n\n\n\nWe can also use boxplots for identifying the median, quartiles, and outliers in the data.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")+\n  xlab(\"Radio Penetration Rate, per K-HH\")\n\n\n\n\n\n\n\n\nWe can create multiple histograms side by side by creating objects for each variable’s histogram, and then laying them out in a grid with ggarange() of the ggpubr package.\n\nCreation of Histogram objectsGrid display of multiple histograms\n\n\n\nradio &lt;- ggplot(data=ict_derived, aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20,color=\"black\", fill=\"light blue\") +\n  xlab(\"Radio PR\") +\n  ylab(\"No. of Townships\")\n\ntv &lt;- ggplot(data=ict_derived, aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"TV PR\") +\n  ylab(\"No. of Townships\")\n\nllphone &lt;- ggplot(data=ict_derived, aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Landline Phone PR\") +\n  ylab(\"No. of Townships\")\n\nmphone &lt;- ggplot(data=ict_derived, aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Mobile Phone PR\") +\n  ylab(\"No. of Townships\")\n\ncomputer &lt;- ggplot(data=ict_derived, aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Computer PR\") +\n  ylab(\"No. of Townships\")\n\ninternet &lt;- ggplot(data=ict_derived, aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Internet PR\") +\n  ylab(\"No. of Townships\")\n\n\n\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, nrow = 2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-choropleth-map",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-choropleth-map",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "EDA using choropleth map",
    "text": "EDA using choropleth map\nThe code chunk below prepares a choropleth map of the Shan state and the Radio penetration rate using qtm()\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\nThe above map is based on the derived penetration rate. We can use choropleth maps to go back to the earliest statement that using the raw variables are likely to be biased on the number of households. We can use the code chunk below to look at them side by side. We use the approach of passing multiple arguments instead of using tmap_arrange()\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"TT_HOUSEHOLDS\", \"RADIO\"),\n          n = 5,style = \"jenks\", \n          title = c(\"Total households\",\"Number Radio\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe above map shows that townships with high number of households with radios, also are towns with the high number of households. We can produce a second map to see if the penetration rate and the total number of households are correlated.\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n          n = 5,style = \"jenks\", \n          title = c(\"Total households\",\"Radio Penetration\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe second pair of maps shows no strong correlation between townships having high number of households and having high radio penetration rates.\nFinally, we can show the six derived variables visually using a similar approach in the code chunk below. The viewer needs to be mindful of the data classes. While the darker the shading means a higher value for that derived variable, the range of values are different between pairs of variables.\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\",\n                  \"MPHONE_PR\", \"COMPUTER_PR\", \"INTERNET_PR\"),\n          n = 5,style = \"jenks\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-and-extracting-cluster-variables",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-and-extracting-cluster-variables",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Selecting and extracting cluster variables",
    "text": "Selecting and extracting cluster variables\nThe code chunk below will be used to extract the clustering variables from the shan_sf dataframe. We have chosen to include COMPUTER_PR rather than INTERNET_PR for the cluster analysis\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nThe next step is to change the row names or indices to the township names rather than the row numbers\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nWe see that the row numbers have been replaced with the township names, however, the township names are now duplicated. We solve this by using the code chunk below\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-standardisation",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-standardisation",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Data standardisation",
    "text": "Data standardisation\nMultiple variables will usually have different range of values. If we use them as is for cluster analysis, then the clustering will be biased towards variables with larger values. It is useful to standardise the clustering variables to reduce the risk of this occuring.\n\nMin-max standardisation\nThe code chunk below uses normalize() of heatmaply package to standardise the clustering variables using min-max method. We then use summary() to show that the ranges of each variable have transformed to [0,1]\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\n\n\nZ-score standardisation\nWe can perform z-score standardisation by using scale() of Base R. We use describe() of psych package to display some statistics of the standardised columns. These show that each of the variables have been transformed to have a mean of 1 and a standard deviation of 1\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\n\n\nVisualising the standardised clustering variables\nAside from viewing the statistics of the standardised variables, it is also good practice to visualise their distribution graphically.\nThe code chunk below produces histograms to show the RADIO_PR field without and with standardisation\n\nr &lt;- ggplot(data=ict_derived, aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\nAlternatively, we can view these as density plots using the code below.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-the-proximity-matrix",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-the-proximity-matrix",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing the proximity matrix",
    "text": "Computing the proximity matrix\nThere are many packages in R that provide functions to compute for the distance matrix. We will use dist() for our case.\nThis function supports six distance calculations: euclidean (default), maximum, manhattan, canberra, binary and minkowski. The code chunk below is used to compute the proximity matrix using the euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below displays the content of proxmat for inspection\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-hierarchical-clustering",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-hierarchical-clustering",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing hierarchical clustering",
    "text": "Computing hierarchical clustering\nThere are several packages in R that can perform hierarchical clustering. In this exercise, we use hclust() of R stats.\nhlcust() employs agglomeration method to compute clusters. Eight clustering algorithms are supported: (1) ward.D, (2) ward.D2, (3) single, (4) complete, (5) average(UPGMA), (6) mcquitty(WPGMA), (7) median(WPGMC), and (8) centroid (UPGMC)\nThe code chunk below performs hierarchical clustering using ward.D method. The output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nOnce ran, we can plot the resulting object as tree by using plot()\n\nplot(hclust_ward, cex = 0.6)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-the-optimal-clustering-algorithm",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-the-optimal-clustering-algorithm",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Selecting the optimal clustering algorithm",
    "text": "Selecting the optimal clustering algorithm\nA challenge in performing hierarchical clustering is identifying strong clustering structures. This can be solved by using agnes() of the cluster package. The function acts like hclust(), but can also get the agglomerative coefficients– or the measure of the strength of the clustering structure. (with a value of 1 indicating a strong structure)\nThe code chunk below computes the agglomerative coefficient of all algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nThe output above shows that Ward’s method provides the best coefficient, and therefore the strongest cluster, among the four methods assessed. We will then focus on this method in succeeding analyses."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#determining-optimal-clusters",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#determining-optimal-clusters",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Determining optimal clusters",
    "text": "Determining optimal clusters\nAnother challenge in cluster analysis is determining the number of clusters to retain. For this, there are three commonly used methods to determine the number of clusters:\n\nElbow method\nAverage Silhouette method\nGap Statistic Method\n\n\nGap statistic method\nThe gap statistic compares intra-cluster variation for different values of k with their expected values under null reference data distribution. The optimal cluster will be the one that maximizes the gap statistic– meaning that the optimal cluster is the farthest from representing a random distribution of points.\nWe use the code chunk below to compute the gap statistic using clusGap() of cluster package. One of the arguments, FUN, use the hcut function which comes from factoextra package indicating that hierarchical clustering is used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nWe can then visualize the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\nWhile the chart above shows that k=1 cluster(s) gives the highest gap statistic, it is not logical. Aside from k=1, we see that k=6 clusters gives the largest statistic and would be the best number of clusters to pick.\nIn addition to the above, the NbClust package provides 30 indices for determining the optimal number of clusters."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#interpreting-the-dendograms",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#interpreting-the-dendograms",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Interpreting the dendograms",
    "text": "Interpreting the dendograms\nEach leaf in the dendogram represents one observation. (townships in our example) Moving up the dendogram, leaves are combined into similar ones using branches. The heights of the fusion indicates the dissimilarity between the two observations, i.e., the higher the height the larger the difference between the two observations. The horizontal axis does not provide any information on the similarity or dissimilarity of pairs of observations.\nThe dendogram can be redrawn with a border around selected (number of) clusters by using rect.hclust() of R stat. The border argument specifies the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visually-driven-hierarchical-clustering-analysis",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visually-driven-hierarchical-clustering-analysis",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Visually-driven hierarchical clustering analysis",
    "text": "Visually-driven hierarchical clustering analysis\nIn this section, we use heatmaply package to perform visually driven hierarchical clustering analysis. With this package, we can build interactive or static cluster heatmaps.\n\nTransforming dataframe into a matrix\nTo create a heatmap, the data needs to be in a matrix. We convert the data frame to this format using the code below\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\nPlotting interactive cluster heatmap using heatmaply()\nThe code chunk below uses heatmaply() of heatmap package to produce an interactive cluster heatmap\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#mapping-the-clusters-formed",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#mapping-the-clusters-formed",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Mapping the clusters formed",
    "text": "Mapping the clusters formed\nWe can use cutree() for R base to derive a 6-cluster model. The code below outputs a list object.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nIn order to visualize the clusters, the list first needs to be appended to the shan_sf simple feature object.\nThis is accomplished in the following code chunk in three steps:\n\nThe object is converted to a matrix\ncbind() is used to append the matrix object onto shan_sf as a new sf object\nrename() is then used to rename the appended field as.matrix.groups into CLUSTER\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nWe then use qtm() of tmap package to produce a quick map of Shan state with the clusters\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\nThe plot shows that the resulting clusters are quite fragmented. This is a limitation of performing non-spatial clustering algorithm."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#converting-into-spatialpolygons-dataframe",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#converting-into-spatialpolygons-dataframe",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Converting into SpatialPolygons DataFrame",
    "text": "Converting into SpatialPolygons DataFrame\nThe skater() function can only support sp objects, so conversion into the appropriate type is required\nThe code chunk performs the conversion using as_Spatial() of sf package.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\nclass(shan_sp)\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\""
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-neighbor-list",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-neighbor-list",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing Neighbor list",
    "text": "Computing Neighbor list\nWe then use poly2nd() of spdep package to generate the neighbor list from the polygon dataframe.\n\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can visualize the neighbor lists using the chunks below. We first plot the boundaries based on shan_sf. We follow this with the neighbor list object shan.nd and use the shape centroids to represent nodes for the graph representation. The add=TRUE argument specifies plotting the network on top of the plot of the boundaries.\n\ncoords &lt;- st_coordinates(\n  st_centroid(st_geometry(shan_sf)))\n\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\nplot(shan.nb,\n     coords, \n     col=\"blue\", \n     add=TRUE)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-minimum-spanning-tree",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-minimum-spanning-tree",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing minimum spanning tree",
    "text": "Computing minimum spanning tree\n\nCalculating edge costs\nWe then use nbcosts() of spdep package to compute the cost of each edge. The cost will be the “distance” between nodes.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nThe function computes the dissimilarity between pairs of neighbors across the value of the five variables.\nWe then incorporate these costs into a weights object which is equivalent to converting the neighbor list into a list weights object by specifying lcosts as weights. We achieve this by using nb2listw() of spdep package in the code chunk below. We specify style=\"B\" to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\nComputing the minimum spanning tree\nWe use mstree() of spdep package to compute the minimal spanning tree.\n\nshan.mst &lt;- mstree(shan.w)\n\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the resulting minimum spanning tree has a dimension n-1 (55-1=54) as it consists of n-1 links to connect the nodes that allow traversal from any any pair of nodes.\nWe can display the content of the spanning tree using head() where we see the first six links and their respective weight\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\nThe plot method for MST includes a way to show the observation (numbers/index) in addition to the connecting edge. The resulting plot will be similar to the neighbor list’s except there will only be at most two edges connecting to any node\n\nplot(st_geometry(shan_sf), \n                 border=gray(.5))\nplot.mst(shan.mst, \n         coords, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-spatially-constrained-clusters-using-skater-method",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-spatially-constrained-clusters-using-skater-method",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing spatially constrained clusters using SKATER method",
    "text": "Computing spatially constrained clusters using SKATER method\nWe can use skater() of spdep package to compute spatially constrained clusters as in the code chink below\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe function requires three mandatory arguments:\n\nThe first two columes of the MST (the edge, or pair of nodes)\nthe data matrix\nthe number of cuts which is equal to the number of clusters minus 1\n\nThe resulting object is of class skater. We can examine the contents using the code chunk below\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe first field groups contains the label of the cluster membership for that observation. The next object contains details of the different clusters which include the nodes and edges in that cluster.\nWe can check the cluster assignments using the following code chunk\n\nccs6 &lt;- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can check how many observations are in each cluster using the table() function.\n\ntable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nWe can also plot the pruned tree showing the six clusters. Note that two of the clusters have only one township each so they will not produce a colored edge in the plot (Group 5 with node 23, group 6 with node 3)\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(clust6, \n     coords, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\", \"black\"),\n     cex.circles=0.005, \n     add=TRUE)\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visualizing-the-clusters-in-a-choropleth-map",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visualizing-the-clusters-in-a-choropleth-map",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Visualizing the clusters in a choropleth map",
    "text": "Visualizing the clusters in a choropleth map\nThe code chunk below can be used to plot the clusters derived from SKATER method using tmap package\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\nFor easier comparison, we can put the clusters generated using hierarchical clustering and spatially constrained hierarchical clustering side by side.\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#ward-like-hierarchical-clustering-clustgeo",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#ward-like-hierarchical-clustering-clustgeo",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Ward-like hierarchical clustering: ClustGeo",
    "text": "Ward-like hierarchical clustering: ClustGeo\nThe package contains a function hclustgeo() to perform Ward-like hierarchical clustering similar to hclust()\nThe function only requires the dissimilarity matrix to perform non-spatially constrained clustering as in the code chunk below. Note that the dissimilarity matrix needs to be of class dist, an object generated by dist()\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#mapping-the-clusters-formed-1",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#mapping-the-clusters-formed-1",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Mapping the clusters formed",
    "text": "Mapping the clusters formed\nWe can plot the generated clusters in a shaded map using the code chunks below\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\nAs it is not spatially-constrained, the resulting clusters are quite fragmented"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#spatially-constrained-hierarchical-clustering",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#spatially-constrained-hierarchical-clustering",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Spatially-constrained hierarchical clustering",
    "text": "Spatially-constrained hierarchical clustering\nBefore performing spatially-constrained hierarchical clustering, we use st_distance() of sf package to generate a spatial distance matrix. We use the code chunk below which also include as.dist() to convert the dataframe into a matrix.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNext, we determine a suitable value for the mixing parameter alpha using choicealpha()\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the charts above, we select alpha = 0.3 as the input to hclustgeo()\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nWe then use cutree() to derive the cluster object\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe then join the group list back into the shan_sf polygon feature dataframe using the code chunk below\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can then use qtm() to map the spatially constrained clusters.\n\nqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visualising-individual-clustering-variable",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visualising-individual-clustering-variable",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Visualising individual clustering variable",
    "text": "Visualising individual clustering variable\nThe code chunk below reveals the distribution of a clustering variable (RADIO_PR) based on one of the clustering approaches (non-spatially constrained ClustGeo)\n\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplot reveals that cluster 3 (in this method) has the highest mean radio penetration rate while clusters 4,5, and 6 have the lowest"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#multi-variate-visualisation",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#multi-variate-visualisation",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Multi-variate visualisation",
    "text": "Multi-variate visualisation\nParallel coordinate plots can be used to reveal insights on clustering variables by clusters effectively, We can perform this using ggparcoord() of GGally package.\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThe scale argument of ggparcorr() provides several options to scale the variables:\n\nstd - univariately, scaled as Z value\nrobust - univariately, subtract median and divide by mean absolute deviation\nuniminmax - univariately, scale to [0-1]\nglobalminmax - no scaling is done, range is defined by global min and max values\ncenter - use uniminmax to standardize vertical height, then center at a specified value (scaleSummary parameter)\ncenterObs - use uniminmax to standardize vertical height, then center at a specified observation’s value (centerObsIDparameter)\n\nWe can also compare the clusters by computing for statistics that will complement the visual interpretation. The code chunk below uses group_by() and summarise() of dplyr to derive the mean values for the clustering variables by cluster\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html",
    "title": "Emerging Hot Spot Analysis",
    "section": "",
    "text": "In this exercise, we perform emerging hotspot analysis on the Hunan GDPPC data."
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#deriving-the-spatial-weights",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#deriving-the-spatial-weights",
    "title": "Emerging Hot Spot Analysis",
    "section": "Deriving the spatial weights",
    "text": "Deriving the spatial weights\nWe use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data"
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#computing-gi",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#computing-gi",
    "title": "Emerging Hot Spot Analysis",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nWe use the following chunk to calculate the local Gi* for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- GDPPC_nb %&gt;%\n  group_by(Year) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#performing-mann-kendall-test-on-gi-of-one-location",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#performing-mann-kendall-test-on-gi-of-one-location",
    "title": "Emerging Hot Spot Analysis",
    "section": "Performing Mann-Kendall Test on Gi of one location",
    "text": "Performing Mann-Kendall Test on Gi of one location\nThe code chunk below evaluates Changsha county for trends\n\ncbg &lt;- gi_stars %&gt;%\n  ungroup() %&gt;%\n  filter(County == \"Changsha\") %&gt;%\n  select(County, Year, gi_star)\n\nWe then plot the result using ggplot2 package\n\nggplot(data = cbg,\n       aes(x = Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\n\n\n\n\n\n\nWe can convert this into an interactive plot by passing the chart into ggplotly()\n\np &lt;- ggplot(data = cbg,\n       aes(x = Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\nWe can conduct the Mann-Kendall test on Changsha by using MannKendall() from the Kendall package in the code chunk below\n\n\\(H_0\\) - No monotonic trend on the GDPPC value of Changsha\n\\(H_1\\) - Monotonic trend is present on the GDPPC value of Changsha\n\n\ncbg %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.485 0.00742    66  136.  589.\n\n\nThe output gives the tau value, the significance level (sl) or p-value."
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#mann-kendall-test-dataframe",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#mann-kendall-test-dataframe",
    "title": "Emerging Hot Spot Analysis",
    "section": "Mann-Kendall Test Dataframe",
    "text": "Mann-Kendall Test Dataframe\nThe code chunk below runs the Mann-Kendall test on all counties in Hunan.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(County) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\nhead(ehsa)\n\n# A tibble: 6 × 6\n  County        tau        sl     S     D  varS\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Anhua      0.191  0.303        26  136.  589.\n2 Anren     -0.294  0.108       -40  136.  589.\n3 Anxiang    0      1             0  136.  589.\n4 Baojing   -0.691  0.000128    -94  136.  589.\n5 Chaling   -0.0882 0.650       -12  136.  589.\n6 Changning -0.750  0.0000318  -102  136.  589.\n\n\nNot all the counties are showing statistically significant result (i.e., sl &lt; 0.05)\nWe can examine some of the counties to observe how their Gi* values change overtime.\nThe code chunk below\n\ncountycheck &lt;- gi_stars %&gt;%\n  ungroup() %&gt;%\n  filter(County == \"Changsha\") %&gt;%\n  select(County, Year, gi_star)\n\np &lt;- ggplot(data = countycheck,\n       aes(x = Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)"
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#visualising-ehsa",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#visualising-ehsa",
    "title": "Emerging Hot Spot Analysis",
    "section": "Visualising EHSA",
    "text": "Visualising EHSA\nTo visualize, we first hoing the ehsa results with the sf dataframe\n\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\nWe can then use tmap package to create a filled map based on the classification filtered for statistically significant results.\n\nehsa_sig &lt;- hunan_ehsa %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(ehsa_sig) +\n  tm_fill(\"classification\") +\n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#b.1.-thailand-subnational-boundary-provincial-level",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#b.1.-thailand-subnational-boundary-provincial-level",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "B.1. Thailand Subnational Boundary, Provincial Level",
    "text": "B.1. Thailand Subnational Boundary, Provincial Level\nWe load the Thailand subnational administrative boundary shapefile into an R dataframe using st_read() from the sf package. We need to analyze at the provincial level so we will be using the files suffixed by “1”.\n\nthai_sf &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThe output states that the object is of multipolygon geometry type containing 77 features (provinces, records) across 16 fields. (columns) We can check the contents of the object using a number of methods. For the code chunk below, we use glimpse() which lists the columns, gives the data type and the first elements.\n\nglimpse(thai_sf)\n\nRows: 77\nColumns: 17\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ ADM1_TH    &lt;chr&gt; \"กรุงเทพมหานคร\", \"สมุทรปราการ\", \"นนทบุรี\", \"ปทุมธานี\", \"พระนครศรีอ…\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH…\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",…\n$ ADM0_TH    &lt;chr&gt; \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศ…\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",…\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18…\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22…\n$ validTo    &lt;date&gt; -001-11-30, -001-11-30, -001-11-30, -001-11-30, -001-11-30…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (…\n\n\nFor clarity, we can clean up this dataframe by:\n\nKeeping only relevant columns: The province name and code, geometry\nRenaming the columns: change ADM1 to Province\n\nThe following code chunk executes these steps by using select() for the first step and rename() for the second step. We again use glimpse() to give a preview of the dataset’s columns.\n\nthai_sf &lt;- thai_sf %&gt;%\n  select(ADM1_EN, ADM1_PCODE, geometry) %&gt;%\n  rename(Province = ADM1_EN, ProvCode = ADM1_PCODE)\n\nglimpse(thai_sf)\n\nRows: 77\nColumns: 3\n$ Province &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"Phr…\n$ ProvCode &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH17…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (((…\n\n\nWe can check if there are any missing values by using is.na() and then check across each column using colSums() from Base R.\n\ncolSums(is.na(thai_sf))\n\nProvince ProvCode geometry \n       0        0        0 \n\n\nThe output shows that there are no missing values for any of the retained columns.\nFinally, we can quickly check if the object depicts Thailand properly by producing a quick map using qtm() from tmap package.\n\nqtm(thai_sf)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#b.2.-thailand-tourism-data-by-province-jan-2019---feb-2023",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#b.2.-thailand-tourism-data-by-province-jan-2019---feb-2023",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "B.2. Thailand Tourism Data by Province, Jan 2019 - Feb 2023",
    "text": "B.2. Thailand Tourism Data by Province, Jan 2019 - Feb 2023\nThe code chunk below loads the tourism statistics data into a dataframe tourism. We use read_csv() to import the data from the file.\n\ntourism &lt;- read_csv(\"data/aspatial/thailand_domestic_tourism_2019_2023.csv\")\n\nRows: 30800 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): province_thai, province_eng, region_thai, region_eng, variable\ndbl  (1): value\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can check the contents by using the code chunk below.\n\ntourism\n\n# A tibble: 30,800 × 7\n   date       province_thai province_eng   region_thai region_eng variable value\n   &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n 1 2019-01-01 กรุงเทพมหานคร  Bangkok        ภาคกลาง     central    occupan…  93.4\n 2 2019-01-01 ลพบุรี          Lopburi        ภาคกลาง     central    occupan…  61.3\n 3 2019-01-01 พระนครศรีอยุธยา Phra Nakhon S… ภาคกลาง     central    occupan…  73.4\n 4 2019-01-01 สระบุรี         Saraburi       ภาคกลาง     central    occupan…  67.3\n 5 2019-01-01 ชัยนาท         Chainat        ภาคกลาง     central    occupan…  79.3\n 6 2019-01-01 นครปฐม        Nakhon Pathom  ภาคกลาง     central    occupan…  71.7\n 7 2019-01-01 สิงห์บุรี         Sing Buri      ภาคกลาง     central    occupan…  64.6\n 8 2019-01-01 อ่างทอง        Ang Thong      ภาคกลาง     central    occupan…  71.2\n 9 2019-01-01 นนทบุรี         Nonthaburi     ภาคกลาง     central    occupan…  75.1\n10 2019-01-01 ปทุมธานี        Pathum Thani   ภาคกลาง     central    occupan…  60.8\n# ℹ 30,790 more rows\n\n\nThe imported data contains 7 fields and 30,800 records at a province and month level.\nBefore we analyze the dataset, let use remove unnecessary columns and rename the column names, similar to the previous dataset, using the code chunk below. (by using select() and rename())\n\ntourism &lt;- tourism %&gt;%\n  select(date, province_eng, region_eng, variable, value) %&gt;%\n  rename(Date = date, Province = province_eng, Region = region_eng, Indicator = variable, Value = value)\n\nhead(tourism)\n\n# A tibble: 6 × 5\n  Date       Province                 Region  Indicator      Value\n  &lt;date&gt;     &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2019-01-01 Bangkok                  central occupancy_rate  93.4\n2 2019-01-01 Lopburi                  central occupancy_rate  61.3\n3 2019-01-01 Phra Nakhon Si Ayutthaya central occupancy_rate  73.4\n4 2019-01-01 Saraburi                 central occupancy_rate  67.3\n5 2019-01-01 Chainat                  central occupancy_rate  79.3\n6 2019-01-01 Nakhon Pathom            central occupancy_rate  71.7\n\n\nWe have kept only five of the columns which provides the date, the English descriptions for the location (province and region) as well as the (potential) tourism indicator and its value.\nWe can also check for any missing values across these five columns using the code below. (using is.na() and colSums())\n\ncolSums(is.na(tourism))\n\n     Date  Province    Region Indicator     Value \n        0         0         0         0         0 \n\n\nEach province will be repeated across multiple dates and across multiple indicators. Let us first doublecheck the different values in Indicator. We use unique() in the code chunk below to achieve this.\n\nunique(tourism$Indicator)\n\n[1] \"occupancy_rate\"      \"no_tourist_occupied\" \"no_tourist_all\"     \n[4] \"no_tourist_thai\"     \"no_tourist_foreign\"  \"net_profit_all\"     \n[7] \"net_profit_thai\"     \"net_profit_foreign\" \n\n\nWe are aware that the ‘net_profit’ indicators are actually revenue so it is better to update them now to avoid misunderstanding later. We use recode() from dplyr to replace instances with alternative values.\n\ntourism &lt;- tourism %&gt;%\n  mutate(Indicator = recode(Indicator,\n                            \"net_profit_all\" = \"revenue_all\",\n                            \"net_profit_thai\" = \"revenue_thai\",\n                            \"net_profit_foreign\" = \"revenue_foreign\"))\n\nunique(tourism$Indicator)\n\n[1] \"occupancy_rate\"      \"no_tourist_occupied\" \"no_tourist_all\"     \n[4] \"no_tourist_thai\"     \"no_tourist_foreign\"  \"revenue_all\"        \n[7] \"revenue_thai\"        \"revenue_foreign\"    \n\n\nWe will not define which indicators to use until we perform some EDA (Exploratory Data Analysis) in the next section.\nBefore we move to the next section, we will also introduce some columns into the dataset to make filtering and other analysis easier. For now, we will do this by adding columns for the months and years based on the Date column.\n\ntourism &lt;- tourism %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  mutate(MonthNum = month(Date)) %&gt;%\n  mutate(Month = month(Date, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(MonthYear = format(ymd(Date), \"%Y-%m\"))\n\nhead(tourism)\n\n# A tibble: 6 × 9\n  Date       Province      Region Indicator Value  Year MonthNum Month MonthYear\n  &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt;    \n1 2019-01-01 Bangkok       centr… occupanc…  93.4  2019        1 Jan   2019-01  \n2 2019-01-01 Lopburi       centr… occupanc…  61.3  2019        1 Jan   2019-01  \n3 2019-01-01 Phra Nakhon … centr… occupanc…  73.4  2019        1 Jan   2019-01  \n4 2019-01-01 Saraburi      centr… occupanc…  67.3  2019        1 Jan   2019-01  \n5 2019-01-01 Chainat       centr… occupanc…  79.3  2019        1 Jan   2019-01  \n6 2019-01-01 Nakhon Pathom centr… occupanc…  71.7  2019        1 Jan   2019-01"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.1-tourism-revenue",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.1-tourism-revenue",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.1 Tourism Revenue",
    "text": "C.1 Tourism Revenue\nWe first look at tourism revenue which is currently reported in million Thai baht. We will use a constant rate of 34.784 THB per USD based on 2023 exchange rates to scale down the numbers and transform it into something more recognizable for most of the readers.\nWe first create a plot for the monthly tourism revenue in total and by foreign and local tourists. The code below selects the relevant data and prepares the line plot using ggplot(). Finally, we use ggplotly() to render it as an interactive chart so we can easily examine the resulting chart.\n\n# Subset the data to just the required indicators\naggregated_data &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"revenue_all\", \"revenue_thai\", \"revenue_foreign\")) %&gt;%\n  group_by(MonthYear, Indicator) %&gt;%\n  summarise(TotalValue = sum(Value, na.rm = TRUE) / 34.784) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'MonthYear'. You can override using the\n`.groups` argument.\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thai Tourism Revenue by Month\",\n       y = \"Million USD\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"revenue_all\" = \"blue\", \"revenue_thai\" = \"green\", \"revenue_foreign\" = \"red\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe resulting chart is consistent with the expectation on the impact and recovery from the pandemic. We can view the above chart as a timeline:\n\nUp to (mid)January 2020: Pre-covid. No travel restrictions have been set yet. (Jan 2019 - Jan 2020, 13 months)\nFeb 2020 to November 2021: Covid. Various lockdown measures in place. All foreign non-essential travel is banned. There is some local tourist activity, but another set of measures in May 2021 again prevents non-essential movement (Feb 2020 - Oct 2021, 22 months)\nNovember 2021 onwards: Post-Covid. Travel restrictions have been eased or lifted and tourism revenues have been recovering (Nov 2021 - Feb 2023, 16 months)\n\nPre- and post-covid, we see that foreign tourists contribute more to the overall revenue, and their contribution has a large amount of variance. Local tourists during the same period have contributed a more stable amount month-on-month.\nWe can code the three periods mentioned above into the tourism dataset for convenience. We use the ifelse() function to do this based on the cutoff dates mentioned above.\n\ntourism$Period &lt;- ifelse(tourism$Date &lt; as.Date(\"2020-02-01\"), \"Pre-Covid\",\n                         ifelse(tourism$Date &gt; as.Date(\"2021-10-01\"), \"Post-Covid\", \"Covid\"))\n\nWe can next check the tourism revenue at the province level. As plotting all 77 provinces across all the periods will not produce readable charts, we will focus on top 20 provinces for the different periods. We will also take the average monthly revenue rather than the total since each period has a different number of months.\nWe prepare a new dataframe that summarizes the indicators for each province in tourism. Aside from the average revenue per period, we will also compute for the average number of visitors as well as the average spend per visitor.\n\ntourism_period &lt;- tourism %&gt;%\n  group_by(Province) %&gt;%\n  summarise(\n    PreCovid_Revenue_total = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_total = sum(Value[Period == \"Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_total = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/16/34.784,\n    PreCovid_Revenue_foreign = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_foreign = sum(Value[Period == \"Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_foreign = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/16/34.784,\n    PreCovid_Revenue_thai = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_thai = sum(Value[Period == \"Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_thai = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/16/34.784,\n    PreCovid_tourists_total = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/13,\n    Covid_tourists_total = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/22,\n    PostCovid_tourists_total = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/16,\n    PreCovid_tourists_foreign = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/13,\n    Covid_tourists_foreign = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/22,\n    PostCovid_tourists_foreign = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/16,\n    PreCovid_tourists_thai = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/13,\n    Covid_tourists_thai = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/22,\n    PostCovid_tourists_thai = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/16\n  ) %&gt;%\n  mutate(PreCovidSpend_total = PreCovid_Revenue_total / PreCovid_tourists_total * 1000000) %&gt;%\n  mutate(CovidSpend_total = Covid_Revenue_total / Covid_tourists_total * 1000000) %&gt;%\n  mutate(PostCovidSpend_total = PostCovid_Revenue_total / PostCovid_tourists_total * 1000000) %&gt;%\n  mutate(PreCovidSpend_foreign = PreCovid_Revenue_foreign / PreCovid_tourists_foreign * 1000000) %&gt;%\n  mutate(CovidSpend_foreign = Covid_Revenue_foreign / Covid_tourists_foreign * 1000000) %&gt;%\n  mutate(PostCovidSpend_foreign = PostCovid_Revenue_foreign / PostCovid_tourists_foreign * 1000000) %&gt;%\n  mutate(PreCovidSpend_thai = PreCovid_Revenue_thai / PreCovid_tourists_thai * 1000000) %&gt;%\n  mutate(CovidSpend_thai = Covid_Revenue_thai / Covid_tourists_thai * 1000000) %&gt;%\n  mutate(PostCovidSpend_thai = PostCovid_Revenue_thai / PostCovid_tourists_thai * 1000000)\n\nWith the summarized dataframe prepared, we can now prepare a few visualizations to look at the provinces with regards to the average monthly tourism revenue.\nFirst, let us try using a scatterplot to see both the average revenue pre-Covid (x-axis) and post-Covid. (y-axis) Provinces with the highest pre-Covid revenue will appear the rightmost, while those that have the highest post-Covid revenue will appear the rightmost.\nThe code below uses the plotly package to produce an interactive scatterplot of the pre- and post-covid average monthly revenue for all tourists. With the interactive chart, the province names will be visible by hovering over and the user can zoom in by creating a selection in the chart.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_Revenue_total,\n  y = ~PostCovid_Revenue_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue - M-USD',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBaed on the plot, we see that Bangkok, Phuket and Chonburi have consistently been the top 3 highest revenue generating before and after the pandemic. When we look further down the list, we see some shifts for some of the provinces.\nTo aid the reader, we recreate the chart with those top 3 provinces excluded using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = filter(tourism_period, !(Province %in% c(\"Bangkok\", \"Phuket\", \"Chonburi\"))),\n  x = ~PreCovid_Revenue_total,\n  y = ~PostCovid_Revenue_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue - M-USD, exc Top 3 Provinces',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nSome key observations from the above charts are:\n\nChiang Mai has moved from top 5 to top 4. A large reason for this is a drop from Krabi. Pre-covid, Krabi was top 4, but has dropped to at least top 10.\nChiang Rai and Prachuap Khiri Khan have risen to top 5 and 6. These provinces were top 9 or lower before.\nSongkhla and Phang Nga were in the top 10 pre-Covid but are also showing a drop in ranking post-Covid\n\nWe can do the same chart for just the revenue from foreign tourists using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_Revenue_foreign,\n  y = ~PostCovid_Revenue_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue from Foreign Tourists - M-USD',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nThe top 3 provinces are the same for both, but there are differences further down the list.\nWe can also show these numbers graphically in a map. Before we do this, let us add one set of measures in tourism_period to indicate the recovery rate. This will be the ratio of the post-covid and pre-covid measures and will indicate how much of the pre-covid level has been achieved. (on average)\nWe perform this using the code chunk below. We do this for all sets of measures across revenue, number of tourists and the average spending.\n\ntourism_period &lt;- tourism_period %&gt;%\n  mutate(Revenue_total_recovery = PostCovid_Revenue_total / PreCovid_Revenue_total) %&gt;%\n  mutate(Revenue_foreign_recovery = PostCovid_Revenue_foreign / PreCovid_Revenue_foreign) %&gt;%\n  mutate(Revenue_thai_recovery = PostCovid_Revenue_thai / PreCovid_Revenue_thai) %&gt;%\n  mutate(Tourists_total_recovery = PostCovid_tourists_total / PreCovid_tourists_total) %&gt;%\n  mutate(Tourists_foreign_recovery = PostCovid_tourists_foreign / PreCovid_tourists_foreign) %&gt;%\n  mutate(Tourists_thai_recovery = PostCovid_tourists_thai / PreCovid_tourists_thai) %&gt;%\n  mutate(Spend_total_recovery = PostCovidSpend_total / PreCovidSpend_total) %&gt;%\n  mutate(Spend_foreign_recovery = PostCovidSpend_foreign / PreCovidSpend_foreign) %&gt;%\n  mutate(Spend_thai_recovery = PostCovidSpend_thai / PreCovidSpend_thai)\n\nWe need to include all the indicators into the sf dataframe. This means merging the tourism_period and the thai_sf dataframes. Let us first check that the naming is the same for both dataframes by checking which values do not have a match. We use the code below which uses left_join() to match and then filter() to check those that do not have matches.\n\n# Identify mismatched Province names in tourism_period\nmismatched_values &lt;- tourism_period %&gt;%\n  left_join(thai_sf, by = \"Province\") %&gt;%\n  filter(is.na(ProvCode)) %&gt;%\n  select(Province)\n\nmismatched_tourism &lt;- mismatched_values$Province\n\n# Identify mismatched Province names in thai_sf\nmismatched_values &lt;- thai_sf %&gt;%\n  left_join(tourism_period, by = \"Province\") %&gt;%\n  filter(is.na(Covid_Revenue_total)) %&gt;%\n  select(Province)\n\nmismatched_thai &lt;- mismatched_values$Province\n\n# Print the mismatched values\nlist(\n  mismatched_in_tourism_period = mismatched_tourism,\n  mismatched_in_thai_sf = mismatched_thai\n)\n\n$mismatched_in_tourism_period\n[1] \"Buriram\"         \"Chainat\"         \"Chonburi\"        \"Lopburi\"        \n[5] \"Nong Bua Lamphu\" \"Phang Nga\"       \"Prachinburi\"     \"Sisaket\"        \n\n$mismatched_in_thai_sf\n[1] \"Lop Buri\"         \"Chai Nat\"         \"Chon Buri\"        \"Prachin Buri\"    \n[5] \"Buri Ram\"         \"Si Sa Ket\"        \"Nong Bua Lam Phu\" \"Phangnga\"        \n\n\nWe see that there are 8 mismatched province names for each of the dataframes. We need to standardize these namings to ensure that the indicators are mapped to the correct province. We will opt to keep the descriptions from tourism_period which gives more compact naming. We use recode() in the code chunk below to accomplish this in a new dataframe.\n\nthaitourism_sf &lt;- thai_sf %&gt;%\n  mutate(Province = recode(Province,\n                            \"Lop Buri\" = \"Lopburi\",\n                            \"Chai Nat\" = \"Chainat\",\n                            \"Chon Buri\" = \"Chonburi\",\n                            \"Prachin Buri\" = \"Prachinburi\",\n                            \"Buri Ram\" = \"Buriram\",\n                            \"Si Sa Ket\" = \"Sisaket\",\n                            \"Nong Bua Lam Phu\" = \"Nong Bua Lamphu\",\n                            \"Phangnga\" = \"Phang Nga\"))\n\nWe can now use leftjoin() in the codechunk below to merge the two datasets.\n\nthaitourism_sf &lt;- left_join(thaitourism_sf, tourism_period,\n                     by=c(\"Province\"=\"Province\"))\n\nThe code chunk below confirms that the new object is still an sf dataframe.\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can use the tmap package to produce side-by side maps of Thailand with the average monthly tourism revenue before and after covid using the code chunk below. Given the wide range of values, we will use quantiles for the data classes. We also include the recovery rate of each of the provinces as a third map.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_Revenue_total\", \"PostCovid_Revenue_total\", \"Revenue_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Monthly Revenue - M-USD\", \"Monthly Revenue - M-USD\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nAs also seen in the scatterplot, we also see some change in rankings with the maps. For example, some provinces previously in the top 20% have moved down to the next 20%. (e.g., Khon Kaen and Phang Nga)\nIf we focus on the third map, we also see what seems like a cluster of provinces in the south which are lagging with regards to their recovery on the average tourism revenue. We will watch out for these once we conduct our cluster analyses."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.2-number-of-tourists",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.2-number-of-tourists",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.2 Number of Tourists",
    "text": "C.2 Number of Tourists\nThe next measure we can look at is the number of tourists. We produce a similar line chart as we did for tourism revenue with the code chunk below. We display the number of tourists in thousands.\n\n# Subset the data to just the required indicators\naggregated_data &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"no_tourist_all\", \"no_tourist_thai\", \"no_tourist_foreign\")) %&gt;%\n  group_by(MonthYear, Indicator) %&gt;%\n  summarise(TotalValue = sum(Value, na.rm = TRUE) / 1000) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'MonthYear'. You can override using the\n`.groups` argument.\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thailand Number of Tourists by Month\",\n       y = \"Tourists, Thousands\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"no_tourist_all\" = \"blue\", \"no_tourist_thai\" = \"green\", \"no_tourist_foreign\" = \"red\"))\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe trend for the total follows the same general movement as the chart for revenue, however, it looks like tourist numbers are primarily driven by locals than foreigners.\nSimilar to the previous section, we can produce an interactive scatterplot to see the number of tourists each province gets on average before and after the pandemic. We do this first for the total number of tourists.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_tourists_total,\n  y = ~PostCovid_tourists_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid No of Tourists',\n    xaxis = list(title = 'PreCovid Average'),\n    yaxis = list(title = 'PostCovid Average')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBangkok, Chonburi and Phuket had the highest number of visitors pre-Covid, but Phuket appears to have dropped off in favor of Kanchanaburi post-covid.\nWe can also look at the foreign tourists using the code chunk below. We skip local tourists and focus only on foreign ones as they deviate from the overall number.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_tourists_foreign,\n  y = ~PostCovid_tourists_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid No of Foreign Tourists',\n    xaxis = list(title = 'PreCovid Average'),\n    yaxis = list(title = 'PostCovid Average')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBangkok, Phuket and Chonburi appear as the top 3 destinations for foreign tourists (in terms of number) before and after Covid. We also see that Krabi dropped from the top 3 post covid.\nWe can also produce side-by-side maps for the number of tourists and their recovery rates using the code chunk below.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_tourists_total\", \"PostCovid_tourists_total\", \"Tourists_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Monthly Tourists\", \"Monthly Tourists\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWe again see the southern region mostly lagging with regards to their recovery post covid also in terms of the number of visitors."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.3-average-spend-per-visitor",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.3-average-spend-per-visitor",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.3 Average Spend Per Visitor",
    "text": "C.3 Average Spend Per Visitor\nWe next look at the average per spend per visitor which is the quotient of the tourism revenue and the total number of tourists. This will tell us whether tourists are spending more or less around the pandemic.\nFirst we produce a similar line graph as before to look at the trend at an overall picture using the code chunk below.\n\n# Subset the data to just the required indicators\naggregated_data &lt;&lt;- tourism %&gt;%\n  group_by(MonthYear) %&gt;%\n  summarise(\n    Spend_total = sum(Value[Indicator == \"revenue_all\"]) / sum(Value[Indicator == \"no_tourist_all\"]) * 1000000 / 34.784,\n    Spend_thai = sum(Value[Indicator == \"revenue_thai\"]) / sum(Value[Indicator == \"no_tourist_thai\"]) * 1000000 / 34.784,\n    Spend_foreign = sum(Value[Indicator == \"revenue_foreign\"]) / sum(Value[Indicator == \"no_tourist_foreign\"]) * 1000000 / 34.784\n  ) %&gt;%\n  pivot_longer(cols = starts_with(\"Spend\"), names_to = \"Indicator\", values_to = \"TotalValue\") %&gt;%\n  mutate(Indicator = case_when(\n    Indicator == \"Spend_total\" ~ \"Spend_total\",\n    Indicator == \"Spend_thai\" ~ \"Spend_thai\",\n    Indicator == \"Spend_foreign\" ~ \"Spend_foreign\"\n  ))\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thailand Average Spend Per Tourist by Month\",\n       y = \"Average Spend, USD\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"Spend_total\" = \"blue\", \"Spend_thai\" = \"green\", \"Spend_foreign\" = \"red\"))\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe chart shows that the average spend of foreign tourists are much higher than local ones and appears to be the same before and after Covid. There appears to be a shift in the average spend for all tourists overall which is probably driven by an increase in the contribution for the number of local tourists versus foreign tourists. Local tourists appear to show a step decrease after covid as well in terms of their average spending.\nNext, we check the average spending of tourists across provinces using a similar scatterplot as before.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovidSpend_total,\n  y = ~PostCovidSpend_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Average Spend Per Tourist',\n    xaxis = list(title = 'PreCovid Average $'),\n    yaxis = list(title = 'PostCovid Average $')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nPhuket saw the highest average spend pre- and post-covid. Krabi was second highest pre-Covid but drop to third, as Bangkok rose from fourth to second over that period.\nWe can do the same for foreign tourists as the earlier chart showed that it was very different from the total. We use the code chunk below tor produce a similar scatterplot but taking the foreign tourist figures.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovidSpend_foreign,\n  y = ~PostCovidSpend_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Average Spend Per Foreign Tourist',\n    xaxis = list(title = 'PreCovid Average $'),\n    yaxis = list(title = 'PostCovid Average $')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nPhuket, Bangkok and Chonburi have been consistent as top 4 highest spend for foreign tourists. From the chart above, we see that Chiang Rai has risen to the number five spot for highest foreign tourist spending post-Covid.\nWe can also produce a map visualization of the average spend using tmap package. We focus on the total tourist population in the visualization below.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovidSpend_total\", \"PostCovidSpend_total\", \"Spend_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Average Tourist Spending\", \"Average Tourist Spending\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWe again see slow recovery in the southern region, but at the same time, this region has the highest average spending pre- and post-covid. (i.e., top 20%)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.4-occupancy-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.4-occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.4 Occupancy Rate",
    "text": "C.4 Occupancy Rate\nThe final single indicator we will look at is the occupancy rate. We have not prepared and included the data for occupancy rate before as this is a ratio measure which be just summed or averaged. In order to be able to aggregate occupancy rate, we not only need the actual occupancy rate from the data, but we also need the number of rooms occupied which is given by the no_tourist_occupied indicator in the data, and also the number of rooms in total– which is not included in the data.\nThe following code chunk prepares a new dataframe from tourist with the following transformation steps:\n\nRetain MonthYear, Province, and records for occupany rate and number of rooms occupied\nKeep values for the two indicators as separate columns. We use average just in case a province appears multiple times on the same date (to resolve conflicts)\nWe compute the total number of rooms as the number of rooms occupied divided by the occupancy rate\nWe add the tags for the period for pre- and post- covid\n\n\noccupancy_df &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"occupancy_rate\", \"no_tourist_occupied\")) %&gt;%\n  group_by(Date, MonthYear, Province) %&gt;%\n  summarise(\n    occupancy = mean(Value[Indicator == \"occupancy_rate\"], na.rm = TRUE),\n    occupied_rooms = mean(Value[Indicator == \"no_tourist_occupied\"], na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(total_rooms = ifelse(occupancy == 0, 0, occupied_rooms / occupancy * 100))\n\n`summarise()` has grouped output by 'Date', 'MonthYear'. You can override using\nthe `.groups` argument.\n\noccupancy_df$Period &lt;- ifelse(occupancy_df$Date &lt; as.Date(\"2020-02-01\"), \"Pre-Covid\",\n                         ifelse(occupancy_df$Date &gt; as.Date(\"2021-10-01\"), \"Post-Covid\", \"Covid\"))\n\nhead(occupancy_df)\n\n# A tibble: 6 × 7\n  Date       MonthYear Province      occupancy occupied_rooms total_rooms Period\n  &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n1 2019-01-01 2019-01   Amnat Charoen      65.2           8551      13125. Pre-C…\n2 2019-01-01 2019-01   Ang Thong          71.2          19140      26878. Pre-C…\n3 2019-01-01 2019-01   Bangkok            93.4        3334971    3571780. Pre-C…\n4 2019-01-01 2019-01   Bueng Kan          73.0          37974      52055. Pre-C…\n5 2019-01-01 2019-01   Buriram            71.3         113655     159493. Pre-C…\n6 2019-01-01 2019-01   Chachoengsao       59.4          38687      65130. Pre-C…\n\n\nBefore we produce the charts, let us update thaitourism_sf with the aggregated occupancy rates by:\n\nComputing Pre- and Post-covid total number of rooms and occupied rooms per province\nComputing Pre- and Post-covid occupancy rate per province based on step 1\nAdd the new columns into thaitourism_sf using left_join()\n\nThe first two steps are accomplished in the first code block while the third step is accomplished in the second.\n\noccupancy_summary &lt;- occupancy_df %&gt;%\n  group_by(Province) %&gt;%\n  summarise(\n    PreCovid_occupied_rooms = sum(occupied_rooms[Period == \"Pre-Covid\"], na.rm = TRUE),\n    PostCovid_occupied_rooms = sum(occupied_rooms[Period == \"Post-Covid\"], na.rm = TRUE),\n    PreCovid_total_rooms = sum(total_rooms[Period == \"Pre-Covid\"], na.rm = TRUE),\n    PostCovid_total_rooms = sum(total_rooms[Period == \"Post-Covid\"], na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    PreCovid_occupancy = ifelse(PreCovid_total_rooms == 0, 0, (PreCovid_occupied_rooms / PreCovid_total_rooms) * 100),\n    PostCovid_occupancy = ifelse(PostCovid_total_rooms == 0, 0, (PostCovid_occupied_rooms / PostCovid_total_rooms) * 100)\n  )\n\nhead(occupancy_summary)\n\n# A tibble: 6 × 7\n  Province    PreCovid_occupied_ro…¹ PostCovid_occupied_r…² PreCovid_total_rooms\n  &lt;chr&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n1 Amnat Char…                 106667                  91166              189198.\n2 Ang Thong                   208960                 116396              324038.\n3 Bangkok                   39621389               23666935            47956613.\n4 Bueng Kan                   362415                 465507              608851.\n5 Buriram                    1319062                1827050             2137721.\n6 Chachoengs…                 518580                 372576              917461.\n# ℹ abbreviated names: ¹​PreCovid_occupied_rooms, ²​PostCovid_occupied_rooms\n# ℹ 3 more variables: PostCovid_total_rooms &lt;dbl&gt;, PreCovid_occupancy &lt;dbl&gt;,\n#   PostCovid_occupancy &lt;dbl&gt;\n\n\n\nthaitourism_sf &lt;- left_join(thaitourism_sf, occupancy_summary, by = c(\"Province\"=\"Province\"))\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nFinally, we add a column for the recovery of the occupancy rate using the code chunk below.\n\nthaitourism_sf &lt;- mutate(thaitourism_sf, Occupancy_recovery = ifelse(PreCovid_occupancy == 0, 0, (PostCovid_occupancy / PreCovid_occupancy)), .before = -1)\n\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nFor the first visualization, let us plot the occupancy rate at a total level. The code below summarizes based on occupancy_df and computes a national occupancy rate to plot in a line chart.\n\naggregated_data &lt;&lt;- occupancy_df %&gt;%\n  group_by(MonthYear) %&gt;%\n  summarise(\n    occupancy_rate = sum(occupied_rooms, na.rm = TRUE) / sum(total_rooms, na.rm = TRUE) * 100\n  )\n\nggplot(aggregated_data, aes(x = MonthYear, y = occupancy_rate, group = 1)) +\n  geom_line() +\n  labs(\n    title = \"Occupancy Rate Over Time\",\n    x = \"MonthYear\",\n    y = \"Occupancy Rate (%)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThe chart shows that occupancy rate has picked up after October 2021 and appears to have more or less reached pre-covid levels in the most recent months.\nWe can also produce a scatterplot to show pre- and post-covid occupancy rates at a province level. We do this using the code chunk below which produces an interactive plot.\n\nplot &lt;- plot_ly(\n  data = occupancy_summary,\n  x = ~PreCovid_occupancy,\n  y = ~PostCovid_occupancy,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste('Province:', Province, '&lt;br&gt;PreCovid Occupancy:', round(PreCovid_occupancy), '&lt;br&gt;PostCovid Occupancy:', round(PostCovid_occupancy)),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Occupancy Rate (%)',\n    xaxis = list(title = 'PreCovid Occupancy'),\n    yaxis = list(title = 'PostCovid Occupancy')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nThe occupancy rates are showing a dispersed pattern as the overall rankings on occupancy rates have significantly changed pre- and post-covid. Bangkok, Chonburi and Suphan Buri reported the highest occupancy rates before covid, but Nan, Chang Rai and Nakhon Phanom have the highest rates post covid.\nThe next visualization for this measure is a side-by-side map for the pre- and post-covid occupancy rates as well as the recovery rate for occupancy. We use the code chunk below which uses tmap package to produce the maps.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_occupancy\", \"PostCovid_occupancy\", \"Occupancy_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Average Occupancy Rate\", \"Average Occupancy Rate\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nBefore we leave this section, let us try to understand occupancy rate a bit more. Going back to earlier, we want to understand if high occupancy rate post-Covid is being driven by the number of available rooms. To help us answer this, we create a scatterplot of the available rooms against the occupancy rate post-Covid using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = occupancy_summary,\n  x = ~PostCovid_total_rooms,\n  y = ~PostCovid_occupancy,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste('Province:', Province, '&lt;br&gt;Number of Rooms:', round(PostCovid_total_rooms), '&lt;br&gt;Occupancy Rate:', round(PostCovid_occupancy)),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Post-Covid Number of Rooms and Occupancy Rate (%)',\n    xaxis = list(title = 'Number of Rooms'),\n    yaxis = list(title = 'Occupancy Rate')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBelow 10M rooms, there appears to be an upward trend in occupancy rate to the number of rooms. Provinces with more than 10M rooms go against the trend and appear to be capped to 60% occupancy."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.1-variable-selection",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.1-variable-selection",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.1 Variable Selection",
    "text": "D.1 Variable Selection\nWe will turn our attention to the first three measures discussed discussed in the previous section: tourism revenue, number of tourists and average tourist spending. We will not analyse the occupancy rate further as it is highly dependent on the number of rooms.\nWe will focus on checking signs of spatial autocorrelation or association before and after covid, as well as the recovery rate at the overall level for these three indicators– so we will be looking at 9 variables for our analysis."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.2-deriving-the-contiguity-and-weight-matrix",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.2-deriving-the-contiguity-and-weight-matrix",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.2 Deriving the Contiguity and Weight Matrix",
    "text": "D.2 Deriving the Contiguity and Weight Matrix\nFor the tests for this section, we need to derive a neighbor list as well as a weight matrix for each province to its neighbors. Given the presence of islands, we need to use distance rather than contiguity to define neighbors.\nThe first step is to understand the distribution of distances between nearest neighbors to find a proper cut-off distance. The code chunk below\n\nlongitude &lt;- map_dbl(thaitourism_sf$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(thaitourism_sf$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.55   51.92   64.33   63.24   76.77  110.94 \n\n\nThe maximum distance is 110.94 so setting a distance threshold of 111 should ensure that each province should have at least one neighbor. We then produce a nearest neighbor list for each province using dnearneigh()\n\nwm_d111 &lt;- dnearneigh(coords, 0, 111, longlat = TRUE)\nwm_d111\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 350 \nPercentage nonzero weights: 5.903188 \nAverage number of links: 4.545455 \n2 disjoint connected subgraphs\n\n\nWe import this as a new column in our sf object and compute for weights using st_weights() based on this.\n\nwm_thai &lt;- thaitourism_sf %&gt;%\n  mutate(nb = I(wm_d111),\n         wt = st_weights(nb,\n                         style=\"W\"),\n         .before=1)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.3-global-morans-i-test",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.3-global-morans-i-test",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.3 Global Moran’s I Test",
    "text": "D.3 Global Moran’s I Test\nGlobal tests of spatial autocorrelation compares the value of each point/province to the overall value in order to conclude on spatial dependence. For this, we will focus on using Global Moran’s I which will work on the following general hypotheses:\n\n\\(H_0\\) - The value of (variable) is randomly distributed across provinces in Thailand\n\\(H_1\\) - The value of (variable) is not randomly distributed across provinces in Thailand\n\nFurther, the value of the test statistic \\(I\\) will also give indication on the underlying pattern:\n\n\\(I &gt; 0\\) - Clustering; observations tend to be similar\n\\(I &lt; 0\\) - Dispersed / regular; observations tend to be dissimilar\nWhere \\(I\\) close to zero - observations are arranged randomly\n\nTo perform Global Moran’s I test, with permutations, we use global_moran_perm() from sfdep package. We will use a 5% significance level for all the testing to be performed, and we will run 100 permutations / simulations for each test.\n\nD.3.1 Global Moran’s Test on Tourism Revenue\nThe code chunks in the tabs below run the Global Moran’s I permutation test on total tourism revenue (per month) pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid RevenuePost-Covid RevenueRevenue Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovid_Revenue_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.049413, observed rank = 93, p-value = 0.14\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovid_Revenue_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.020361, observed rank = 84, p-value = 0.32\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Revenue_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.43763, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nThe results show no evidence to reject spatial independence for the total revenue pre- and post-Covid. However, it shows signs of clustering for the revenue recovery rate as the p-value is below 0.05 and the statistic is above 1.\n\n\nD.3.2 Global Moran’s Test on Number of Tourists\nThe code chunks in the tabs below run the Global Moran’s I permutation test on total number of tourists (per month) pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid Number of TouristsPost-Covid Number of TouristsNumber of Tourists Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovid_tourists_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.062493, observed rank = 92, p-value = 0.16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovid_tourists_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.10696, observed rank = 95, p-value = 0.1\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Tourists_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.27768, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nWe see similar results here. The results show no evidence to reject spatial independence for the total number of tourists pre- and post-Covid. However, it shows signs of clustering for the number of tourists recovery rate as the p-value is below 0.05 and the statistic is above 1.\n\n\nD.3.3 Global Moran’s Test on Average Tourist Spend\nThe code chunks in the tabs below run the Global Moran’s I permutation test on average tourist spend pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid Average SpendPost-Covid Average SpendAverage Spend Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovidSpend_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.423, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovidSpend_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.20651, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Spend_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.31497, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nAverage tourist spending is showing signs of clustering on all dimensions (pre-Covid, post-Covid and the recovery rate) as the p-value is below 0.05 and the I statistic is above 0 in all cases.\n\n\nD.3.3 Global Moran’s Test Summary\nBased on the results of the testing on the total revenue, number of tourists and spend, we see that the following variables are not exhibiting a random distribution, and show signs of clustering:\n\nTotal tourism revenue recovery rate\nTotal number of tourists recovery rate\nPre-Covid Average spend per Tourist\nPost-Covid Average spend per Tourist\nAverage spend per tourist recovery rate\n\nWe will conduct tests for local association to identify the clusters and outliers among provinces for each of these variables."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.1-analysing-lisa-total-tourism-recovery-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.1-analysing-lisa-total-tourism-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.1 Analysing LISA: Total tourism recovery rate",
    "text": "E.1 Analysing LISA: Total tourism recovery rate\nWe first compute for the LISA for the recovery rate of the total number of tourists using local_moran() function in the code chunk below. The code below uses 100 simulations to produce the test results.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Revenue_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe can then visualize the test statistic and p-values for each province in a map using tmap package in the code chunk below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(c(\"ii\", \"p_ii_sim\"), title = c(\"Local Moran's I\",\"P Value\")) +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"LISA for Total Revenue Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"grey90\")\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nOutliers are generally provinces where the test statistic is negative, and clusters where it is positive– if they are significant. We see some potential outliers. We can produce a different set of plots to allow us to identify these types of provinces.\nUsing a LISA map, we can show graphically the location of clusters and outliers based on this (tourism revenue recovery rate)\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Revenue Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can also observe the contents of the object lisa_sig to see the statistics for the identified significant provinces. The code chunk below shows each class separately for easier reference.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 6.422716 xmax: 100.3366 ymax: 10.12626\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Krabi               Low-Low (((99.11329 7.489274, 99.11337 7.489274, 99.11343…\n3 Phang Nga           Low-Low (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7…\n4 Phuket              Low-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417…\n5 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n6 Satun               Low-Low (((100.0903 6.425736, 100.09 6.425543, 100.0896 6…\n7 Trang               Low-Low (((99.47579 6.97262, 99.47565 6.972616, 99.47537 …\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.984 ymin: 14.94191 xmax: 101.3582 ymax: 19.63808\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province    mean                                                      geometry\n  &lt;chr&gt;       &lt;fct&gt;                                           &lt;MULTIPOLYGON [°]&gt;\n1 Nan         High-High (((100.8948 19.63432, 100.8952 19.63431, 100.8957 19.63…\n2 Uthai Thani High-High (((99.13905 15.79655, 99.13918 15.79652, 99.13965 15.79…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nWe can summarize the findings from this analysis as:\n\nThere is a cluster of 7 provinces at the south of Thailand that have slower recovery in terms of their average tourism revenue. This includes popular destinations like Phuket and Krabi and their neighboring provinces\nThere is a cluster of 3 provinces in the north that have faster recovery on the same metric. This includes Chiang Rai, Nan and Phayao\nThe analysis revealed two outliers. Chachoengsao has high recovery while neighboring provinces are low. Nakhon Ratchasima has low recovery while neighboring provinces are high\n\nreveal the following"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.2-analysing-lisa-number-of-tourists-recovery-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.2-analysing-lisa-number-of-tourists-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.2 Analysing LISA: Number of tourists recovery rate",
    "text": "E.2 Analysing LISA: Number of tourists recovery rate\nWe compute for the LISA using local_moran() function for the number of tourist recovery rate in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Tourists_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will go straight to producing the LISA map based on the lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"No of Tourist Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nTo aid interpretation, we display results tabularly as before using the code chunk below.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 100.3366 ymax: 10.12626\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Krabi               Low-Low (((99.11329 7.489274, 99.11337 7.489274, 99.11343…\n3 Phang Nga           Low-Low (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7…\n4 Phuket              Low-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417…\n5 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.15364 ymin: 6.422716 xmax: 101.9901 ymax: 13.9767\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province     mean                                                     geometry\n  &lt;chr&gt;        &lt;fct&gt;                                          &lt;MULTIPOLYGON [°]&gt;\n1 Chachoengsao High-Low (((101.0612 13.97613, 101.0625 13.976, 101.0629 13.9760…\n2 Satun        High-Low (((100.0903 6.425736, 100.09 6.425543, 100.0896 6.42572…\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe findings can be summarized as:\n\nThere is a cluster at the south of five provinces with slower recovery with regards to the number of tourists– including Phuket and Krabi (similar to previous metric)\nSatun at the southern part has high recovery rate while its neighboring provinces are lower"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.3-analysing-lisa-pre-covid-average-spend",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.3-analysing-lisa-pre-covid-average-spend",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.3 Analysing LISA: Pre-Covid Average Spend",
    "text": "E.3 Analysing LISA: Pre-Covid Average Spend\nWe compute for the LISA using local_moran() function for the average tourist spending post-Covid in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    PreCovidSpend_total, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Pre-Covid Average Tourist Spend\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 10 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 14.06424 xmax: 105.637 ymax: 18.22037\nGeodetic CRS:  WGS 84\n# A tibble: 10 × 3\n   Province         mean                                                geometry\n   &lt;chr&gt;            &lt;fct&gt;                                     &lt;MULTIPOLYGON [°]&gt;\n 1 Lopburi          Low-Low (((101.3453 15.75254, 101.3457 15.75224, 101.3466 1…\n 2 Sing Buri        Low-Low (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.…\n 3 Chainat          Low-Low (((100.1199 15.41243, 100.121 15.41234, 100.1229 15…\n 4 Ubon Ratchathani Low-Low (((105.0633 16.09675, 105.0634 16.09671, 105.0638 1…\n 5 Yasothon         Low-Low (((104.3952 16.34843, 104.3983 16.34707, 104.4 16.3…\n 6 Khon Kaen        Low-Low (((102.7072 17.08713, 102.708 17.087, 102.7096 17.0…\n 7 Loei             Low-Low (((102.095 18.21708, 102.0962 18.21675, 102.0971 18…\n 8 Roi Et           Low-Low (((104.314 16.43758, 104.3135 16.43452, 104.3137 16…\n 9 Nakhon Sawan     Low-Low (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.…\n10 Suphan Buri      Low-Low (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 99.41499 ymax: 9.478956\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  Province  mean                                                        geometry\n  &lt;chr&gt;     &lt;fct&gt;                                             &lt;MULTIPOLYGON [°]&gt;\n1 Krabi     High-High (((99.11329 7.489274, 99.11337 7.489274, 99.11343 7.48929…\n2 Phang Nga High-High (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7.744467,…\n3 Phuket    High-High (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.47851…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe findings can be summarized as:\n\nThere are multiple clusters, totaling 10 provinces, in the center of Thailand that have low average spending pre-Covid. These are composed of lesser known tourist destinations\nPhuket and Phang Nga make up a two-province cluster with high average tourist spending\nThere are no outliers identified in the analysis"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.4-analysing-lisa-post-covid-average-spend",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.4-analysing-lisa-post-covid-average-spend",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.4 Analysing LISA: Post-Covid Average Spend",
    "text": "E.4 Analysing LISA: Post-Covid Average Spend\nWe compute for the LISA using local_moran() function for the average tourist spending post-Covid in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    PostCovidSpend_total, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Post-Covid Average Tourist Spend\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe again display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 14.06424 xmax: 105.637 ymax: 18.22037\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  Province         mean                                                 geometry\n  &lt;chr&gt;            &lt;fct&gt;                                      &lt;MULTIPOLYGON [°]&gt;\n1 Sing Buri        Low-Low (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.0…\n2 Ubon Ratchathani Low-Low (((105.0633 16.09675, 105.0634 16.09671, 105.0638 16…\n3 Loei             Low-Low (((102.095 18.21708, 102.0962 18.21675, 102.0971 18.…\n4 Roi Et           Low-Low (((104.314 16.43758, 104.3135 16.43452, 104.3137 16.…\n5 Mukdahan         Low-Low (((104.2527 16.89302, 104.2527 16.89274, 104.2527 16…\n6 Nakhon Sawan     Low-Low (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.1…\n7 Suphan Buri      Low-Low (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.0…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 99.41499 ymax: 9.478956\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  Province  mean                                                        geometry\n  &lt;chr&gt;     &lt;fct&gt;                                             &lt;MULTIPOLYGON [°]&gt;\n1 Krabi     High-High (((99.11329 7.489274, 99.11337 7.489274, 99.11343 7.48929…\n2 Phang Nga High-High (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7.744467,…\n3 Phuket    High-High (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.47851…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe results very closely resember the ones for Pre-Covid average spending. One significant change is that the high spend cluster at the south now includes Krabi. (so it now consists of three provinces)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.5-analysing-lisa-average-spend-recovery-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.5-analysing-lisa-average-spend-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.5 Analysing LISA: Average Spend Recovery Rate",
    "text": "E.5 Analysing LISA: Average Spend Recovery Rate\nWe compute for the LISA using local_moran() function for the average tourist spending recovery rate in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Spend_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Average Tourist Spend Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe again display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.32594 ymin: 5.613038 xmax: 101.7248 ymax: 10.78906\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n3 Ranong              Low-Low (((98.35294 9.440758, 98.35316 9.440558, 98.3533 …\n4 Trang               Low-Low (((99.47579 6.97262, 99.47565 6.972616, 99.47537 …\n5 Pattani             Low-Low (((101.2827 6.952051, 101.2839 6.95182, 101.2848 …\n6 Yala                Low-Low (((101.2927 6.681118, 101.2937 6.679529, 101.2939…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.01629 ymin: 15.3183 xmax: 101.7972 ymax: 17.178\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province       mean                                                   geometry\n  &lt;chr&gt;          &lt;fct&gt;                                        &lt;MULTIPOLYGON [°]&gt;\n1 Kamphaeng Phet High-High (((99.48875 16.91044, 99.48883 16.91016, 99.48884 16…\n2 Phetchabun     High-High (((101.3987 17.17792, 101.399 17.17781, 101.3993 17.…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.25791 ymin: 7.478502 xmax: 98.48333 ymax: 8.200333\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  Province mean                                                         geometry\n  &lt;chr&gt;    &lt;fct&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n1 Phuket   High-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.478513,…\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.42 ymin: 14.64684 xmax: 101.4044 ymax: 15.75613\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  Province mean                                                         geometry\n  &lt;chr&gt;    &lt;fct&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n1 Lopburi  Low-High (((101.3453 15.75254, 101.3457 15.75224, 101.3466 15.75236,…\n\n\nThe findings can be summarized as:\n\nThere are two clusters at the south, totaling 5 provinces, that have slow recovery. This includes the provinces Surat Thani, Nakhon Si Thammarat, Trang, Pattani and Yala\nThere is a cluster of three provinces at the center that have high recovery rate. This includes Kanpaeng Phet, Phichit and Phetchabun\nPhuket is appearing as an outlier with high tourist spend recovery rate relative to its neighbors."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.1-ehsa---post-covid-tourism-revenue",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.1-ehsa---post-covid-tourism-revenue",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.1 EHSA - Post-covid tourism revenue",
    "text": "F.1 EHSA - Post-covid tourism revenue\nWe perform EHSA on the post-covid (2022 onwards) values for the total tourism revenue.\nWe create post-Covid versions of our datasets using the code chunk below so it is easier to refer to them later. We also include a column for the year and month as integers as EHSA requires discrete numeric values as a time variable.\n\ntourism_postCov &lt;- subset(tourism, Date &gt; as.Date(\"2021-12-31\"))\ntourism_postCov$YYYYMM &lt;- as.integer(format(tourism_postCov$Date, \"%Y%m\"))\n\noccupancy_postCov &lt;- subset(occupancy_df, Date &gt; as.Date(\"2021-12-31\"))\noccupancy_postCov$YYYYMM &lt;- as.integer(format(occupancy_postCov$Date, \"%Y%m\"))\n\n\nF.1.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the post-covid tourism revenue. We use filter() to only select the rows for the measure of interest, and then select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(filter(tourism_postCov, tourism_postCov$Indicator == \"revenue_all\"),\n                           YYYYMM, Province, Value),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.1.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    Value, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.1.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe Mann Kendall test checks for signs of monotonicity for a the local \\(G_i^*\\) statistic. Where the results are significant, the test infers that there are signs of monotonicity for that province / observation.\nThe code chunk below runs the Mann Kendall test (without permutations) on each province for the selected variable using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 33 × 6\n   Province         tau         sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen  0.956 0.00000250    87  91.0  334.\n 2 Bangkok        0.473 0.0215        43  91.0  334.\n 3 Bueng Kan      0.626 0.00217       57  91.0  334.\n 4 Buriram        0.758 0.000197      69  91.0  334.\n 5 Chaiyaphum     0.780 0.000127      71  91.0  334.\n 6 Chanthaburi    0.626 0.00217       57  91.0  334.\n 7 Kalasin        0.802 0.0000809     73  91.0  334.\n 8 Kamphaeng Phet 0.890 0.0000119     81  91.0  334.\n 9 Lampang        0.626 0.00217       57  91.0  334.\n10 Lamphun        0.956 0.00000250    87  91.0  334.\n# ℹ 23 more rows\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 21 × 6\n   Province        tau        sl     S     D  varS\n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ang Thong    -0.582 0.00442     -53  91.0  334.\n 2 Chachoengsao -0.802 0.0000809   -73  91.0  334.\n 3 Chainat      -0.626 0.00217     -57  91.0  334.\n 4 Chiang Rai   -0.429 0.0375      -39  91.0  334.\n 5 Chumphon     -0.890 0.0000119   -81  91.0  334.\n 6 Kanchanaburi -0.890 0.0000119   -81  91.0  334.\n 7 Mae Hong Son -0.429 0.0375      -39  91.0  334.\n 8 Mukdahan     -0.473 0.0215      -43  91.0  334.\n 9 Phetchabun   -0.824 0.0000510   -75  91.0  334.\n10 Phetchaburi  -0.802 0.0000809   -73  91.0  334.\n# ℹ 11 more rows\n\n\nThe results show that 33 of the 77 provinces are showing significant positive trend– which might be expected as provinces are recovering post-Covid. However, there are 21 provinces which are showing a significant negative trend which might be a concern if any of these are expected to be major tourist destinations.\n\n\nF.1.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"Value\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourism Revenue\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n  Province  classification         geometry                      \n1 \"Bangkok\" \"intensifying hotspot\" MULTIPOLYGON (((100.6139 13...\n   Province        classification        geometry                      \n2  \"Nonthaburi\"    \"consecutive hotspot\" MULTIPOLYGON (((100.3415 14...\n8  \"Khon Kaen\"     \"consecutive hotspot\" MULTIPOLYGON (((102.7072 17...\n10 \"Sakon Nakhon\"  \"consecutive hotspot\" MULTIPOLYGON (((103.5404 18...\n11 \"Nakhon Phanom\" \"consecutive hotspot\" MULTIPOLYGON (((104.192 18....\n15 \"Kanchanaburi\"  \"consecutive hotspot\" MULTIPOLYGON (((98.58631 15...\n   Province                   classification     \n3  \"Phra Nakhon Si Ayutthaya\" \"sporadic coldspot\"\n4  \"Sing Buri\"                \"sporadic coldspot\"\n5  \"Trat\"                     \"sporadic coldspot\"\n6  \"Chachoengsao\"             \"sporadic coldspot\"\n7  \"Buri Ram\"                 \"sporadic coldspot\"\n9  \"Nong Khai\"                \"sporadic coldspot\"\n12 \"Mukdahan\"                 \"sporadic coldspot\"\n13 \"Lamphun\"                  \"sporadic coldspot\"\n14 \"Lampang\"                  \"sporadic coldspot\"\n16 \"Prachuap Khiri Khan\"      \"sporadic coldspot\"\n17 \"Ranong\"                   \"sporadic coldspot\"\n18 \"Phatthalung\"              \"sporadic coldspot\"\n19 \"Pattani\"                  \"sporadic coldspot\"\n   geometry                      \n3  MULTIPOLYGON (((100.5131 14...\n4  MULTIPOLYGON (((100.3691 15...\n5  MULTIPOLYGON (((102.5216 11...\n6  MULTIPOLYGON (((101.0612 13...\n7  MULTIPOLYGON (((102.9303 15...\n9  MULTIPOLYGON (((103.2985 18...\n12 MULTIPOLYGON (((104.2527 16...\n13 MULTIPOLYGON (((99.18821 18...\n14 MULTIPOLYGON (((99.58445 19...\n16 MULTIPOLYGON (((99.56326 11...\n17 MULTIPOLYGON (((98.35294 9....\n18 MULTIPOLYGON (((99.96416 7....\n19 MULTIPOLYGON (((101.2827 6....\n\n\nThe results identifies Bangkok as an intensifying hotspot, and six others as consecutive hotspots.\nThere are 13 provinces identified as sporadic coldspots which are locations that are cold spots for less than 90% of the time, but never identified as significant hotspots."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.2-ehsa---post-covid-number-of-tourists",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.2-ehsa---post-covid-number-of-tourists",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.2 EHSA - Post-covid number of tourists",
    "text": "F.2 EHSA - Post-covid number of tourists\nWe perform EHSA on the post-covid (2022 onwards) values for the total number of tourists.\n\nF.2.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the post-covid number of tourists. We use filter() to only select the rows for the measure of interest, and then select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(filter(tourism_postCov, tourism_postCov$Indicator == \"no_tourist_all\"),\n                           YYYYMM, Province, Value),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.2.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    Value, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.2.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 14 × 6\n   Province                   tau        sl     S     D  varS\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok                  0.648 0.00150      59  91.0  334.\n 2 Krabi                    0.714 0.000459     65  91.0  334.\n 3 Nakhon Si Thammarat      0.890 0.0000119    81  91.0  334.\n 4 Nonthaburi               0.626 0.00217      57  91.0  334.\n 5 Pattani                  0.648 0.00150      59  91.0  334.\n 6 Phatthalung              0.780 0.000127     71  91.0  334.\n 7 Phra Nakhon Si Ayutthaya 0.604 0.00311      55  91.0  334.\n 8 Phuket                   0.670 0.00102      61  91.0  334.\n 9 Prachuap Khiri Khan      0.648 0.00150      59  91.0  334.\n10 Ratchaburi               0.429 0.0375       39  91.0  334.\n11 Samut Prakan             0.473 0.0215       43  91.0  334.\n12 Surat Thani              0.648 0.00150      59  91.0  334.\n13 Tak                      0.692 0.000688     63  91.0  334.\n14 Trang                    0.451 0.0285       41  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 18 × 6\n   Province            tau        sl     S     D  varS\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Chachoengsao     -0.758 0.000197    -69  91.0  334.\n 2 Kanchanaburi     -0.692 0.000688    -63  91.0  334.\n 3 Khon Kaen        -0.604 0.00311     -55  91.0  334.\n 4 Loei             -0.516 0.0118      -47  91.0  334.\n 5 Mukdahan         -0.648 0.00150     -59  91.0  334.\n 6 Nakhon Nayok     -0.824 0.0000510   -75  91.0  334.\n 7 Nakhon Pathom    -0.495 0.0160      -45  91.0  334.\n 8 Nong Khai        -0.648 0.00150     -59  91.0  334.\n 9 Phetchabun       -0.560 0.00620     -51  91.0  334.\n10 Phetchaburi      -0.780 0.000127    -71  91.0  334.\n11 Samut Sakhon     -0.802 0.0000809   -73  91.0  334.\n12 Samut Songkhram  -0.780 0.000127    -71  91.0  334.\n13 Sing Buri        -0.626 0.00217     -57  91.0  334.\n14 Surin            -0.560 0.00620     -51  91.0  334.\n15 Trat             -0.780 0.000127    -71  91.0  334.\n16 Ubon Ratchathani -0.495 0.0160      -45  91.0  334.\n17 Udon Thani       -0.407 0.0487      -37  91.0  334.\n18 Yasothon         -0.758 0.000197    -69  91.0  334.\n\n\nThe results show that 14 of the 77 provinces are showing significant positive trend. However, there are 18 provinces (more) which are showing a significant negative trend.\n\n\nF.2.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"Value\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Number of Tourists\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n  Province       classification     geometry                      \n1 \"Nonthaburi\"   \"sporadic hotspot\" MULTIPOLYGON (((100.3415 14...\n2 \"Sakon Nakhon\" \"sporadic hotspot\" MULTIPOLYGON (((103.5404 18...\n  Province  classification      geometry                      \n3 \"Lamphun\" \"sporadic coldspot\" MULTIPOLYGON (((99.18821 18...\n6 \"Ranong\"  \"sporadic coldspot\" MULTIPOLYGON (((98.35294 9....\n7 \"Yala\"    \"sporadic coldspot\" MULTIPOLYGON (((101.2927 6....\n  Province       classification        geometry                      \n4 \"Nakhon Sawan\" \"consecutive hotspot\" MULTIPOLYGON (((100.0266 16...\n5 \"Phuket\"       \"consecutive hotspot\" MULTIPOLYGON (((98.31437 7....\n\n\nThe results identifies Phuket and Nakhon Sawan as consecutive hotspots which means they had a single uninterrupted run of being significant hotspots, but have been significant hotspot for less than 90% of the time.\nNonthaburi and Sakon Nakhon are sporadic hotspots, while Lamphun, Ranong and Yala are sporadic coldspots which mean that they have been on-and-off as hot and coldspots for the number of tourists."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.2-ehsa---occupancy-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.2-ehsa---occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.2 EHSA - Occupancy Rate",
    "text": "F.2 EHSA - Occupancy Rate\nWe perform EHSA on the post-covid (2022 onwards) values for the tourist occupancy rates.\n\nF.3.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the occupancy rates. We use select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(occupancy_postCov,\n                           YYYYMM, Province, occupancy),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.3.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    occupancy, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.3.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 17 × 6\n   Province              tau         sl     S     D  varS\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok             0.934 0.00000429    85  91.0  334.\n 2 Buriram             0.604 0.00311       55  91.0  334.\n 3 Chumphon            0.670 0.00102       61  91.0  334.\n 4 Krabi               0.714 0.000459      65  91.0  334.\n 5 Nakhon Si Thammarat 0.824 0.0000510     75  91.0  334.\n 6 Narathiwat          0.626 0.00217       57  91.0  334.\n 7 Nonthaburi          0.714 0.000459      65  91.0  334.\n 8 Phatthalung         0.692 0.000688      63  91.0  334.\n 9 Phuket              0.890 0.0000119     81  91.0  334.\n10 Prachuap Khiri Khan 0.934 0.00000429    85  91.0  334.\n11 Ranong              0.560 0.00620       51  91.0  334.\n12 Ratchaburi          0.604 0.00311       55  91.0  334.\n13 Rayong              0.626 0.00217       57  91.0  334.\n14 Samut Prakan        0.736 0.000302      67  91.0  334.\n15 Songkhla            0.648 0.00150       59  91.0  334.\n16 Surat Thani         0.912 0.00000715    83  91.0  334.\n17 Yala                0.495 0.0160        45  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 23 × 6\n   Province         tau       sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen -0.780 0.000127   -71  91.0  334.\n 2 Chachoengsao  -0.582 0.00442    -53  91.0  334.\n 3 Chainat       -0.473 0.0215     -43  91.0  334.\n 4 Chaiyaphum    -0.736 0.000302   -67  91.0  334.\n 5 Chanthaburi   -0.429 0.0375     -39  91.0  334.\n 6 Kanchanaburi  -0.582 0.00442    -53  91.0  334.\n 7 Lopburi       -0.451 0.0285     -41  91.0  334.\n 8 Nakhon Nayok  -0.604 0.00311    -55  91.0  334.\n 9 Nakhon Pathom -0.648 0.00150    -59  91.0  334.\n10 Nakhon Sawan  -0.451 0.0285     -41  91.0  334.\n# ℹ 13 more rows\n\n\nThe results show that 17 of the 77 provinces are showing significant positive trend. However, there are 23 provinces (more) which are showing a significant negative trend.\n\n\nF.3.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"occupancy\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourist Occupancy Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n   Province                   classification      \n1  \"Nonthaburi\"               \"oscilating hotspot\"\n2  \"Phra Nakhon Si Ayutthaya\" \"oscilating hotspot\"\n5  \"Chanthaburi\"              \"oscilating hotspot\"\n7  \"Sa Kaeo\"                  \"oscilating hotspot\"\n8  \"Loei\"                     \"oscilating hotspot\"\n9  \"Maha Sarakham\"            \"oscilating hotspot\"\n11 \"Sakon Nakhon\"             \"oscilating hotspot\"\n13 \"Lamphun\"                  \"oscilating hotspot\"\n16 \"Nakhon Sawan\"             \"oscilating hotspot\"\n18 \"Kamphaeng Phet\"           \"oscilating hotspot\"\n19 \"Tak\"                      \"oscilating hotspot\"\n20 \"Sukhothai\"                \"oscilating hotspot\"\n21 \"Kanchanaburi\"             \"oscilating hotspot\"\n25 \"Nakhon Si Thammarat\"      \"oscilating hotspot\"\n26 \"Phuket\"                   \"oscilating hotspot\"\n   geometry                      \n1  MULTIPOLYGON (((100.3415 14...\n2  MULTIPOLYGON (((100.5131 14...\n5  MULTIPOLYGON (((102.2517 12...\n7  MULTIPOLYGON (((102.1877 14...\n8  MULTIPOLYGON (((102.095 18....\n9  MULTIPOLYGON (((103.1562 16...\n11 MULTIPOLYGON (((103.5404 18...\n13 MULTIPOLYGON (((99.18821 18...\n16 MULTIPOLYGON (((100.0266 16...\n18 MULTIPOLYGON (((99.48875 16...\n19 MULTIPOLYGON (((97.97318 17...\n20 MULTIPOLYGON (((99.60051 17...\n21 MULTIPOLYGON (((98.58631 15...\n25 MULTIPOLYGON (((99.77467 9....\n26 MULTIPOLYGON (((98.31437 7....\n   Province          classification      geometry                      \n3  \"Sing Buri\"       \"sporadic coldspot\" MULTIPOLYGON (((100.3691 15...\n4  \"Chai Nat\"        \"sporadic coldspot\" MULTIPOLYGON (((100.1199 15...\n10 \"Roi Et\"          \"sporadic coldspot\" MULTIPOLYGON (((104.314 16....\n12 \"Chiang Mai\"      \"sporadic coldspot\" MULTIPOLYGON (((99.52512 20...\n15 \"Phrae\"           \"sporadic coldspot\" MULTIPOLYGON (((100.1597 18...\n17 \"Uthai Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.13905 15...\n23 \"Samut Songkhram\" \"sporadic coldspot\" MULTIPOLYGON (((100.0116 13...\n27 \"Surat Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.96396 9....\n28 \"Songkhla\"        \"sporadic coldspot\" MULTIPOLYGON (((100.5973 7....\n29 \"Satun\"           \"sporadic coldspot\" MULTIPOLYGON (((100.0903 6....\n31 \"Narathiwat\"      \"sporadic coldspot\" MULTIPOLYGON (((101.6323 6....\n   Province       classification         geometry                      \n6  \"Prachin Buri\" \"consecutive coldspot\" MULTIPOLYGON (((101.4881 14...\n24 \"Phetchaburi\"  \"consecutive coldspot\" MULTIPOLYGON (((99.75869 13...\n   Province        classification        geometry                      \n14 \"Uttaradit\"     \"consecutive hotspot\" MULTIPOLYGON (((101.0924 18...\n22 \"Nakhon Pathom\" \"consecutive hotspot\" MULTIPOLYGON (((100.2231 14...\n30 \"Yala\"          \"consecutive hotspot\" MULTIPOLYGON (((101.2927 6....\n\n\nThe results identify a number of cold and hotspots. If we focus on the coldspots:\n\nwe see that Prachin Buri and Phetchaburi are consecutive coldspots\nA list of 11 provinces, including Chiang Mai, are sporadic coldspots"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.3-ehsa---occupancy-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.3-ehsa---occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.3 EHSA - Occupancy Rate",
    "text": "F.3 EHSA - Occupancy Rate\nWe perform EHSA on the post-covid (2022 onwards) values for the tourist occupancy rates.\n\nF.3.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the occupancy rates. We use select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(occupancy_postCov,\n                           YYYYMM, Province, occupancy),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.3.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    occupancy, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.3.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 17 × 6\n   Province              tau         sl     S     D  varS\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok             0.934 0.00000429    85  91.0  334.\n 2 Buriram             0.604 0.00311       55  91.0  334.\n 3 Chumphon            0.670 0.00102       61  91.0  334.\n 4 Krabi               0.714 0.000459      65  91.0  334.\n 5 Nakhon Si Thammarat 0.824 0.0000510     75  91.0  334.\n 6 Narathiwat          0.626 0.00217       57  91.0  334.\n 7 Nonthaburi          0.714 0.000459      65  91.0  334.\n 8 Phatthalung         0.692 0.000688      63  91.0  334.\n 9 Phuket              0.890 0.0000119     81  91.0  334.\n10 Prachuap Khiri Khan 0.934 0.00000429    85  91.0  334.\n11 Ranong              0.560 0.00620       51  91.0  334.\n12 Ratchaburi          0.604 0.00311       55  91.0  334.\n13 Rayong              0.626 0.00217       57  91.0  334.\n14 Samut Prakan        0.736 0.000302      67  91.0  334.\n15 Songkhla            0.648 0.00150       59  91.0  334.\n16 Surat Thani         0.912 0.00000715    83  91.0  334.\n17 Yala                0.495 0.0160        45  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 23 × 6\n   Province         tau       sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen -0.780 0.000127   -71  91.0  334.\n 2 Chachoengsao  -0.582 0.00442    -53  91.0  334.\n 3 Chainat       -0.473 0.0215     -43  91.0  334.\n 4 Chaiyaphum    -0.736 0.000302   -67  91.0  334.\n 5 Chanthaburi   -0.429 0.0375     -39  91.0  334.\n 6 Kanchanaburi  -0.582 0.00442    -53  91.0  334.\n 7 Lopburi       -0.451 0.0285     -41  91.0  334.\n 8 Nakhon Nayok  -0.604 0.00311    -55  91.0  334.\n 9 Nakhon Pathom -0.648 0.00150    -59  91.0  334.\n10 Nakhon Sawan  -0.451 0.0285     -41  91.0  334.\n# ℹ 13 more rows\n\n\nThe results show that 17 of the 77 provinces are showing significant positive trend. However, there are 23 provinces (more) which are showing a significant negative trend.\n\n\nF.3.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"occupancy\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourist Occupancy Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n   Province                   classification      \n1  \"Nonthaburi\"               \"oscilating hotspot\"\n2  \"Phra Nakhon Si Ayutthaya\" \"oscilating hotspot\"\n5  \"Chanthaburi\"              \"oscilating hotspot\"\n7  \"Sa Kaeo\"                  \"oscilating hotspot\"\n8  \"Loei\"                     \"oscilating hotspot\"\n9  \"Maha Sarakham\"            \"oscilating hotspot\"\n11 \"Sakon Nakhon\"             \"oscilating hotspot\"\n13 \"Lamphun\"                  \"oscilating hotspot\"\n16 \"Nakhon Sawan\"             \"oscilating hotspot\"\n18 \"Kamphaeng Phet\"           \"oscilating hotspot\"\n19 \"Tak\"                      \"oscilating hotspot\"\n20 \"Sukhothai\"                \"oscilating hotspot\"\n21 \"Kanchanaburi\"             \"oscilating hotspot\"\n25 \"Nakhon Si Thammarat\"      \"oscilating hotspot\"\n26 \"Phuket\"                   \"oscilating hotspot\"\n   geometry                      \n1  MULTIPOLYGON (((100.3415 14...\n2  MULTIPOLYGON (((100.5131 14...\n5  MULTIPOLYGON (((102.2517 12...\n7  MULTIPOLYGON (((102.1877 14...\n8  MULTIPOLYGON (((102.095 18....\n9  MULTIPOLYGON (((103.1562 16...\n11 MULTIPOLYGON (((103.5404 18...\n13 MULTIPOLYGON (((99.18821 18...\n16 MULTIPOLYGON (((100.0266 16...\n18 MULTIPOLYGON (((99.48875 16...\n19 MULTIPOLYGON (((97.97318 17...\n20 MULTIPOLYGON (((99.60051 17...\n21 MULTIPOLYGON (((98.58631 15...\n25 MULTIPOLYGON (((99.77467 9....\n26 MULTIPOLYGON (((98.31437 7....\n   Province          classification      geometry                      \n3  \"Sing Buri\"       \"sporadic coldspot\" MULTIPOLYGON (((100.3691 15...\n4  \"Chai Nat\"        \"sporadic coldspot\" MULTIPOLYGON (((100.1199 15...\n10 \"Roi Et\"          \"sporadic coldspot\" MULTIPOLYGON (((104.314 16....\n12 \"Chiang Mai\"      \"sporadic coldspot\" MULTIPOLYGON (((99.52512 20...\n15 \"Phrae\"           \"sporadic coldspot\" MULTIPOLYGON (((100.1597 18...\n17 \"Uthai Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.13905 15...\n23 \"Samut Songkhram\" \"sporadic coldspot\" MULTIPOLYGON (((100.0116 13...\n27 \"Surat Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.96396 9....\n28 \"Songkhla\"        \"sporadic coldspot\" MULTIPOLYGON (((100.5973 7....\n29 \"Satun\"           \"sporadic coldspot\" MULTIPOLYGON (((100.0903 6....\n31 \"Narathiwat\"      \"sporadic coldspot\" MULTIPOLYGON (((101.6323 6....\n   Province       classification         geometry                      \n6  \"Prachin Buri\" \"consecutive coldspot\" MULTIPOLYGON (((101.4881 14...\n24 \"Phetchaburi\"  \"consecutive coldspot\" MULTIPOLYGON (((99.75869 13...\n   Province        classification        geometry                      \n14 \"Uttaradit\"     \"consecutive hotspot\" MULTIPOLYGON (((101.0924 18...\n22 \"Nakhon Pathom\" \"consecutive hotspot\" MULTIPOLYGON (((100.2231 14...\n30 \"Yala\"          \"consecutive hotspot\" MULTIPOLYGON (((101.2927 6....\n\n\nThe results identify a number of cold and hotspots. If we focus on the coldspots:\n\nwe see that Prachin Buri and Phetchaburi are consecutive coldspots\nA list of 11 provinces, including Chiang Mai, are sporadic coldspots"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html",
    "title": "Geographically Weighted Regression Model",
    "section": "",
    "text": "In this hands-on exercise, we learn to use GWR or geographically weighted regression. GWR is a technique that takes non-stationary variables and models their relationships to an outcome of interest. We use GWR to build hedonic pricing models for the resale prices of condominiums in Singapore. (from 2015)\nThis exercise is based on Chapter 13 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#data-sources",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#data-sources",
    "title": "Geographically Weighted Regression Model",
    "section": "Data Sources",
    "text": "Data Sources\nTo datasets will be used for this exercise:\n\n2014 Master Plan subzone boundary in shapefile format\n2015 condo resale prices in csv format"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#installing-and-launching-r-packages",
    "title": "Geographically Weighted Regression Model",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of the following R packages:\n\nolsrr - for building OLS (ordinary least squares) regression models and performing diagnostic tests\nGWmodel - for calibrating geographically weighted family of models\ntmap - for plotting cartographic quality maps\ncorrplot - for multivariate data visualization and analysis\nsf - spatial data handling\ntidyverse - attribute data handling\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#geospatial-data-loading-and-preparation",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#geospatial-data-loading-and-preparation",
    "title": "Geographically Weighted Regression Model",
    "section": "Geospatial data loading and preparation",
    "text": "Geospatial data loading and preparation\nThe code chunk below uses st_read() of the sf package to load the geospatial data. (master plan boundaries) This data is in svy21 projected coordinate systems.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex10\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nAs the new object does not have EPSG information, we will use the following code with st_transform() to apply the correct code of 3414.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\nmpsz_svy21 &lt;- st_make_valid(mpsz_svy21)\n\nWe can use st_crs() to verify that the operation was successful.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nWe can use st_bbox() to reveal the limits of the bounding box or the extent of the sf object.\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#aspatial-data-loading",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#aspatial-data-loading",
    "title": "Geographically Weighted Regression Model",
    "section": "Aspatial data loading",
    "text": "Aspatial data loading\nThe code chunk below uses read_csv() of readr to import the 2015 condo resale prices from the csv file.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can verify that the load is successful and get an idea of the data structure by using a function like glimpse()\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nWe can use head() to inspect the first few (default 6) elements. We can use it for select columns/fields, as we do in the next code chunk for longitude and latitude.\n\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nWe can use summary() of base R to display summary statistics across columns in the same dataframe.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#converting-aspatial-dataframe-into-an-sf-object",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#converting-aspatial-dataframe-into-an-sf-object",
    "title": "Geographically Weighted Regression Model",
    "section": "Converting aspatial dataframe into an sf object",
    "text": "Converting aspatial dataframe into an sf object\nTo convert the condo_resale object into a spatial object, we can use the following code chunk that utilizes st_as_sf() from sf package. The final line of the code chunk converts the data frame from wgs84 to svy21 using the indicated crs values.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nWe can again use head() to inspect the first few elements of the new object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#eda-using-statistical-graphics",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#eda-using-statistical-graphics",
    "title": "Geographically Weighted Regression Model",
    "section": "EDA using statistical graphics",
    "text": "EDA using statistical graphics\nWe can produce a histogram of the selling price by using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Distribution of Resale Selling Price\") +\n  labs(x = \"Selling Price\", y = \"Records\")\n\n\n\n\n\n\n\n\nThe figure shows a right-skewed distribution for price– that more units were sold at lower prices.\nSkewed distributions are undesirable for modeling variables but can be solved through methods like log transformation. The code chunk below creates a new variable which is the log transformation of the original selling price variable. It utilizes the function log() to perform this.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nWe can now replot the transformed variable in a similar method using ggplot.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Distribution of log of Resale Selling Price\") +\n  labs(x = \"log(Selling Price)\", y = \"Records\")\n\n\n\n\n\n\n\n\nThe new variable has less skewness compared to the original one."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#multiple-histogram-plots-of-variables",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#multiple-histogram-plots-of-variables",
    "title": "Geographically Weighted Regression Model",
    "section": "Multiple histogram plots of variables",
    "text": "Multiple histogram plots of variables\nWe will use ggarrange() of the ggpubr package to produce small multiple histograms or trellis plots.\nThe code chunk below uses ggarrange() to produce 12 small histograms arranged in columns of 4 rows.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#drawing-statistical-point-map",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#drawing-statistical-point-map",
    "title": "Geographically Weighted Regression Model",
    "section": "Drawing statistical point map",
    "text": "Drawing statistical point map\nWe can show the geospatial distribution of resale prices using the tmap package.\nThe code chunk below produces an interactive map (by toggling with tmap_mode(\"view\")) of the selling price. The set.zoom.limits argument of tm_view() constrains the minimum and the maximum zoom levels. The code chunk ends by turning interactive mode off to ensure that there is no active connection.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#simple-linear-regression-method",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#simple-linear-regression-method",
    "title": "Geographically Weighted Regression Model",
    "section": "Simple linear regression method",
    "text": "Simple linear regression method\nWe build a simple linear regression model by using SELLING_PRICE as the dependent variable and then AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nNote that lm() returns an lm object (or c(mlm, lm) for multiple responses)\nThe summary and output can be obtained by using summary() and anova() functions.\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output includes the estimate of the best fit line based on the coefficients table displayed. In this case it is:\n\\[\nSELLINGPRICE = -258181.1 + 1.4719 (AREA)\n\\]\nThe R-squared value of 0.4518 states that the model is able to explain 45% of the values of the selling/resale price.\nThe p-value of less than 0.01 indicates that the regression model is a good estimator of the resale price.\nTo visualize the best fit line graphically, we can produce the scatterplot and then incorporate lm() function for the smoothed line in ggplot as below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  ggtitle(\"Fl0or area vs Resale Price\") +\n  labs(x = \"Floor Area\", y = \"Resale Price\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#multiple-linear-regression-method",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#multiple-linear-regression-method",
    "title": "Geographically Weighted Regression Model",
    "section": "Multiple linear regression method",
    "text": "Multiple linear regression method\n\nVisualizing the relationship of the independent variables\nBefore building a multiple LM model, it is important to ensure that the independent variables used are not highly correlated with each other. A correlation matrix is commonly used to visually inspect the relationships between these variables.\nThe pairs() function of R as well as other packages can be used. For this section, we will use the corrplot package.\nThe code chunk below uses corrplot() from that package to show the correlation coefficient between every pair of independent variable.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\n\n\n\n\nMatrix reorder, controlled by the order argument, is important to uncover hidden structures or patterns. There are four methods available: AOE, FPC, hclust and alphabet. AOE is used in the code above and uses the angular order of the eigenvectors method.\nInspecting the output above, it is clear that FREEHOLD is highly correlated with LEASE_99YEAR– so it is best to only include one of these. For our model, we will just keep the first variable.\n\n\nBuilding a hedonic pricing model using multiple linear regression method\nThe code chunk below uses lm() to calbrate a multiple linear regression model. It also produces the summary of the model using summary()\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nPreparing publication quality table using olsrr\nWith reference to the results above, it is clear that some of the variables are not statistically significant. We revise the model to exclude such variables and then produce the summary using ols_regress().\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.591 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\nPreparing publication quality table using gtsummary\nThe gtsummary package provides an alternative way to produce publication-grade summaries in R.\nThe code chunk below uses tbl_regression() to create a formatted regression report.\n\ngtsummary::tbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWith the gtsummary package, model statistics can also be added by appending them to the output using add_glance_table() or as a source not by using add_glance_source_note() as in the code chunk below.\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nChecking for multicolllinearity\nIn the code chunk below, we use ols_vif_tol() of olsrr package to check for signs of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nAs the VIF of each of the independent variables is less than 10, we can safely assume that there is no multicollinearity in our model.\n\n\nTesting for non-linearity\nWhen performing multiple linear regression, we need to check whether the assumptions of linearity and additivity are not violated.\nFor linearity, we use the ols_plot_resid_fit() of olsrr package in the code chunk below.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\n\n\n\nAs the residuals / points lie around the zero line, we have confidence that the linearity assumption is not violated.\n\n\nTest for normality\nThe code chunk below uses ols_plot_resid_hist() of olsrr package to check for normality.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\n\n\n\nThe output reveals that the residuals follow a normal distribution.\nThe oslrr package can also perform regular statistical tests for normality and display in a tabular format using ols_test_normality()\n\nols_test_normality(condo.mlr1)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the one-sample Kolmogorov-Smirnov test\n\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\n\n\nTesting for spatial autocorrelation\nTo test for spatial autocorrelation, we need to convert the resell prices sf data frame into a SpatialPointsDataFrame.\nWe first need to export the residuals of the regression model and save it as a dataframe.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nWe then include this as a new field in the condo_resale.sf object by using the code chunk below\n\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf,\n                             condo.mlr1$residuals) %&gt;%\n  rename(`MLR_RES` = `condo.mlr1.residuals`)\n\nWe then use the code chunk below to convert the object into SpatialPointsDataFrame format to be able to use spdep package functions on it.\n\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nThe code chunk below creates an interactive map using tmap to visualize the data.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nThe map reveals no clear signs of autocorrelation as there are no clear clusters with high or low residual values.\nTo verify this conclusion, we can perform Moran’s I test.\nFirst, we generate the distance-based weight matrix by using dnearneigh() of spdep package.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, we use nb2listw() to convert the neighbours into spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, we use lm.morantest() of spdep package to perform Moran’s I test for the residual spatial autocorrelation.\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nAs the p-value is less than our level of confidence α = 0.05, we reject the hypothesis of spatial randomness. As the test statistic I is positive, we infer that the residuals exhibit clustering."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#building-fixed-bandwidth-gwr-model",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#building-fixed-bandwidth-gwr-model",
    "title": "Geographically Weighted Regression Model",
    "section": "Building fixed bandwidth GWR model",
    "text": "Building fixed bandwidth GWR model\n\nComputing fixed bandwidth\nIn the code chunk below, we use bw.gwr() of the GWR package to determine an optimal fixed bandwidth. The adaptive=\"FALSE\" argument value indicates that we are computing for a fixed bandwidth.\nWe use the approach argument to define the stopping rule which can either be \"CV\" or cross-validation approach, or \"AICc\" or AIC corrected approach. We use the former in the code chunk.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe output shows that the recommended bandwidth is 971.3405 (meters)\n\n\nGWModel method - fixed bandwidth\nWe can calibrate the gwr model using fixed bandwidth and a gaussian kernel using the code chunk below.\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe object contains the output and is in class gwrm. Calling the object displays the model output.\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-14 16:05:46.792674 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2024-10-14 16:05:47.512285 \n\n\nThe report shows that the AICc of the gwr is signigicantly smaller than that of the global multiple lm (42263.61 &lt; 42967.1)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#building-adaptive-bandwidth-gwr-model",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#building-adaptive-bandwidth-gwr-model",
    "title": "Geographically Weighted Regression Model",
    "section": "Building adaptive bandwidth GWR model",
    "text": "Building adaptive bandwidth GWR model\n\nComputing adaptive bandwidth\nWe again use bw.gwr() of the GWR package to determine the bandwidth. This time adaptive=\"TRUE\" argument value indicates that we are computing for an adaptive bandwidth.\n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe output shows that 30 is the recommended data points to be used.\n\n\nGWModel method - adaptive bandwidth\nWe can calibrate the gwr model using adaptive bandwidth and a gaussian kernel using the code chunk below.\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nThe object contains the output and is in class gwrm. Calling the object displays the model output.\n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-14 16:05:52.861739 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-10-14 16:05:53.74023"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#visualizing-gwr-output",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#visualizing-gwr-output",
    "title": "Geographically Weighted Regression Model",
    "section": "Visualizing GWR Output",
    "text": "Visualizing GWR Output\nThe output table includes various fields aside from the residuals and are all stored in the SpatialPointsDataFrame or SpatialPolygonsDataFrame object in an object called SDF.\n\nConverting SDF into SF dataframe\nTo visualize the fields in SDF, we first convert it into an sf dataframe using the code chunks below\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\n\ncondo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414)\n\n\ngwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nWe then use glimpse() to check the contents of the last object.\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       &lt;dbl&gt; 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               &lt;dbl&gt; 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               &lt;dbl&gt; 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\nThere are 77 fields that are included in the dataframe. We can use summary() to check the statistics of the yhat field as below.\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\nVisualizing local R2\nThe code chunk below is used to create an interactive point symbol map based on the Local_R2 values.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\n\n\nVisualizing coefficient estimates\nThe code chunk below creates side-by-side interactive map of the standard error and t-value of the AREA_SQM variable.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nWe can also focus on a particular region like the central region and show the R2 values using the code chunk below\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "",
    "text": "In this in-class exercise, we learn to use GWR or geographically weighted regression. We use sf methods unlike the ones used in the corresponding hands-oon exercise"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#data-sources",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#data-sources",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Data Sources",
    "text": "Data Sources\nTo datasets will be used for this exercise:\n\n2014 Master Plan subzone boundary in shapefile format\n2015 condo resale prices in csv format"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#installing-and-launching-r-packages",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#installing-and-launching-r-packages",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of the following R packages:\n\nolsrr - for building OLS (ordinary least squares) regression models and performing diagnostic tests\nGWmodel - for calibrating geographically weighted family of models\ntmap - for plotting cartographic quality maps\ncorrplot - for multivariate data visualization and analysis\nsf - spatial data handling\ntidyverse - attribute data handling\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, ggstatsplot, sfdep)"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#geospatial-data-loading-and-preparation",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#geospatial-data-loading-and-preparation",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Geospatial data loading and preparation",
    "text": "Geospatial data loading and preparation\nThe code chunk below uses st_read() of the sf package to load the geospatial data and apply the right EPSG code to convert it to svy21.\n\nmpsz_svy21 = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\") %&gt;%\n  st_transform(3414) %&gt;%\n  st_make_valid()\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#aspatial-data-loading",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#aspatial-data-loading",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Aspatial data loading",
    "text": "Aspatial data loading\nThe code chunk below uses read_csv() of readr to import the 2015 condo resale prices from the csv file.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#converting-aspatial-dataframe-into-an-sf-object",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#converting-aspatial-dataframe-into-an-sf-object",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Converting aspatial dataframe into an sf object",
    "text": "Converting aspatial dataframe into an sf object\nTo convert the condo_resale object into a spatial object, we can use the following code chunk that utilizes st_as_sf() from sf package. The final line of the code chunk converts the data frame from wgs84 to svy21 using the indicated crs values.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#checking-for-correlation",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#checking-for-correlation",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Checking for correlation",
    "text": "Checking for correlation\nIn the hands-on exercise, we used corrplot() to generate the correlation plot.\nAn alternative approach is to use ggcorrmat() of ggstatsplot which requires a simpler line of code.\n\nggcorrmat(condo_resale[, 5:23])"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#using-olsrr-to-display-results-and-for-vif",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#using-olsrr-to-display-results-and-for-vif",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Using olsrr to display results and for VIF",
    "text": "Using olsrr to display results and for VIF\nWe can use the olsrr package to run diagnostic tests on the model.\nWe can pass the results to ols_regress() to produce a formatted model report, better than the one coming from summary().\n\nols_regress(condo.mlr)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     750537.537 \nR-Squared                    0.652       MSE                571262902261.223 \nAdj. R-Squared               0.647       Coef. Var                    43.160 \nPred R-Squared               0.637       AIC                       42971.173 \nMAE                     412117.987       SBC                       43081.835 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.515738e+15          19        7.977571e+13    139.648    0.0000 \nResidual      8.089083e+14        1416    571262902261.223                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     543071.420    136210.918                   3.987    0.000     275874.535     810268.305 \n            AREA_SQM      12688.669       370.119        0.579     34.283    0.000      11962.627      13414.710 \n                 AGE     -24566.001      2766.041       -0.166     -8.881    0.000     -29991.980     -19140.022 \n            PROX_CBD     -78121.985      6791.377       -0.267    -11.503    0.000     -91444.227     -64799.744 \n      PROX_CHILDCARE    -333219.036    111020.303       -0.087     -3.001    0.003    -551000.984    -115437.089 \n    PROX_ELDERLYCARE     170949.961     42110.748        0.083      4.060    0.000      88343.803     253556.120 \nPROX_URA_GROWTH_AREA      38507.622     12523.661        0.059      3.075    0.002      13940.700      63074.545 \n  PROX_HAWKER_MARKET      23801.197     29299.923        0.019      0.812    0.417     -33674.725      81277.120 \n   PROX_KINDERGARTEN     144097.972     82738.669        0.030      1.742    0.082     -18205.570     306401.514 \n            PROX_MRT    -322775.874     58528.079       -0.123     -5.515    0.000    -437586.937    -207964.811 \n           PROX_PARK     564487.876     66563.011        0.148      8.481    0.000     433915.162     695060.590 \n    PROX_PRIMARY_SCH     186170.524     65515.193        0.072      2.842    0.005      57653.253     314687.795 \nPROX_TOP_PRIMARY_SCH       -477.073     20597.972       -0.001     -0.023    0.982     -40882.894      39928.747 \n  PROX_SHOPPING_MALL    -207721.520     42855.500       -0.109     -4.847    0.000    -291788.613    -123654.427 \n    PROX_SUPERMARKET     -48074.679     77145.257       -0.012     -0.623    0.533    -199405.956     103256.599 \n       PROX_BUS_STOP     675755.044    138551.991        0.133      4.877    0.000     403965.817     947544.272 \n         NO_Of_UNITS       -216.180        90.302       -0.046     -2.394    0.017       -393.320        -39.040 \n     FAMILY_FRIENDLY     142128.272     47055.082        0.056      3.020    0.003      49823.107     234433.438 \n            FREEHOLD     300646.543     77296.529        0.117      3.890    0.000     149018.525     452274.561 \n      LEASEHOLD_99YR     -77137.375     77570.869       -0.030     -0.994    0.320    -229303.551      75028.801 \n-----------------------------------------------------------------------------------------------------------------\n\n\nWe can then use ols_vif_tol() to run the test for multicollinearity using the VIF or variance inflation factor.\n\nfilter(ols_vif_tol(condo.mlr), VIF &gt; 5) # sign of multicollinearity\n\n[1] Variables Tolerance VIF      \n&lt;0 rows&gt; (or 0-length row.names)\n\nfilter(ols_vif_tol(condo.mlr), VIF &lt;= 5) # no sign of multicollinearity\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8601326 1.162611\n2                   AGE 0.7011585 1.426211\n3              PROX_CBD 0.4575471 2.185567\n4        PROX_CHILDCARE 0.2898233 3.450378\n5      PROX_ELDERLYCARE 0.5922238 1.688551\n6  PROX_URA_GROWTH_AREA 0.6614081 1.511926\n7    PROX_HAWKER_MARKET 0.4373874 2.286303\n8     PROX_KINDERGARTEN 0.8356793 1.196631\n9              PROX_MRT 0.4949877 2.020252\n10            PROX_PARK 0.8015728 1.247547\n11     PROX_PRIMARY_SCH 0.3823248 2.615577\n12 PROX_TOP_PRIMARY_SCH 0.4878620 2.049760\n13   PROX_SHOPPING_MALL 0.4903052 2.039546\n14     PROX_SUPERMARKET 0.6142127 1.628100\n15        PROX_BUS_STOP 0.3311024 3.020213\n16          NO_Of_UNITS 0.6543336 1.528272\n17      FAMILY_FRIENDLY 0.7191719 1.390488\n18             FREEHOLD 0.2728521 3.664990\n19       LEASEHOLD_99YR 0.2645988 3.779307\n\n\nAs there is no variable with VIF value above 5, we are ensured that there is no signs of multicollinearity using this criterion."
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#using-olsrr-for-variable-selection-using-stepwise-regression",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#using-olsrr-for-variable-selection-using-stepwise-regression",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Using olsrr for variable selection using stepwise regression",
    "text": "Using olsrr for variable selection using stepwise regression\nForward stepwise regression adds variables one at a time from an empty model by picking the one with the highest rank for a criteria (and within a threshold) and that improves the model. (i.e., adj R squared) The criteria is typically the significance level (e.g., choose minimum p-value below 0.05)\nols_step_forward_p() performs forward stepwise regression using the p-value. Other criteria include AIC, BIC, r-squared. A 0.05 max p-value is defined using the p_val argument. The details argument instructs whether the results are printed out while each step is run.\n\ncondo_fw_mlr &lt;- ols_step_forward_p(condo.mlr, p_val = 0.05, details = FALSE)\n\nWe can pass the ols regression object into plot() to display the results graphically. The charts show the improvement of four of the model metrics with each variable added.\n\nplot(condo_fw_mlr)\n\n\n\n\n\n\n\n\nWe can plot the residuals using ols_plot_resid_fit() and then passing the model object inside the mlr object. This tests for the linearity assumption.\n\nols_plot_resid_fit(condo_fw_mlr$model)\n\n\n\n\n\n\n\n\nWe can plot the residuals using ols_plot_resid_hist() to test for the normality assumption graphically.\n\nols_plot_resid_hist(condo_fw_mlr$model)\n\n\n\n\n\n\n\n\nWe can also do it using the traditional stats using the following\n\nols_test_normality(condo_fw_mlr$model)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the one-sample Kolmogorov-Smirnov test\n\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#testing-spatial-autocorrelation",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#testing-spatial-autocorrelation",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Testing spatial autocorrelation",
    "text": "Testing spatial autocorrelation\nWe export the residuals of the hedonic pricing model as a dataframe\n\nmlr_output &lt;- as.data.frame(condo_fw_mlr$model$residuals) %&gt;%\n  rename('FW_MLR_RES' = 'condo_fw_mlr$model$residuals')\n\nWe then join the new dataframe to the sf object.\n\ncondo_resale.sf &lt;- cbind(condo_resale.sf,\n                         mlr_output$FW_MLR_RES) %&gt;%\n  rename('MLR_RES' = 'mlr_output.FW_MLR_RES')\n\nWith this, we can produce an interactive map of the residuals using the sf object.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntmap_options(check.and.fix = TRUE) # can be added to the layer with a problem\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\", title = \"Residual\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nThere appears to be clusters with high residuals– there appears to be signs of spatial autocorrelation.\nTo prove our suspicions, we conduct Moran’s I using the sfdep package (without needing to convert and use spdep as in the hands-on exercise)\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(nb = st_knn(geometry, k = 6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nWe can then run the Global Moran’s I permutation test directly on the object\n\nglobal_moran_perm(condo_resale.sf$MLR_RES,\n                  condo_resale.sf$nb,\n                  condo_resale.sf$wt,\n                  alternative = \"two.sided\",\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.32254, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nSince the test p-value is less than 0.05, the result is significant, and as the test statistic is positive, then there are signs of clusters.\nAs there is spatial autocorrelation, we can build the GWR model."
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#using-fixed-bandwidth-method",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#using-fixed-bandwidth-method",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Using fixed bandwidth method",
    "text": "Using fixed bandwidth method\nThe following code chunk derives the optimal fixed bandwidth for the model from the regression method (have to type in full as it cannot take in the ols regression object.\n\nbw_fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK +\n                  PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\n                  NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                  data = condo_resale.sf,\n                  approach = \"CV\",\n                  kernel = \"gaussian\",\n                  adaptive = FALSE,\n                  longlat = FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe results show that ~971.3405 is the recommended bandwidth.\nThe code below calibrates the gwr model using fixed bandwidth\n\ngwr_fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK +\n                  PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\n                  NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                  data = condo_resale.sf,\n                  bw = bw_fixed,\n                  kernel = \"gaussian\",\n                  longlat = FALSE)\n\nThe output is saved in a list of class gwrm\n\ngwr_fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-14 22:19:43.205686 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw_fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2024-10-14 22:19:43.872735 \n\n\nThe resulting object has a field named SDF which contains the parameter estimates. To visualize elements of it, we transfer this information into the sf object. (only considering a few columns)\n\ngwr_fixed_output &lt;- as.data.frame(gwr_fixed$SDF) %&gt;%\n  select(-c(2:15))\n\ngwr_sf_fixed &lt;- cbind(condo_resale.sf, gwr_fixed_output)\n\nWe can use glimpse() to check the contents of the new object\n\nglimpse(gwr_sf_fixed)\n\nRows: 1,436\nColumns: 63\n$ nb                      &lt;nb&gt; &lt;66, 77, 123, 238, 239, 343&gt;, &lt;21, 162, 163, 19…\n$ wt                      &lt;list&gt; &lt;0.1666667, 0.1666667, 0.1666667, 0.1666667, …\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               &lt;dbl&gt; 1580824.71, 1509406.28, 3583211.16, -444860.49…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2900355.0, 3499796.0, 3628135.3, 5359292.0, 13…\n$ residual                &lt;dbl&gt; 99644.96, 380204.01, -303135.30, -1109292.00, …\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.58164609, 1.00012683, -0.88571524, -2.513378…\n$ Intercept_SE            &lt;dbl&gt; 3395011.7, 1352467.0, 1339841.1, 370353.7, 242…\n$ AREA_SQM_SE             &lt;dbl&gt; 1580.6464, 1221.7304, 1119.0686, 616.3396, 154…\n$ AGE_SE                  &lt;dbl&gt; 15224.251, 9536.861, 7725.609, 5978.954, 9264.…\n$ PROX_CBD_SE             &lt;dbl&gt; 156762.55, 72009.21, 79937.75, 359165.60, 4780…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 568613.2, 463408.9, 398867.7, 305347.0, 806691…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 579655.1, 156842.5, 178208.6, 118670.9, 414128…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 422324.6, 191103.4, 125333.4, 366726.0, 563178…\n$ PROX_MRT_SE             &lt;dbl&gt; 606123.6, 560744.6, 334275.1, 271991.5, 454983…\n$ PROX_PARK_SE            &lt;dbl&gt; 399605.9, 435453.7, 374942.3, 216766.2, 469595…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 512060.9, 268609.3, 238048.9, 226860.0, 278753…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 482696.6, 239509.0, 142155.9, 153273.0, 376752…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 1508504.5, 636969.8, 518721.9, 543058.9, 81911…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 806.8444, 266.4264, 234.1108, 324.0807, 353.41…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 251502.9, 162760.6, 173178.3, 107958.2, 182177…\n$ FREEHOLD_SE             &lt;dbl&gt; 370362.8, 205263.5, 165806.1, 134885.6, 237553…\n$ Intercept_TV            &lt;dbl&gt; 0.4656316, 1.1160393, 2.6743554, -1.2011773, 0…\n$ AREA_SQM_TV             &lt;dbl&gt; 6.162087, 12.196149, 11.585038, 32.977013, 4.3…\n$ AGE_TV                  &lt;dbl&gt; -0.62722500, -4.56438096, -3.26172325, -15.228…\n$ PROX_CBD_TV             &lt;dbl&gt; -0.26572481, -2.24375590, -3.39501541, 3.74802…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 0.314199728, 0.596072954, -0.662701339, 1.8528…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -0.61040359, 1.27422394, 3.26841613, 2.1260488…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -0.51714116, 0.30511964, -2.01288530, -4.31616…\n$ PROX_MRT_TV             &lt;dbl&gt; -0.68841147, -4.18708291, -2.84832358, -6.7173…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.54239001, 0.92304803, 0.76850923, -2.802534…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 0.5897363, 2.7676171, 2.3052085, 13.1016452, 0…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 0.72864945, -1.32424806, -1.12186305, -0.69972…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 0.79146959, 2.90629848, 2.87414548, 11.7683749…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.59112146, -0.80578975, 0.18687430, -0.615698…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; 0.235784421, 0.665313652, -0.356298596, 13.572…\n$ FREEHOLD_TV             &lt;dbl&gt; 0.86274374, 1.82752304, 0.94991015, 8.41398102…\n$ Local_R2                &lt;dbl&gt; 0.9473297, 0.9136782, 0.8989196, 0.8994818, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n$ geometry.1              &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\nWe can then plot using tmap\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_fixed) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html",
    "title": "Geographically Weighted Predictive Model",
    "section": "",
    "text": "In this hands-on exercise, we learn about geographically weighted prediction models. In these, occurrences of events are assumed to not be random or uniformly distributed over space.\nThis exercise is based on Chapter 14 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#data-sources",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#data-sources",
    "title": "Geographically Weighted Predictive Model",
    "section": "Data Sources",
    "text": "Data Sources\nTo data for this exercise comes in an rds file and is based on the following sources:\n\nHDB resale data in Singapore from 2017 onwards from data.gov.sg\n2014 Master Plan Planning subzone boundary in shapefile format\nLocational factors with geographic coordinates from data.gov.sg\n\nList and locations of eldercare centres, hawker centres, parks, supermarkets, CHAS clinics, childcare service centres, kindergartens\n\nLocational factors with geographic coordinates from datamall.lto.gov.sg\n\nMRT stations and locations, bus stops and locations\n\nLocational factors without geographic coordinates from data.gov.sg\n\nList of primary schools\n\nLocational factors without geographic coordinates from other sources\n\nCBD coordinates from Google, Shopping malls from Wikipedia, “good or top” primary schools from Local Salary forum"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#installing-and-launching-r-packages",
    "title": "Geographically Weighted Predictive Model",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of eight R packages.\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#reading-from-rds-file",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#reading-from-rds-file",
    "title": "Geographically Weighted Predictive Model",
    "section": "Reading from RDS file",
    "text": "Reading from RDS file\nThe code chunk below uses read_rds() to load the exercise data.\n\nmdata &lt;- read_rds(\"data/rds/mdata.rds\")\n\nWe can use class() to verify the data type, and head() to inspect the first few elements of the object.\n\nclass(mdata)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nhead(mdata)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 28240.06 ymin: 38382.85 xmax: 30637.92 ymax: 39745.94\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 18\n  resale_price floor_area_sqm storey_order remaining_lease_mths PROX_CBD\n         &lt;dbl&gt;          &lt;dbl&gt;        &lt;int&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1       330000             92            1                  684     8.82\n2       360000             91            3                  738     9.84\n3       370000             92            1                  733     9.56\n4       375000             99            2                  700     9.61\n5       380000             92            2                  715     8.35\n6       380000             92            4                  732     9.49\n# ℹ 13 more variables: PROX_ELDERLYCARE &lt;dbl&gt;, PROX_HAWKER &lt;dbl&gt;,\n#   PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;, PROX_GOOD_PRISCH &lt;dbl&gt;, PROX_MALL &lt;dbl&gt;,\n#   PROX_CHAS &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, WITHIN_350M_KINDERGARTEN &lt;int&gt;,\n#   WITHIN_350M_CHILDCARE &lt;int&gt;, WITHIN_350M_BUS &lt;int&gt;,\n#   WITHIN_1KM_PRISCH &lt;int&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#data-sampling",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#data-sampling",
    "title": "Geographically Weighted Predictive Model",
    "section": "Data sampling",
    "text": "Data sampling\nBuilding a predictive model requires splitting at least into training and a test set. We will use a 65:35 ratio. The code chunk below uses initial_split() of the rsample package to perform the split.\n\nset.seed(1234)\nresale_split &lt;- initial_split(mdata, \n                              prop = 6.5/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)\n\nWe can save these into respective rds files to make it easier to reload and replicate the results and model\n\nwrite_rds(train_data, \"data/rds/train_data.rds\")\nwrite_rds(test_data, \"data/rds/test_data.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#converting-the-sf-data-frame-to-spatialpointdataframe",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#converting-the-sf-data-frame-to-spatialpointdataframe",
    "title": "Geographically Weighted Predictive Model",
    "section": "Converting the sf data frame to SpatialPointDataFrame",
    "text": "Converting the sf data frame to SpatialPointDataFrame\nThe first step is to convert the training sf object into a SpatialPointDataFrame format using as_Spatial() in the code chunk below\n\ntrain_data_sp &lt;- as_Spatial(train_data)\ntrain_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 10335 \nextent      : 11597.31, 42623.63, 28217.39, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,          PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       218000,             74,            1,                  555, 0.999393538715878, 1.98943787433087e-08, 0.0333358643817954, 0.0220407324774434, 0.0441643212802781, 0.0652540365486641,                0, 6.20621206270077e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1186888,            133,           17,                 1164,  19.6500691667807,     3.30163731686804,   2.86763031236184,   2.13060636038504,   2.41313695915468,   10.6223726149914, 2.27100643784442,    0.808332738794272,     1.57131703651196,                        7,                    20, ..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-adaptive-bandwidth",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-adaptive-bandwidth",
    "title": "Geographically Weighted Predictive Model",
    "section": "Computing adaptive bandwidth",
    "text": "Computing adaptive bandwidth\nWe then use bw.gwr() of GWmodel package to determine the optimal (adaptive) bandwidth to be used.\n\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\nThe output suggests to use 40 as the adaptive bandwidth. To save on time in the future, (as the code takes some time to run) we can save the results in an RDS file.\n\nwrite_rds(bw_adaptive, \"data/rds/bw_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#constructing-the-adaptive-bandwidth-gwr-model",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#constructing-the-adaptive-bandwidth-gwr-model",
    "title": "Geographically Weighted Predictive Model",
    "section": "Constructing the adaptive bandwidth GWR model",
    "text": "Constructing the adaptive bandwidth GWR model\nWe can then construct the gwr-based hedonic pricing model using adaptive bandwidth and Gaussian kernel with the code chunk below. This uses the gwr.basic() function\n\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\nWe can save the model into an RDS file for future use.\n\nwrite_rds(gwr_adaptive, \"data/rds/gwr_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#retrieve-gwr-output-object",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#retrieve-gwr-output-object",
    "title": "Geographically Weighted Predictive Model",
    "section": "Retrieve GWR output object",
    "text": "Retrieve GWR output object\nThe code chunk below displays the model output by calling the object.\n\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-20 20:39:37.422622 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + storey_order + \n    remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n    PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data_sp, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_order remaining_lease_mths PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_PRISCH\n   Number of data points: 10335\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\n   floor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\n   storey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\n   remaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\n   PROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\n   PROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\n   PROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\n   PROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\n   PROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\n   PROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\n   PROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\n   WITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\n   WITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\n   WITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\n   WITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 61650 on 10320 degrees of freedom\n   Multiple R-squared: 0.7373\n   Adjusted R-squared: 0.737 \n   F-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 3.922202e+13\n   Sigma(hat): 61610.08\n   AIC:  257320.2\n   AICc:  257320.3\n   BIC:  247249\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 40 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.2478e+08 -4.7727e+05 -8.3004e+03  5.5025e+05\n   floor_area_sqm           -2.8714e+04  1.4475e+03  2.3011e+03  3.3900e+03\n   storey_order              3.3186e+03  8.5899e+03  1.0826e+04  1.3397e+04\n   remaining_lease_mths     -1.4431e+03  2.6063e+02  3.9048e+02  5.2865e+02\n   PROX_CBD                 -1.0837e+07 -5.7697e+04 -1.3787e+04  2.6552e+04\n   PROX_ELDERLYCARE         -3.2195e+07 -4.0643e+04  1.0562e+04  6.1054e+04\n   PROX_HAWKER              -2.3985e+08 -5.1365e+04  3.0026e+03  6.4287e+04\n   PROX_MRT                 -1.1632e+07 -1.0488e+05 -4.9373e+04  5.1037e+03\n   PROX_PARK                -6.5961e+06 -4.8671e+04 -8.8128e+02  5.3498e+04\n   PROX_MALL                -1.8112e+07 -7.4238e+04 -1.3982e+04  4.9779e+04\n   PROX_SUPERMARKET         -4.5761e+06 -6.3461e+04 -1.7429e+04  3.5616e+04\n   WITHIN_350M_KINDERGARTEN -4.1823e+05 -6.0040e+03  9.0209e+01  4.7127e+03\n   WITHIN_350M_CHILDCARE    -1.0273e+05 -2.2375e+03  2.6668e+02  2.6388e+03\n   WITHIN_350M_BUS          -1.1757e+05 -1.4719e+03  1.1626e+02  1.7584e+03\n   WITHIN_1KM_PRISCH        -6.6465e+05 -5.5959e+03  2.6916e+02  5.7500e+03\n                                  Max.\n   Intercept                1.6493e+08\n   floor_area_sqm           5.0907e+04\n   storey_order             2.9537e+04\n   remaining_lease_mths     1.8119e+03\n   PROX_CBD                 2.2411e+07\n   PROX_ELDERLYCARE         8.2444e+07\n   PROX_HAWKER              5.9654e+06\n   PROX_MRT                 2.0189e+08\n   PROX_PARK                1.5188e+07\n   PROX_MALL                1.0443e+07\n   PROX_SUPERMARKET         3.8330e+06\n   WITHIN_350M_KINDERGARTEN 6.6799e+05\n   WITHIN_350M_CHILDCARE    1.0802e+05\n   WITHIN_350M_BUS          3.7313e+04\n   WITHIN_1KM_PRISCH        5.0231e+05\n   ************************Diagnostic information*************************\n   Number of data points: 10335 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1730.101 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 8604.899 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 238871.9 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 237036.9 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 238209.1 \n   Residual sum of squares: 4.829191e+12 \n   R-square value:  0.967657 \n   Adjusted R-square value:  0.9611534 \n\n   ***********************************************************************\n   Program stops at: 2024-10-20 20:40:56.413755"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-test-data-predictions-converting-to-spatialpointdataframe",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-test-data-predictions-converting-to-spatialpointdataframe",
    "title": "Geographically Weighted Predictive Model",
    "section": "Computing test data predictions: Converting to SpatialPointDataFrame",
    "text": "Computing test data predictions: Converting to SpatialPointDataFrame\nIn order to compute for the predicted values for the test, we first need to also convert the test data into SpatialPointDataFrame.\n\ntest_data_sp &lt;- test_data %&gt;%\n  as_Spatial()\ntest_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 5566 \nextent      : 11597.31, 42623.63, 28287.8, 48669.59  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,         PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       230888,             74,            1,                  546, 1.00583660772922, 3.34897933104965e-07, 0.0474019664161957, 0.0414043955932523, 0.0502664084494264, 0.0907500295577619,                0, 4.55547870890763e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1050000,            138,           14,                 1151,  19.632402730488,     3.30163731686804,   2.83106651960209,   2.13060636038504,   2.41313695915468,   10.6169590126272, 2.26056404492346,     0.79249074802552,     1.53786629004208,                        7,                    16, ..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-test-data-predictions",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-test-data-predictions",
    "title": "Geographically Weighted Predictive Model",
    "section": "Computing test data predictions",
    "text": "Computing test data predictions\nWe then use gwr.predict() to generate predictions for the test data using a model derived from the training data.\n\ndmat_gwr &lt;- gw.dist(st_coordinates(train_data), st_coordinates(test_data), focus=0, p=2, theta=0, longlat=F)\nwrite_rds(dmat_gwr, \"data/rds/dmat_gwr.rds\")\n\n\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data_sp, \n                        predictdata = test_data_sp, \n                        bw=40, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE,\n                        dMat1 = dmat_gwr)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#extracting-coordinates-data",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#extracting-coordinates-data",
    "title": "Geographically Weighted Predictive Model",
    "section": "Extracting coordinates data",
    "text": "Extracting coordinates data\nWe use the code chunk below to extract the coordinates from the data and the training and test splits. We then write these into RDS files for easy access later.\n\ncoords &lt;- st_coordinates(mdata)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\ncoords &lt;- write_rds(coords, \"data/rds/coords.rds\" )\ncoords_train &lt;- write_rds(coords_train, \"data/rds/coords_train.rds\" )\ncoords_test &lt;- write_rds(coords_test, \"data/rds/coords_test.rds\" )"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#dropping-the-geometry-field",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#dropping-the-geometry-field",
    "title": "Geographically Weighted Predictive Model",
    "section": "Dropping the geometry field",
    "text": "Dropping the geometry field\nNext, we need to drop the geometry field of the training data by using st_drop_geometry() from sf package.\n\ntrain_data_nogeom &lt;- train_data %&gt;% \n  st_drop_geometry()"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#calibrating-using-test-data",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#calibrating-using-test-data",
    "title": "Geographically Weighted Predictive Model",
    "section": "Calibrating using test data",
    "text": "Calibrating using test data\nThe code chunk below calibrates a geographic random forest model using grf() of SpatialML package\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data_nogeom, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\nWe can save the model using the code chunk below.\n\nwrite_rds(gwRF_adaptive, \"data/rds/gwRF_adaptive.rds\")\n\nThen we can reload the object using the following code chunk.\n\ngwRF_adaptive &lt;- read_rds(\"data/rds/gwRF_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#predicting-with-test-data",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#predicting-with-test-data",
    "title": "Geographically Weighted Predictive Model",
    "section": "Predicting with test data",
    "text": "Predicting with test data\n\nPreparing the test data\nIn a similar fashion, we also need to remove the geometry from the test data.\n\ntest_data_nogeom &lt;- cbind(test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\nPredicting with test data\nWe use predict.grf() to predict using the test data in the code chunk below, and then write onto an rds file in the same code block\n\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data_nogeom, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\nwrite_rds(gwRF_pred, \"data/rds/GRF_pred.rds\")\n\n\nhead(gwRF_pred)\n\n[1] 383250.2 352868.8 411256.3 375408.6 419465.5 378760.3\n\n\n\n\nConverting the output into a dataframe\nThe output is in vector form but it is better to convert it into a dataframe so it is easier for analysis and visualizations.\n\nGRF_pred_df &lt;- as.data.frame(gwRF_pred)\n\nWe then add the predicted values into the dataset using the chunk below.\n\ntest_data_p &lt;- cbind(test_data_nogeom, GRF_pred_df)\nwrite_rds(test_data_p, \"data/rds/test_data_p.rds\")\n\n\n\nCalculating Root Mean Square Error (RMSE)\nThe RMSE measures how far predited values are from the actual test values. The code chunk below uses rmse() of Metrics package to compute it for the model against the test data.\n\nrmse(test_data_p$resale_price, \n     test_data_p$gwRF_pred)\n\n[1] 27302.9\n\n\n\n\nVisualizing the predicted values\nWe can also use a scatterplot to visualize the predicted prices against the actual prices\n\nggplot(data = test_data_p,\n       aes(x = gwRF_pred,\n           y = resale_price)) +\n  geom_point()"
  }
]