[
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "",
    "text": "We look into financial inclusion in Tanzania based on a 2023 survey of close to 10 thousand residents. We achieve this by calibrating global and geographically weighted models in R using the GWModel package for the latter. We identified key factors that are linked to higher levels or probability of financial inclusion that include access to technology and education. The geographically weighted models then revealed districts where an inverse relationship is observed, and need to be studied further."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#a.1-background",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#a.1-background",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.1 Background",
    "text": "A.1 Background\nThe World Bank defines financial inclusion as the state of having access to useful and affordable financial products to meet one’s needs. Financial inclusion is an enabler to 7 of the Sustainable Development Goals, and is seen as the key enabler to reduce extreme poverty.\nOne key dimension of financial inclusion that the World Bank looked at in their latest Global Findex Database 2021 is the ownership of bank accounts for adults. In this report, 76% of the global adult population have their own accounts, but only 71% of the developing nations’ do. In some countries like Tanzania this number is even lower at 52%. Banking is just one traditional dimension. Other vehicles like mobile payments can bridge the gap in access to services for some of these nations.\nTanzania recognizes the importance of financial inclusion in promoting economic growth and with the Bank of Tanzania, the country’s central bank, the Microfinance Policy of 2000 was developed and focused on expanding financial services for low-income individuals.\nThe program behind financial inclusion has been structured from 2014 with the first National Financial Inclusion Framework for 2014-2016, with the latest version being the third framework for 2023-2028. While there has been significant progress, (e.g., access to financial services has risen from 42% in 2013 to 89% in 2023) the country continues to aim for inclusion for the whole population by increasing access, encouraging usage and enhancing the quality of financial services."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#a.2-objectives",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#a.2-objectives",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nFinscope Tanzania 2023 is a public-private sector collaboration and aimed, among others, to understand and describe the financial behavior of individuals in the country and to establish an updated view of the level of financial inclusion across various measures. A large part of the findings is showing the change (improvements) of the overall measures against the previous 2017 report.\nThe objective of this study is to build on the Finscope Tanzania 2023 by identifying influential variables and identifying if geospatial factors influence the effect of those variables.\nIn order to satisfy this, the specific deliverables for the study will be:\n\nto build a global or non-spatial explanatory model for the level of financial inclusion across Tanzania;\nto build a geographically weighted explanatory model for the same response variables; and,\nto assess the advantage of the geographically weighted model and to analyse the geographically weighted model"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#a.3-data-sources",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#a.3-data-sources",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe following data sources are used for this analysis:\n\nFinscope Tanzania 2023 individual survey data from Finscope Tanzania\n\nThe dataset is contained in a csv and translates the responses from 9,915 individuals who answered the survey\nThe respondents are all adults aged 16 years and above take across Tanzania\nThe dataset also includes derived fields which include different indicators for financial inclusion based on different criteria\n\nDistrict-level boundaries in Tanzania as a shapefile from geoBoundaries.org portal"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#a.4-importing-and-launching-r-packages",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#a.4-importing-and-launching-r-packages",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, the following R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nolsrr - for building OLS (ordinary least squares) regression models and performing diagnostic tests\nGWmodel - for calibrating geographically weighted family of models\ntmap - for plotting cartographic quality maps\nggstatsplot - for multivariate data visualization and analysis\nsf - spatial data handling\ntidyverse - attribute data handling\n\n\n\n\npacman::p_load(olsrr, sf, GWmodel, tmap, tidyverse, ggstatsplot, sfdep)"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.1.-tanzania-district-boundaries",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.1.-tanzania-district-boundaries",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.1. Tanzania District boundaries",
    "text": "B.1. Tanzania District boundaries\nTBC.\n\ntz_dist &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"geoBoundaries-TZA-ADM3\")\n\nReading layer `geoBoundaries-TZA-ADM3' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3644 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.59019 ymin: -11.7457 xmax: 40.44514 ymax: -0.9857875\nGeodetic CRS:  WGS 84\n\n\nxxx\n\nqtm(tz_dist)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "",
    "text": "In this exercise, we apply spatial point pattern analysis to analyse the distribution of road traffic accidents in the Bangkok Metropolitan Region. We demonstrate how kernel density estimation can be used to visualize hotspots in a network constrained and non-network constrained context. We use different approaches to display charts side by side, especially to compare hotspots across different time dimensions or different conditions. Finally, we demonstrate how K- and G-functionss can be used to support any claims on the randomness, clustering or dispersion of a spatial point distribution."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.1-background",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.1-background",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.1 Background",
    "text": "A.1 Background\nRoad traffic accidents account for 1.19million deaths and up to 50 million non-fatal injuries according to a report by the WHO last year.\nThe same report identifies major risk groups: low- and middle-income countries, (esp in Africa and Europe) the working population, and males. It also identifies some key risk factors which include human error, speeding, driving under the influence of alcohol, distracted driving, unsafe road infrastructure, unsafe vehicles, and law enforcement. Most of the factors identified are behavioral in nature but do not discount that other factors may also contribute to a higher risk of occurrence.\nWithin Southeast Asia, Thailand has ranked the highest in terms of incidence of road traffic accidents with an average number of of 20,000 deaths a year or 56 a day. The country has also seen an increase in the number of accidents from 2014 to 2021. A large 19% of these accidents occurred in national highways, and the chances of encountering an accident-prone zone was found to be 66%."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.2-objectives",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.2-objectives",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nThis study aims to take a deeper look into the road accidents in Thailand, focusing on the Bangkok Metropolitan Region (BMR) which contains the capital Bangkok, and five neighboring provinces. (Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakan, Samut Sakhon)\nAs most literature has focused on behavioral and environmental factors, the study will focus on identifying spatiotemporal factors influencing the occurrence of road accidents in BMR. At the minimum, the study deliverables include the following:\n\nVisualization of spatiotemporal dynamics of road traffic accidents in BMR\nDetailed spatial analysis of road traffic accidents in BMR\nDetailed spatiotemporal analysis of road traffic accidents in BMR\n\nThe appropriate technique must be used for these deliverables and all the analysis and visualizations will be carried out using R."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.3-data-sources",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.3-data-sources",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe study makes use of the following datasets which are publicly available online.\n\n\n\nDataset Short Name\nDescription\nDatasource\n\n\n\n\nTHRA\nThailand road accident data from 2019 to 2022\nKaggle\n\n\nTHOSM\nThailand roads open street map in shapefile format\nHDX\n\n\nTHSAB\nThailand - Subnational Administrative Boudaries shapefile\nHDX"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.4-importing-and-launching-r-packages",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.4-importing-and-launching-r-packages",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, four R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nsf - package for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - package with functions for plotting cartographic quality maps\nsPNetwork - provides functions for performing SPPA methods like KDE and K-function on a network. The package can also be used to build spatial matrices to conduct traditional spatial analyses with spatial weights based on reticular distances\nspatstat - package for plotting, EDA and simulation of spatial data\n\n\n\n\npacman::p_load(sf, spNetwork, tmap, tidyverse, spatstat)\n\n\n\n\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.1-thailand-subnational-administrative-boundary-shapefile",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.1-thailand-subnational-administrative-boundary-shapefile",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.1 Thailand Subnational Administrative Boundary, Shapefile",
    "text": "B.1 Thailand Subnational Administrative Boundary, Shapefile\nWe load the Thailand subnational administrative boundary shapefile into an R dataframe using st_read() from the sf package. The source provides the geospatial data in varying levels as indicated by their suffix: country (0), province (1), district (2), and sub-district. (3) For focusing on the BMR, which covers Bangkok and neighboring provinces, province is the most likely level of detail we will need so we will use the code chunk below to load the appropriate layer first.\n\nthsab_prov &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nWe examine the loaded data to confirm the load has been done properly and to get some initial observations of the data.\n\nCalling ObjectChecking crs information with st_crs()\n\n\n\nthsab_prov\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   Shape_Leng Shape_Area                  ADM1_EN       ADM1_TH ADM1_PCODE\n1    2.417227 0.13133873                  Bangkok  กรุงเทพมหานคร       TH10\n2    1.695100 0.07926199             Samut Prakan    สมุทรปราการ       TH11\n3    1.251111 0.05323766               Nonthaburi         นนทบุรี       TH12\n4    1.884945 0.12698345             Pathum Thani        ปทุมธานี       TH13\n5    3.041716 0.21393797 Phra Nakhon Si Ayutthaya พระนครศรีอยุธยา       TH14\n6    1.739908 0.07920961                Ang Thong        อ่างทอง       TH15\n7    5.693342 0.54578838                 Lop Buri          ลพบุรี       TH16\n8    1.778326 0.06872655                Sing Buri         สิงห์บุรี       TH17\n9    2.896316 0.20907828                 Chai Nat         ชัยนาท       TH18\n10   4.766446 0.29208711                 Saraburi         สระบุรี       TH19\n   ADM1_REF ADM1ALT1EN ADM1ALT2EN ADM1ALT1TH ADM1ALT2TH  ADM0_EN   ADM0_TH\n1      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n2      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n3      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n4      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n5      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n6      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n7      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n8      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n9      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n10     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n   ADM0_PCODE       date    validOn    validTo                       geometry\n1          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.6139 13...\n2          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.7306 13...\n3          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3415 14...\n4          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.8916 14...\n5          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.5131 14...\n6          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3332 14...\n7          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((101.3453 15...\n8          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3691 15...\n9          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.1199 15...\n10         TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((101.3994 15...\n\n\n\n\n\nst_crs(thsab_prov)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\nThe output confirms that we have a multipolygon sf object with 77 rows and 17 columns. There is a column named ADM1_EN which appears to contain the province names needed to define the BMR boundaries. It also shows that the dataset is using a coordinate reference system rather than a projected reference system.\nFirst, we reload the data to use a projected reference system and apply the correct reference system with EPSG code of 32647 using st_transform(). This transformation can be confirmed with st_crs() The tmap package is then used to visualize the object to see if it properly depicts the boundaries of Thailand and its provinces.\n\nLoad Object and Transform CRS informationChecking crs information with st_crs()Plot of thsab_prov using tmap\n\n\n\nthsab_prov &lt;- st_read(dsn=\"data/geospatial\",\n                          layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\nst_crs(thsab_prov)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n\ntm_shape(thsab_prov) +\n  tm_polygons(\"grey\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.2-filtering-thsab-for-the-bangkok-metropolitan-region",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.2-filtering-thsab-for-the-bangkok-metropolitan-region",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.2 Filtering THSAB for the Bangkok Metropolitan Region",
    "text": "B.2 Filtering THSAB for the Bangkok Metropolitan Region\nBefore further analyzing the data, we will limit the scope to only consider the Bangkok Metropolitan Region or BMR. This would encompass Bangkok, Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakan, Samut Sakhon. While it is good to get insights outside of BMR, it is out of the study scope and it is best to focus on the objectives.\nThe code chunk below checks if all the provinces in the BMR appear as is under the ADM1_EN column of thsab_prov\n\nfilter(thsab_prov, ADM1_EN %in% c(\"Bangkok\", \"Nonthaburi\",\"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\"))$ADM1_EN\n\n[1] \"Bangkok\"       \"Samut Prakan\"  \"Nonthaburi\"    \"Pathum Thani\" \n[5] \"Nakhon Pathom\" \"Samut Sakhon\" \n\n\nWith the previous code returning all 6 provinces, we have confirmation that the provinces are all present and spelled as is in the data source. We create a new object bmr_boundary to contain only the provinces in BMR. We also take this opportunity to only keep the relevant columns in the dataset using the select() function of dplyr package.\n\nCreate BMR boundary object using filter()Plot of bmr_boundary using tmap\n\n\n\nbmr_boundary &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647) %&gt;%\n  filter(ADM1_EN %in% c(\"Bangkok\", \"Nonthaburi\",\"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\")) %&gt;% dplyr::select(Shape_Leng, Shape_Area, ADM1_EN, geometry)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\ntm_shape(bmr_boundary) +\n  tm_polygons(\"grey\")\n\n\n\n\n\n\n\n\n\n\n\nThe code below creates a second object which is just a union of all the provinces. (i.e., borders between provinces are lost) This is done using the st_union() function.\n\nbmr_full = st_union(bmr_boundary)\n\n\ntm_shape(bmr_full) +\n  tm_polygons(\"grey\")\n\n\n\n\n\n\n\n\nThe code below keeps the final boundary objects into files to make loading more convenient for later analyses.\n\nwrite_rds(bmr_boundary, \"data/rds/bmr_boundary.rds\")\nwrite_rds(bmr_full, \"data/rds/bmr_full.rds\")\n\nThe code below then reloads the same objects into R:\n\nbmr_boundary = read_rds(\"data/rds/bmr_boundary.rds\")\nbmr_full = read_rds(\"data/rds/bmr_full.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.3-road-accident-data-aspatial-csv-file",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.3-road-accident-data-aspatial-csv-file",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.3 Road Accident Data, Aspatial, csv-file",
    "text": "B.3 Road Accident Data, Aspatial, csv-file\nThe road accident data is contained in a csv file. We use the code block in the first tab below to load it into the thra object with some necessary transformations that we identified upon inspecting the raw file. The second tab gives an explanation of the different nested functions used in the code\n\nCode to import and transform road accident dataExplanation of the code lines / functions used\n\n\n\nbmracc &lt;- read_csv(\"data/aspatial/thai_road_accident_2019_2022.csv\")  %&gt;%\n  filter(!is.na(longitude) & longitude != \"\", \n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;%\n  st_transform(crs = 32647)\n\nbmracc &lt;- filter(bmracc, geometry %in% st_intersection(bmr_full, bmracc)) %&gt;%\n  mutate(Year = year(incident_datetime)) %&gt;%\n  mutate(MonthNum = month(incident_datetime)) %&gt;%\n  mutate(Month = month(incident_datetime, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(DayOfWeek = wday(incident_datetime, label = TRUE, abbr = TRUE))\n\n\n\n\nread_csv() used to import a csv file into an R object\nfilter(!is.na(longitude) & longitude != \"\", !is.na(latitude) & latitude != \"\") used to exclude any records where the longitude or latitude information is missing\nst_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) used to convert the dataframe into an sf object using a reference system (WGS84) based on the coordinates\nst_transform(crs = 32647) used to apply the correct EPSG code to the sf object\nfilter(bmracc, geometry %in% st_intersection(bmr_full, bmracc)) used to leave only records which fall within the BMR boundaries\nmutate(...) these lines are used to add additional columns to quickly reference the year, month and day of the week that each accident occured as these dimensions allow for some temporal analyses\n\n\n\n\nCalling the new object shows that it has 12,989 rows across 20 fields.\n\nbmracc\n\nWe use the code chunks below to check the data and visualize the data on the BMR boundary map.\n\ntm_shape(bmr_boundary) +\n  tm_polygons(col = \"grey\") +\n  tm_shape(bmracc) +\n  tm_dots(col = \"red\", size = 0.01, alpha = 0.5)\n\n\n\n\n\n\n\n\nThe code chunk below writes the resulting accident dataset into an rds file for convenient loading.\n\nwrite_rds(bmracc, \"data/rds/bmracc.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.4-thailand-roads-open-streetmap-shapefile",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.4-thailand-roads-open-streetmap-shapefile",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.4 Thailand Roads Open StreetMap, Shapefile",
    "text": "B.4 Thailand Roads Open StreetMap, Shapefile\nThe second geospatial object is the street map shapefile. We will use the object network to contain the final road network for the study.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"hotosm_tha_roads_lines_shp\")\n\nRunning the above code confirms that the dataset is in multilinestring sf format and that it contains 2.8M records across 15 variables. It also shows that there is no CRS applied to the dataset.\nBased on these, the following steps need to be done: apply the right CRS/EPSG code of 32647 or the same as bmr_full, and, filter the network to only include BMR.\nThe code below does the first step of applying a reference system and updating the EPSG code to 32647 using st_set_crs() and st_crs() from the sf package.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"hotosm_tha_roads_lines_shp\") %&gt;%\n  st_make_valid() %&gt;% st_set_crs(4326) %&gt;% st_transform(crs = st_crs(bmr_full))\n\nThe code below then finds the network within BMR by using st_intersection() to find the overlap between the full road network and the BMR boundary. We also include write_rds() in the chunk to store this object into an rds file for easy future loading.\n\nnetwork &lt;- st_intersection(network, bmr_full)\nwrite_rds(network, \"data/rds/network.rds\")\n\nCalling the object name allows us to inspect the contents.\n\nnetwork\n\nThe size of the object has now been reduced to 585K features from the original 2.8M. This still appears a very large number if we want to visualize the data, so we need to inspect if there are any opportunities to reduce the dataset by excluding any irrelevant records.\nThe data includes a column named highway which gives information on the the type or classification of the road.\n\nggplot(network, aes(x = reorder(highway, table(highway)[highway]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Roads by Highway type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"Highway Type\", y = \"Number of Roads\")\n\nThe resulting chart shows that roads with highway type of residential and service make up 522K of the 585K roads in the dataset. We refer to the OpenStreetMap wiki page to see the definition of the different types of highways in the Thailand map and see that these two highway types are access roads for residences or specific buildings. For our objective, we should be able to limit to roads where accidents (are expected to frequently) happen, and only to roads that should be accessible by vehicles. Going through the definition of the highway types, we see that the following 6 types could be out of scope for our study:\n\nresidential - road within a residential area that gives public access to one or multiple residences\nservice - minor road that gives access to buildings or places outside a residential area (e.g., to a religious site, an attraction, part of an estate)\nfootway - pathways designed for pedestrian access\ntrack - road whose only function is to provide access to surrounding land, and is most of the time unpaved\npath - multi-purpose path intended for non-motor vehicles\nsteps\n\nWe can then use the following code chunk which uses the filter() function to remove these classifications from the current network object. We call the object name in the succeeding code chunk to check the new dataset.\n\nbmr_network &lt;- bmr_network %&gt;% \n  filter(!(highway %in% c(\"residential\", \"service\", \"footway\", \"track\", \"path\", \"steps\")))\n\n\nbmr_network\n\nWhile this looks good, it looks like the object is being identified as a GEOMETRY rather than a LINESTRING object. We can use the code below to correct it.\n\nbmr_network &lt;- st_cast(bmr_network, \"LINESTRING\")\n\n\nbmr_network\n\nThe new road network object is now reduced to 34K records or roads which is a 94% reduction in the number of records. We will use some visual inspection to see if this reduction in records will affect our analysis. The two code chunks below plot the road network within the boundaries, while the second plots the three objects together. We use the tmap function to create these maps.\n\nBMR filtered road network onlyBMR filtered road network with road accident dataset\n\n\n\ntm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\") +\n  tm_shape(bmracc) +\n  tm_dots(col = \"red\", alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nFrom these two maps, we see that:\n\nwhile we have filtered 90% of the original records, the resulting map still appears dense, especially in some central areas; and,\nthe road accident locations appear to fall along the network\n\nBased on these, we will go ahead with this version of the network for our analysis.\nThe following code writes the resulting network into an rds file for more convenient loading in the future.\n\nwrite_rds(bmr_network, \"data/rds/bmr_network.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.5-resolving-duplicate-points",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.5-resolving-duplicate-points",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.5 Resolving duplicate points",
    "text": "B.5 Resolving duplicate points\nIn this section, we perform some additional transformations to perform the required analyses.\nFirst, we check the event or accident dataset to see if there are any duplicated data points or locations as the methods require that the points are unique. The code below checks if any duplicate points exist. Note that we specify the column in the argument as we are double-checking duplicate locations rather than completely duplicate records.\n\nany(duplicated(bmracc$geometry))\n\n[1] TRUE\n\n\nAs the code returned TRUE, it confirms the presence of duplicate points, we use st_jitter() from the sf package to introduce some jitter to each point and ensure that points do not lie on the same location. Without any additional arguments, the function uses a default factor 0.002 of the bounding box diagonal as the bounds for the amount of jitter introduced. In the code below, we define an amount of 0.01 instead.\n\nbmracc_jitt &lt;- st_jitter(bmracc, 0.01)\n\nRerunning the check using duplicated() shows that there are no duplicate points anymore.\n\nany(duplicated(bmracc_jitt$geometry))\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#c.1-categories-of-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#c.1-categories-of-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "C.1 Categories of accidents",
    "text": "C.1 Categories of accidents\nWe first would like to understand the different labels we can use from the BMR road accident dataset bmracc The following columns appear to be able to give some insight about the nature of the accident:\n\nvehicle_type\npresumed_cause\naccident_type\nnumber_of_vehicles_involved\nnumber_of_fatalities\nweather_condition\n\nWe will try to be brief in analysing these variables as the main intent is to understand which ones will add the most value to the spatial analysis needed to address the main study objectives.\nNote that while we use bmracc rather than the modified bmracc_jitt in the codes below, the result will be the same as we do not concern ourselves with the geometry innformation yet.\n\nC.1.1 Vehicle Type\nThis variable is intended to give the type of vehicle involved in the accident. We use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(vehicle_type, table(vehicle_type)[vehicle_type]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Vehicle type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nThe output shows that out of 12,989 accidents, 35% are with private or passenger cars, 27% are with pickup trucks and 13% are with motorcycles. These three make up 75% of all the recorded accidents while the remaining 11 types make up the balance 25%\n\n\nC.1.2 Presumed Cause\nThis variable is intended to give the presumed cause of the accident. We again use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(presumed_cause, table(presumed_cause)[presumed_cause]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Presumed Cause\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that 10,146 or 75% of the accidents are presumed to be caused by speeding. The next largest named presumed cause only accounts for 5% of the overall data.\n\n\nC.1.3 Accident Type\nThis variable is intended to give the type or nature of the accident. We again use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(accident_type, table(accident_type)[accident_type]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that “rear-end collisions” and “rollover/fallen on straight road” are the leading causes recorded and account for 83% of the accidents.\n\n\nC.1.4 Number of Fatalities\nThis variable is intended to give the number of fatalities resulting from the accident. We again use the code block below to understand the categories under this variable using a simple histogram created through ggplot().\n\nggplot(bmracc, aes(x = number_of_fatalities)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"darkgrey\") +\n  scale_x_continuous(breaks = seq(min(bmracc$number_of_fatalities), max(bmracc$number_of_fatalities), by = 1)) +\n  labs(title = \"Accidents by Number of Fatalities\", x = \"Number of Fatalities\", y = \"Number of Accidents\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.5)\n\n\n\n\n\n\n\n\nThe plot shows that 94% of the recorded accidents are non-fatal. Only 719 were fatal. Although this is a small number, it might be worth looking at the location of such fatal accidents later. We can use the code below to introduce a new column fatal into the data for more convenient filtering later.\n\nbmracc$fatal &lt;- bmracc$number_of_fatalities &gt; 0\n\n\n\nC.1.5 Weather Condition\nThis variable is intended to indicate the weather condition when the accident was recorded. We first use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(weather_condition, table(weather_condition)[weather_condition]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Weather Condition\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that 90% of the accidents occurred during “clear” weather. Online sources suggest that Bangkok experieces a long rainy season and has 153 rainy days per year, so 10% for the occurrence of accidents appears low. The sources also say that the wettest month is September.\nWe can use the code chunk below to plot the number of accidents that were not recorded on clear weather (i.e., rainy) by month.\n\nggplot(filter(bmracc, !weather_condition == \"clear\"), aes(x = reorder(Month, table(Month)[Month]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Month during Non-clear Weather\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe plot does align with the expectation that September is the wettest month. The very low number of accidents during rainy weather is still questionable though so we will watch out for this if we will use this variable later."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.1.-converting-objects-into-spatstats-formats",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.1.-converting-objects-into-spatstats-formats",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.1. Converting Objects into spatstat’s formats",
    "text": "D.1. Converting Objects into spatstat’s formats\nThe events need to be converted into spatstat’s ppp object using as.ppp()\n\nbmracc_ppp &lt;- as.ppp(st_geometry(bmracc_jitt))\n\nWe then prepare an owin object to define the boundaries using the as.owin() function.\n\nbmr_owin &lt;- as.owin(bmr_full)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.2-kde-for-all-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.2-kde-for-all-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.2 KDE for all Accidents",
    "text": "D.2 KDE for all Accidents\nWe first combine the accidents (all years, all types) into the owin using the following code chunk\n\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n\nWe can then compute for the kde using the density() function. We use the four common methods for automatic bandwidth selection and examine them in a grid using the code chunk below.\n\npar(mfrow=c(2,2))\nplot(density(bmracc_for_kde, sigma=bw.diggle,\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Diggle Method\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Scott's Rule\")\nplot(density(bmracc_for_kde, sigma=bw.CvL(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Cronie & Van Lieshout Criterion\")\nplot(density(bmracc_for_kde, sigma=bw.ppl(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Likelihood Cross Validation\")\n\n\n\n\n\n\n\n\nAmong the four methods, it looks like Scott’s rule (using sigma=bw.scott()) is identifying hot spots unlike the others. There is a hotspot in the southwest and southeast of Bangkok. There also appears to be a high density strip (or maybe a major highway) stretching up northwards.\nWe will use this bandwidth selection method for our succeeding analysis.\n\nD.3 KDE for accidents across years\nWe then want to see if the hotspots move across the years. To do this, we effectively need to compute for the kde across years and see if there are any visible signs of shifts in the hotspots.\nFirst, let us try to understand the distribution of accidents by year. This will help us understand if the numberical values of the density will move because of change in the absolute number of accidents.\nWe use ggplot() in the code chunk below to achieve this.\n\nggplot(bmracc, aes(x = reorder(Year, table(Year)[Year]))) +\n  geom_bar() +\n  ggtitle(\"Number of Accidents by Year\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), vjust = +2) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 12),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that there were significantly more accidents in 2022 compared to the previous years. The other years are within 10% of each other. We should expect that 2022 KDE may have higher numerical values compared to the other years.\nWe can then use the following code to generate four different kde’s, one for each year.\n\npar(mfrow=c(2,2))\nfor (i in c(2019, 2020, 2021, 2022)) {\n  bmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, Year == i)))\n  bmracc_for_kde = bmracc_ppp[bmr_owin]\n  bmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n  plot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = paste(\"KDE for year\",i))\n}\n\n\n\n\n\n\n\n\nWe see that there appears to be a shift between 2020 and 2021. Before 2020, there appeared to be two separate promininet hotspots for accidents– in the central and southeastern portion of the region. However, after 2020, it seems that the accidents are more frequent in the southeastern part, and in a much wider area."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.3-kde-across-months",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.3-kde-across-months",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.3 KDE across Months",
    "text": "D.3 KDE across Months\nWe can apply a similar approach of analysing by month using the code chunk below. For now, we are aggregating accidents by month across all years, so the insights will apply to the whole period and not any particular year.\n\npar(mfrow=c(3,4))\nfor (i in 1:12) {\n  bmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, MonthNum == i)))\n  bmracc_for_kde = bmracc_ppp[bmr_owin]\n  bmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n  plot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = paste(\"KDE for month\",i))\n}\n\n\n\n\n\n\n\n\nThe output reveals no drastic shift in hotspots (using kde) across months. There are months where the intensities and relative intensities differ, but it appears like the hotspots remain in the same areas."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.4-kde-for-clear-vs-rainy-days",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.4-kde-for-clear-vs-rainy-days",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.4 KDE for Clear vs “Rainy” Days",
    "text": "D.4 KDE for Clear vs “Rainy” Days\nThe final analysis we want to perform before moving to network-constrained analysis is on clear vs non-clear days. This is indicated in the field called weather_condition in the accident dataset.\nTo produce the kde visualization, we can use the code chunk below.\n\npar(mfrow=c(1,2))\n\nbmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, weather_condition == \"clear\")))\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n             edge=TRUE,kernel=\"gaussian\"), main = \"KDE for Clear Days\")\n\nbmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, !(weather_condition == \"clear\"))))\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n             edge=TRUE,kernel=\"gaussian\"), main = \"KDE for Rainy Days\")\n\n\n\n\n\n\n\n\nThe output are very similar too some of the charts generated earlier. The hotspot in the center of BMR appears to dissipate during rainy days. (relative to the one in the southeast portion of the region."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.1-preparation-of-data-for-network-constrained-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.1-preparation-of-data-for-network-constrained-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.1 Preparation of Data for Network Constrained Analysis",
    "text": "E.1 Preparation of Data for Network Constrained Analysis\nBefore we are able to perform network-constrained NKDE, we first need to define sample points along the road network, and to do that, we can use the midpoint of the lixels of the network.\n\nE.1.1 Preparing the lixels\nTo lixelize a network, the minimum and (maximum) length of lixels need to be defined. A logical distance needs to be chosen for a given study. In our case, we might no have enough information to understand what road segment length is relevant to group accidents into. However, we can start with understanding the road lengths in the network.\nWe can use the code block below to show the distribution of the road length values using summary() to give the quartiles, and quantile() to give a wider range of view.\n\nsummary(st_length(bmr_network))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n    0.143    31.581   124.691   413.002   433.312 24745.584 \n\nquantile(st_length(bmr_network), probs = seq(.1, .9, by = .1))\n\nUnits: [m]\n       10%        20%        30%        40%        50%        60%        70% \n  13.78247   24.36581   41.45909   71.64269  124.69070  207.11305  337.56988 \n       80%        90% \n 556.85935 1074.26696 \n\n\nThe output shows that there is a very wide range of values. There is also a surprisingly large number of roads (&gt;40%) that are less than 100m– which seem to be too short for typical roads. We can first choose a min distance of 200m which would allow for at least 40% of roads to not be split. As for the maximum length, let us first set it to 600m so only a little over 20% of the roads will be split into smaller segments.\nWe implement this using lixelize_lines() in the code chunk below.\n\nlixels &lt;- lixelize_lines(bmr_network$geometry, \n                         600, \n                         mindist = 200)\n\n\n\nE.1.2 Generating sample points\nThe next step is to define sample points along the network which will be the points where the KDE function will be computed on.\nWe can create sample points on the lixel centers using lines_center() in the code below.\n\nsamples &lt;- lines_center(lixels)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.2-all-years-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.2-all-years-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.2 All Years, All Accidents, Initial Analysis",
    "text": "E.2 All Years, All Accidents, Initial Analysis\nLet’s start by looking at the highest levels. We can compute for the nkde for all accidents in the dataset (2019-2022) using the code chunk below. This uses the accidents with the jitter applied.This code uses a bandwidth of 300m, anduses “quartic” for the kernel function, and uses simple calculation method for the KDE\n\ndensities &lt;- nkde(bmr_network, \n                  events = bmracc_jitt,\n                  w = rep(1, nrow(bmracc_jitt)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe import the densities into the lixel and sample object using the code below. We use a multiple of 1000 to convert the figures from accidents per square meter to accidents per square kilometer\n\nsamples$density_all &lt;- densities*1000000\nlixels$density_all &lt;- densities*1000000\n\nWe use the code below to produce a map with just the calculated densities.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe resulting map surprisingly does not show a lot of high density road segments, which is not aligned to the earlier map with the locations of the accidents. We can check if the jitter has caused displacement of the locations and hidden high density road segments by rerunning the below code chunk which uses the accident locations without jitter applied.\n\ndensities &lt;- nkde(bmr_network, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nTo see if using the original event points will address our concern, we repeat the code chunk below to create a static map.\n\nsamples$density_all &lt;- densities*1000000\nlixels$density_all &lt;- densities*1000000\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nIt looks like the use of the original accident data does little to reveal dense locations. If we examine the interactive map and zoom in, we see one possible reason for the problem. Major roads are being split into multiple semi-parallel roads. These might denote different directions on the same highway, service roads, etc. These might cause accidents on the same “parent” road to be split across their parts.\nWe try to solve this problem by recreating our network while merging such roads into one."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.3-transformation-step---merging-of-parallel-roads",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.3-transformation-step---merging-of-parallel-roads",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.3 Transformation step - merging of parallel roads",
    "text": "E.3 Transformation step - merging of parallel roads\nOne way to merge the roads, is to first use st_buffer() to create a width dimension on the roads. We would prefer to use the actual width of the roads, plus the islands here, but there is no way to do this accurately and this would vary from road to road. (e.g., some roads could have one lane, while others could have four or more lanes) For our case, we will use a width of 2 x 15m, which is based on a ~3m lane width estimate, which means we are buffering up to the width of five lanes or five cars on each side.\nWe use the code chunk below to produce a buffered network.\n\nbmr_network_buffered &lt;- st_buffer(bmr_network, dist = 15)\nbmr_network_dissolved &lt;- st_union(bmr_network_buffered)\n\nThe next step is to convert or cast this into a linestring object, but before that we would want to make sure that the geometries are simple enough so the casting is executed properly. To do this, we fist use st_simplify() which simplifies objects by reducing vertices.\n\nbmr_network_simplified &lt;- st_simplify(bmr_network_dissolved, dTolerance = 1)\n\nWith the network simplified, we can then use st_cast() to convert the geometries back into linestrings. Note that we use two calls since we cannot cast polygons directly into linestrings.\n\nbmr_network_v2 &lt;- st_cast(bmr_network_simplified, \"MULTILINESTRING\")\nbmr_network_v2 &lt;- st_cast(bmr_network_v2, \"LINESTRING\")\n\nNote that the resulting object was a list rather than a dataframe, we can use st_as_sf() to ensure that it is in an sf dataframe format.\n\nbmr_network_v2 &lt;- st_as_sf(bmr_network_v2)\n\nWe can examine the original and new network side by side using tmap_arrange() in the code below to see that the new network has worked sufficiently.\n\norig_network &lt;- tm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\") +\n  tm_layout(title = \"Original Road Network\")\n\nnew_network &lt;- tm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network_v2) +\n  tm_lines(col = \"black\")+\n  tm_layout(title = \"Simplified Road Network\")\n\n\ntmap_arrange(orig_network, new_network, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nThe transformation seems to have worked, and we see by counting the objects using length() st_geometry() in the code below, that the number of roads has been reduced dramatically from 34K to 4.2K– even with the very similar high level map.\n\nlength(st_geometry(bmr_network))\n\n[1] 34056\n\nlength(st_geometry(bmr_network_v2))\n\n[1] 4162\n\n\nLet us examine the road lengths in the updated network using summary() and quantile() in the code chunk below.\n\nsummary(st_length(bmr_network_v2))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n      4.0     326.1    1935.9    4529.2    5826.1 1263640.1 \n\nquantile(st_length(bmr_network_v2), probs = seq(.1, .9, by = .1))\n\nUnits: [m]\n       10%        20%        30%        40%        50%        60%        70% \n  111.8646   236.2789   474.0794  1023.5185  1935.9245  3231.7745  4846.3553 \n       80%        90% \n 7074.5520 10733.3337 \n\n\nThe simplified network now has longer road segments with the median being close to 2km in length.\nWe can then repeat the preparation of data from the creation of the lixels to the creation of the sample points. we will use exactly a similar code using lixelize_lines() as in the earlier sections. Given the distribution of the lengths, we decide to use longer lixel lengths with this new network. We choose 1km and 2km for the parameters.\n\nlixels_v2 &lt;- lixelize_lines(st_geometry(bmr_network_v2), \n                         2000, \n                         mindist = 1000)\nsamples_v2 &lt;- lines_center(lixels_v2) \n\nBefore we move, let us remove the intermediate objects from memory using rm()\n\nrm(bmr_network_buffered)\n\nWarning in rm(bmr_network_buffered): object 'bmr_network_buffered' not found\n\nrm(bmr_network_dissolved)\n\nWarning in rm(bmr_network_dissolved): object 'bmr_network_dissolved' not found\n\nrm(bmr_network_simplified)\n\nWarning in rm(bmr_network_simplified): object 'bmr_network_simplified' not\nfound"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-distribution-of-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-distribution-of-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.4 Distribution of All Accidents, Initial Analysis",
    "text": "E.4 Distribution of All Accidents, Initial Analysis\nWe now rerun the highest level KDE with the updated network to see if we are getting more insightful output.\nWe rerun nkde() to compute the network constrained KDE on the new network and using the new sample points.\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe again transfer these densities into the lixels and sample dataframes using the code chunk below\n\nsamples_v2$density_all &lt;- densities*1000000\nlixels_v2$density_all &lt;- densities*1000000\n\nNext, we can create a static map to show the computed KDEs visually using tmap package in the code chunk below.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWhile the network is simplified, it looks like we still get a very homoegenous, yellow chart. This might be due to the bottom-most bin including zeros. We can inspect the number of zeros and the range of the nonzero kde’s using summary() in the code chunk below.\n\nprint(\"Distribution of all Densities\")\n\n[1] \"Distribution of all Densities\"\n\nsummary(lixels_v2$density_all)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   1.623   0.000 214.461 \n\nprint(\"Distribution of non-zero Densities\")\n\n[1] \"Distribution of non-zero Densities\"\n\nsummary(filter(lixels_v2, density_all &gt; 0)$density_all)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n  0.00004   2.83624   4.71730  11.10733  11.22094 214.46127 \n\n\nIt looks like non-zero vlues are sparse. Less than 25% of the lixles have non-zero values. We can either use a custom palette or add a layer to grey out the zero denisty lixels. We use the latter in the code chunk below. We also add some additional elements like the provinces, and modify the formatting, in order to make the chart more information-rich and readable.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 2, title.col = \"Per sq km\") +\n  tm_shape(filter(lixels_v2, density_all == 0)) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe plot now shows clear road segments where there is higher density. It reveals the most dense segments lie within Bangkok. Pathum Thani and Samut Sakhon also show some high density segments. Meanwhile, Nakohn Pathom appears to have the least accident dense road segments."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-year",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-year",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.5 Distribution of Accidents by Year",
    "text": "E.5 Distribution of Accidents by Year\nWe then look into the distribution of accidents across years to see if there is a change or shift that has occured. To do this, we need to generate the nkde for each year using the code below. The code writes a new column for the lixels and the samples dataframes for each year’s KDE values.\n\nfor (i in 2019:2022) {\n   densities &lt;- nkde(bmr_network_v2, \n                  events = filter(bmracc, Year == i),\n                  w = rep(1, nrow(filter(bmracc, Year == i))),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n  lixels_v2[[paste(\"density_\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n  samples_v2[[paste(\"density_\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n  }\n\nWe can then use the code block below to generate the map of the four different years using tmap package.\n\ncolumns_to_map &lt;- c(\"density_2019\", \"density_2020\", \"density_2021\", \"density_2022\")\nyearly = list()\nfor (col in columns_to_map)\n{\n  yearly[[col]] &lt;- tm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=col, palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_shape(lixels_v2[lixels_v2[[col]] == 0, ]) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = paste(\"Year -\",substr(col,9,12)),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n}\n\n\ntmap_arrange(yearly[[1]], yearly[[2]], yearly[[3]], yearly[[4]], ncol = 2)\n\n\n\n\n\n\n\n\nThe output shows no significant change in the location of the hotspots across years. Before we look at another dimension, let us try to test for complete spatial randomness on the most recent year."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-test-for-csr---2022-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-test-for-csr---2022-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.6 Test for CSR - 2022 Accidents",
    "text": "E.6 Test for CSR - 2022 Accidents\nIt clearly looks like accidents are not randomly or homogeneously distributed in the network. We can verify this using tests for CSR (Complete Spatial Randomness) using the K- or G-functions.\nFor our CSR test, the test hypotheses will be:\n\n\\(H_0\\) - Road accidents in 2022 are randomly distributed along the BMR road network\n\\(H_1\\) - Road accidents in 2022 are not randomly distributed along the BMR road network\n\nThe code chunk below runs these two functions for testing CSR using kfunctions() from the spNetwork package. We specify a range of 0m (start) and 2km (end) to evaluate the function. We also specify 50 Monte Carlo simulations (nsim + 1) to draw the envelope. A confidence interval (1 - conf_int) of 95%, and intervals of 200m for the steps and the donut width. We also use an agg argument to allow consolidation of events. (as the function cannot work with duplicate points)\n\nkfun_bmracc &lt;- kfunctions(bmr_network_v2, \n                             filter(bmracc_jitt, Year == 2022),\n                             start = 0, \n                             end = 2000, \n                             step = 200, \n                             width = 200, \n                             nsim = 49, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05,\n                             agg = 100)\n\nWe can output the K-function by calling on the plotk field, and the G-function by calling on the plotg field of the resulting object\n\nkfun_bmracc$plotg\n\n\n\n\n\n\n\nkfun_bmracc$plotk\n\n\n\n\n\n\n\n\nThe envelop depicts a 95% confidence level CSR interval for each function. Both functions do not support th hypothesis of CSR except for very short intervals. (where the blue lines fall within the envelope) The K-function supports the view on clustering from a distance of around 300m-1.9km, while the G-function supports this from around 250-650m. The G-function supports a view on regular distribution beyond 750m. While these differ in the details, both tests do not support CSR for the distribution of accidents in 2022."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-month",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-month",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.5 Distribution of Accidents by Month",
    "text": "E.5 Distribution of Accidents by Month\nWe can then look into the distribution of accidents across months. Seasonal events including holidays, climate, etc can be linked to the months, so it is good to see if there are months that deviate from most of the others.\nTo do this, we need to generate the nkde for each month similar to the approach for the yearly analysis.\n\nfor (i in 1:12) {\n   densities &lt;- nkde(bmr_network_v2, \n                  events = filter(bmracc, MonthNum == i),\n                  w = rep(1, nrow(filter(bmracc, MonthNum == i))),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n   if (i &gt; 9){\n     lixels_v2[[paste(\"density_M\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n     samples_v2[[paste(\"density_M\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n   }\n   else{\n     lixels_v2[[paste(\"density_M0\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n     samples_v2[[paste(\"density_M0\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n   }\n  \n  }\n\nWe can then use the code block below to generate the map of the different months using tmap package.\n\ncolumns_to_map &lt;- c(\"density_M01\", \"density_M02\", \"density_M03\",\n                    \"density_M04\",\"density_M05\",\"density_M06\",\n                    \"density_M07\", \"density_M08\", \"density_M09\",\n                    \"density_M10\",\"density_M11\",\"density_M12\")\nmonthly = list()\nfor (col in columns_to_map)\n{\n  monthly[[col]] &lt;- tm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=col, palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_shape(lixels_v2[lixels_v2[[col]] == 0, ]) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = paste(\"Month -\",substr(col,10,11)),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n}\n\nTo be able to view the maps clearly, we display them individually in the tabs below.\n\nJanuaryFebruaryMarchAprilMayJuneJulyAugustSeptemberOctoberNovemberDecember\n\n\n\nmonthly[[1]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[2]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[3]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[4]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[5]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[6]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[7]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[8]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[9]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[10]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[11]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[12]]\n\n\n\n\n\n\n\n\n\n\n\nThe output shows a few hotspots arising on specific months. We mention the ones where there are highly dense segments outside Bangkok:\n\nPathum Thani - January, March, June\nSamut Sakhon - January"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-fatal-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-fatal-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.6 Fatal Accidents",
    "text": "E.6 Fatal Accidents\nAs mentioned in the earlier section, only a small 6% or 719 of the total number of accidents were fatal. While small, this is still a large number for the affected families. We expect that such accidents would have also caused more disruption compared to most of the non-fatal ones.\n\nE.6.1 Fatal Accidents by Year and by Province, Non-Network Constrained\nIf we look at the distribution of these accidents across years using the chart below, the annual number has ranged from 153-203, and 2020 and 2021 had 33% more accidents than the other years.\n\nggplot(filter(bmracc, number_of_fatalities &gt; 0), aes(x = reorder(Year, table(Year)[Year]))) +\n  geom_bar() +\n  ggtitle(\"Number of Fatal Accidents by Year\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), vjust = +2) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 12),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nWe can then visualize the location of these using the tmap package. The code below displays a map of the location of the fatal accidents in BMR between 2019 and 2022.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(bmr_network_v2)+\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = \"BMR Fatal Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nIt looks like fatal accidents are more frequent outside Bangkok. With the exception of Nakhon Pathom, it looks like there is a larger number of accidents, in number and in density, happening in the four other provinces.\nIt is hard to judge whether or not there are more accidents in a province using the graph above because of the presence of close or overlapping dots. One approach is to use the functions from sf package to count the accidents or events that fall within each province and then also compute for a density by computing the areas of each province.\nWe first compute for the number of fatal accidents that occur in each province by using st_intersects() function from sf package. We produce this for the total number of accidents and the accidents for each year by using the code chunk below. We create a copy of the bmr_boundary object to store these values.\n\nbmr_with_fatacc &lt;- bmr_boundary %&gt;%\n  mutate('FatAcc19-22' = lengths(st_intersects(bmr_boundary,filter(bmracc, number_of_fatalities &gt; 0)))) %&gt;%\n  mutate('FatAcc19' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2019) )))) %&gt;%\n  mutate('FatAcc20' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2020) )))) %&gt;%\n  mutate('FatAcc21' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2021) )))) %&gt;%\n  mutate('FatAcc22' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2022) ))))\n\nWe then compute for the area of each province using st_area() in the code chunk below.\n\nbmr_with_fatacc$Area &lt;- st_area(bmr_with_fatacc)\n\nFinally, we can compute for the density of accidents in each province by taking the ration of the last two measures we computed. Note that we are multiplying each by 1 million to convert the units from per meter to per square kilometer.\n\nbmr_with_fatacc$FatDensAll &lt;- bmr_with_fatacc$`FatAcc19-22` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens19 &lt;- bmr_with_fatacc$`FatAcc19` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens20 &lt;- bmr_with_fatacc$`FatAcc20` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens21 &lt;- bmr_with_fatacc$`FatAcc21` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens22 &lt;- bmr_with_fatacc$`FatAcc22` / bmr_with_fatacc$Area * 1000000\n\nWe can now compare the occurence of fatal accidents across provinces visually. First, we can produce a choropleth map for the number of accidents and the density of accidents side by side using the code below. This is done by passing a list of arguments for the different tmap elements. In the code below, we use this on the color of the polygons, the label and the chart title.\n\ntm_shape(bmr_with_fatacc)+\n  tm_polygons(col = c('FatAcc19-22', 'FatDensAll'), style = \"equal\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = c(\"Number\", \"Per Sq Km\"))+\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = c(\"Fatal Road Accidents 19-22\",\"Fatal Road Accidents Density\"),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe plot shows that Pathum Thani in the northeast and Samut Prakan in the southeast have the highest number of accidents. However, if we normalize by the area, Nonthaburi in the center and Samut Prakan, still, have the highest density of fatal accidents.\nWe can use the same approach to look at the density across the different years.\n\ntm_shape(bmr_with_fatacc)+\n  tm_polygons(col = c('FatDens19', 'FatDens20', 'FatDens21', 'FatDens22'), style = \"equal\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Per Sq Km\")+\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = c(\"2019 Fatal Accidents\",\"2020 Fatal Accidents\",\"2021 Fatal Accidents\",\"2022 Fatal Accidents\"),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nAt a province-level, Nonthaburi has been consistently the most dense with regards to fatal road accidents. In the meantime, Samut Prakan appears to have increased its density and risen in rank between 2020 and 2021.\nWe will not attempt to perform kde on this set of accidents and go straight to an nkde which considers the road network.’\n\n\nE.6.2 Fatal Accidents, Network Constrained\nThe final analyses we will perform is on the distribution of the fatal accidents along the road network. For this, we will also focus only on one year– 2022, as we recognize a shift in the hotspots over the years, at least across provinces\nTo facilitate the analysis, we create a subset of the accident dataset to only consider fatal accidents and the latest year.\n\nbmr_fatacc_2022 &lt;- filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2022) )\n\n\nbmr_fatacc_2022\n\nSimple feature collection with 159 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 610274.5 ymin: 1489002 xmax: 706582.6 ymax: 1570658\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 159 × 22\n   acc_code incident_datetime   report_datetime     province_th  province_en  \n *    &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;        &lt;chr&gt;        \n 1  5484851 2022-01-01 02:30:00 2022-01-02 02:43:00 สมุทรปราการ   Samut Prakan \n 2  5493686 2022-01-01 05:30:00 2022-01-03 10:23:00 นนทบุรี        Nonthaburi   \n 3  5493725 2022-01-01 14:00:00 2022-01-03 10:28:00 นครปฐม       Nakhon Pathom\n 4  6950193 2022-01-03 21:00:00 2022-09-21 10:53:00 นนทบุรี        Nonthaburi   \n 5  5837858 2022-01-03 23:34:00 2022-03-03 14:19:00 กรุงเทพมหานคร Bangkok      \n 6  6567040 2022-01-04 21:19:00 2022-01-05 11:27:00 สมุทรปราการ   Samut Prakan \n 7  6567055 2022-01-09 22:00:00 2022-01-11 10:56:00 ปทุมธานี       Pathum Thani \n 8  5861456 2022-01-11 18:00:00 2022-03-07 11:28:00 สมุทรปราการ   Samut Prakan \n 9  5579080 2022-01-15 00:10:00 2022-01-15 09:11:00 กรุงเทพมหานคร Bangkok      \n10  5843213 2022-01-16 11:30:00 2022-03-04 10:48:00 ปทุมธานี       Pathum Thani \n# ℹ 149 more rows\n# ℹ 17 more variables: agency &lt;chr&gt;, route &lt;chr&gt;, vehicle_type &lt;chr&gt;,\n#   presumed_cause &lt;chr&gt;, accident_type &lt;chr&gt;,\n#   number_of_vehicles_involved &lt;dbl&gt;, number_of_fatalities &lt;dbl&gt;,\n#   number_of_injuries &lt;dbl&gt;, weather_condition &lt;chr&gt;, road_description &lt;chr&gt;,\n#   slope_description &lt;chr&gt;, geometry &lt;POINT [m]&gt;, Year &lt;dbl&gt;, MonthNum &lt;dbl&gt;,\n#   Month &lt;ord&gt;, DayOfWeek &lt;ord&gt;, fatal &lt;lgl&gt;\n\n\nThere are 159 fatal accidents in the BMR in 2022, and in the new dataset– consistent with the summary in the previous section.\nWe then use the following code chunk to compute for the network-constrained KDE of the 2022 fatal accidents in our network. Note that in the code, we create a duplicate lixels and samples object to keep the original one unchanged. We take this opportunity to widen the computation range by increasing the bandwidth or bw parameter and the max_depth parameter.\n\nsamples_22 &lt;- samples_v2\nlixels_22 &lt;- lixels_v2\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmr_fatacc_2022,\n                  w = rep(1, nrow(bmr_fatacc_2022)),\n                  samples = samples_22,\n                  kernel_name = \"quartic\",\n                  bw = 10000, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 20,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nsamples_22$density_22 &lt;- densities*1000000\nlixels_22$density_22 &lt;- densities*1000000\n\nWe can then use the following codeblock to visualize the nKDE using tmap package.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(lixels_22)+\n  tm_lines(col=\"density_22\", palette = \"-inferno\", lwd = 2, title.col = \"Per sq km\") +\n  tm_shape(filter(lixels_22, density_22 == 0)) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = \"BMR Fatal Road Accident Density - 2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")"
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html",
    "title": "Geographic Accessibility",
    "section": "",
    "text": "We revise the hands-on exercise and look at the calibration in a bit more detail."
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#data-sources",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#data-sources",
    "title": "Geographic Accessibility",
    "section": "Data Sources",
    "text": "Data Sources\nThe data for this exercise comes in the form of four files:\n\n2014 Master Plan Planning subzone boundary in shapefile format sourced from data.gov.sg\nSingapore GIS data with hexagons of 250m radius generated by using st_make_grid() of the sf package\nLocation of eldercare centres sourced from data.gov.sg and in shapefile format\nA distance matrix in csv format from the hexagons to the eldercare centres. The data also contains fields for the entry, exit and network costs which give the distance between roads and the hexagon, between roads and eldercare centres, and between the network points of the hexagon and eldercare centre.\n\nAside from the first dataset, the balance are already processed datasets c/o Prof Kam to be used by his students for this exercise."
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#installing-and-launching-r-packages",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#installing-and-launching-r-packages",
    "title": "Geographic Accessibility",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will load the same R packages. The package ggstatsplot will also be used more in this in-class exercise.\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(tmap, SpatialAcc, sf, \n               ggstatsplot, reshape2,\n               tidyverse)"
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#importing-geospatial-data",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#importing-geospatial-data",
    "title": "Geographic Accessibility",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\nWe use st_read() of sf package to load the three geospatial datasets into R.\n\nLoading Planning SubzoneLoading HexagonsLoading Eldercare Centre Location\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\nhexagons &lt;- st_read(dsn = \"data/geospatial\", layer = \"hexagons\") \n\nReading layer `hexagons' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\n\neldercare &lt;- st_read(dsn = \"data/geospatial\", layer = \"ELDERCARE\") \n\nReading layer `ELDERCARE' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\n\nThe outputs show that all the objects are in sf format. The object mpsz is both in multipolygon class and currently does not have EPSG information."
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#updating-crs-information",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#updating-crs-information",
    "title": "Geographic Accessibility",
    "section": "Updating CRS information",
    "text": "Updating CRS information\nThe code chunk below assigns and ensures that all objects have the same EPSG code of 3414.\n\nmpsz &lt;- st_transform(mpsz, 3414)\neldercare &lt;- st_transform(eldercare, 3414)\nhexagons &lt;- st_transform(hexagons, 3414)"
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#cleaning-and-updating-attribute-fields-of-the-geospatial-data",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#cleaning-and-updating-attribute-fields-of-the-geospatial-data",
    "title": "Geographic Accessibility",
    "section": "Cleaning and updating attribute fields of the geospatial data",
    "text": "Cleaning and updating attribute fields of the geospatial data\nIf we inspect the last two objects, we see that there are a number of redundant or unnecessary fields. We use the code chunk below to only keep the necessary ones using select(). We also add a new field called capacity to both objects using mutate().\n\neldercare &lt;- eldercare %&gt;%\n  select(fid, ADDRESSPOS) %&gt;%\n  mutate(capacity = 100)\n\nhexagons &lt;- hexagons %&gt;%\n  select(fid) %&gt;%\n  mutate(demand = 100)\n\nFor this exercise we use a dummy value of 100 for the capacity, but, in practice, this number needs to be updated with the actual capacity of the location."
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#importing-the-distance-matrix",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#importing-the-distance-matrix",
    "title": "Geographic Accessibility",
    "section": "Importing the Distance Matrix",
    "text": "Importing the Distance Matrix\nThe code chunk below uses read_csv() to load the distance matrix into R\n\nODMatrix &lt;- read_csv(\"data/aspatial/OD_Matrix.csv\", skip = 0)\n\nRows: 375000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): origin_id, destination_id, entry_cost, network_cost, exit_cost, tot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe data is already preprocessed, but if one wants to generate the distances themselves, they may use the package r5r.\nWe can inspect the object using head()\n\nhead(ODMatrix)\n\n# A tibble: 6 × 6\n  origin_id destination_id entry_cost network_cost exit_cost total_cost\n      &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1         1              1       668.       19847.      47.6     20562.\n2         1              2       668.       45027.      31.9     45727.\n3         1              3       668.       17644.     173.      18486.\n4         1              4       668.       36010.      92.2     36770.\n5         1              5       668.       31068.      64.6     31801.\n6         1              6       668.       31195.     117.      31980."
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#tidying-the-distance-matrix",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#tidying-the-distance-matrix",
    "title": "Geographic Accessibility",
    "section": "Tidying the distance matrix",
    "text": "Tidying the distance matrix\nBased on our inspection, we see that the distance matrix is not yet in the typical matrix format where the origin is on one axis and the destination is on another. Instead, the origins and the destinations are in their respective columns and there are fields for the distances.\nIn order to transform the object into matrix format, we use spread() of tidyr in the code chunk below. pivot_wider() can also be used to achieve the same result\n\ndistmat &lt;- ODMatrix %&gt;%\n  select(origin_id, destination_id, total_cost) %&gt;%\n  spread(destination_id, total_cost)%&gt;%\n  select(c(-c('origin_id')))\n\n\nclass(distmat)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe matrix is still in dataframe format and is in meters. We use the code below to convert it to km and turn it into a matrix.\n\ndistmat_km &lt;- as.matrix(distmat/1000)\n\n\nclass(distmat_km)\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#computing-hansen-accessibility",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#computing-hansen-accessibility",
    "title": "Geographic Accessibility",
    "section": "Computing Hansen Accessibility",
    "text": "Computing Hansen Accessibility\nTo compute for the Hansen accessibility we use ac() of the SpatialAcc package. The code chunk below does this and also uses data.frame() to convert the output into dataframe format.\n\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\n\nglimpse(acc_Hansen)\n\nRows: 3,125\nColumns: 1\n$ ac.hexagons.demand..eldercare.capacity..distmat_km..power...2.. &lt;dbl&gt; 1.6483…\n\n\nWe see that the function returns one field and its default field name is a unreadable. We can fix this by assigning a new name to colnames()\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"\n\n\nglimpse(acc_Hansen)\n\nRows: 3,125\nColumns: 1\n$ accHansen &lt;dbl&gt; 1.648313e-14, 1.096143e-16, 3.865857e-17, 1.482856e-17, 1.05…\n\n\nWe then convert the accessibility measure to tibble format and then bind it with the hexagon data frame using the two lines of code in the following code chunk\n\nacc_Hansen &lt;- tibble::as_tibble(acc_Hansen)\n\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)"
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#visualising-hansens-accessibility",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#visualising-hansens-accessibility",
    "title": "Geographic Accessibility",
    "section": "Visualising Hansen’s Accessibility",
    "text": "Visualising Hansen’s Accessibility\nWe first extract the extent of the hexagons object using st_bbox() of sf package\n\nmapex &lt;- st_bbox(hexagons)\n\nWe use tmap package in the code chunk below to create a visualization of the accessibility of eldercare centres across Singapore using Hansen method.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "In-class/In-class_Ex08/In-class_Ex08.html#statistical-graphical-representation",
    "href": "In-class/In-class_Ex08/In-class_Ex08.html#statistical-graphical-representation",
    "title": "Geographic Accessibility",
    "section": "Statistical graphical representation",
    "text": "Statistical graphical representation\nWe can compare the distribution of the accessibility values (using Hansen method) across planning regions.\nFirst, we need to include the planning region field into hexagon_Hansen object by using st_join() in the code chunk below.\n\nhexagon_Hansen &lt;- st_join(hexagon_Hansen, mpsz, \n                          join = st_intersects)\n\nWe then use ggbetweenstats() to produce a more analytic statistical visual.\n\nggbetweenstats(data = hexagon_Hansen,\n               x= REGION_N,\n               y = accHansen,\n               type = \"p\")"
  },
  {
    "objectID": "In-class/In-class_Ex08/data/geospatial/ELDERCARE.html",
    "href": "In-class/In-class_Ex08/data/geospatial/ELDERCARE.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  ELDERCARE  ENG dataset\n\nELDERCARE\n\n                 0 0     false"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "",
    "text": "In this in-class exercise, we learn to use GWR or geographically weighted regression. We use sf methods unlike the ones used in the corresponding hands-oon exercise"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#data-sources",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#data-sources",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Data Sources",
    "text": "Data Sources\nTo datasets will be used for this exercise:\n\n2014 Master Plan subzone boundary in shapefile format\n2015 condo resale prices in csv format"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#installing-and-launching-r-packages",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#installing-and-launching-r-packages",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of the following R packages:\n\nolsrr - for building OLS (ordinary least squares) regression models and performing diagnostic tests\nGWmodel - for calibrating geographically weighted family of models\ntmap - for plotting cartographic quality maps\ncorrplot - for multivariate data visualization and analysis\nsf - spatial data handling\ntidyverse - attribute data handling\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, ggstatsplot, sfdep)"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#geospatial-data-loading-and-preparation",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#geospatial-data-loading-and-preparation",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Geospatial data loading and preparation",
    "text": "Geospatial data loading and preparation\nThe code chunk below uses st_read() of the sf package to load the geospatial data and apply the right EPSG code to convert it to svy21.\n\nmpsz_svy21 = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\") %&gt;%\n  st_transform(3414) %&gt;%\n  st_make_valid()\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#aspatial-data-loading",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#aspatial-data-loading",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Aspatial data loading",
    "text": "Aspatial data loading\nThe code chunk below uses read_csv() of readr to import the 2015 condo resale prices from the csv file.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#converting-aspatial-dataframe-into-an-sf-object",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#converting-aspatial-dataframe-into-an-sf-object",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Converting aspatial dataframe into an sf object",
    "text": "Converting aspatial dataframe into an sf object\nTo convert the condo_resale object into a spatial object, we can use the following code chunk that utilizes st_as_sf() from sf package. The final line of the code chunk converts the data frame from wgs84 to svy21 using the indicated crs values.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#checking-for-correlation",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#checking-for-correlation",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Checking for correlation",
    "text": "Checking for correlation\nIn the hands-on exercise, we used corrplot() to generate the correlation plot.\nAn alternative approach is to use ggcorrmat() of ggstatsplot which requires a simpler line of code.\n\nggcorrmat(condo_resale[, 5:23])"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#using-olsrr-to-display-results-and-for-vif",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#using-olsrr-to-display-results-and-for-vif",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Using olsrr to display results and for VIF",
    "text": "Using olsrr to display results and for VIF\nWe can use the olsrr package to run diagnostic tests on the model.\nWe can pass the results to ols_regress() to produce a formatted model report, better than the one coming from summary().\n\nols_regress(condo.mlr)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     750537.537 \nR-Squared                    0.652       MSE                571262902261.223 \nAdj. R-Squared               0.647       Coef. Var                    43.160 \nPred R-Squared               0.637       AIC                       42971.173 \nMAE                     412117.987       SBC                       43081.835 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.515738e+15          19        7.977571e+13    139.648    0.0000 \nResidual      8.089083e+14        1416    571262902261.223                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     543071.420    136210.918                   3.987    0.000     275874.535     810268.305 \n            AREA_SQM      12688.669       370.119        0.579     34.283    0.000      11962.627      13414.710 \n                 AGE     -24566.001      2766.041       -0.166     -8.881    0.000     -29991.980     -19140.022 \n            PROX_CBD     -78121.985      6791.377       -0.267    -11.503    0.000     -91444.227     -64799.744 \n      PROX_CHILDCARE    -333219.036    111020.303       -0.087     -3.001    0.003    -551000.984    -115437.089 \n    PROX_ELDERLYCARE     170949.961     42110.748        0.083      4.060    0.000      88343.803     253556.120 \nPROX_URA_GROWTH_AREA      38507.622     12523.661        0.059      3.075    0.002      13940.700      63074.545 \n  PROX_HAWKER_MARKET      23801.197     29299.923        0.019      0.812    0.417     -33674.725      81277.120 \n   PROX_KINDERGARTEN     144097.972     82738.669        0.030      1.742    0.082     -18205.570     306401.514 \n            PROX_MRT    -322775.874     58528.079       -0.123     -5.515    0.000    -437586.937    -207964.811 \n           PROX_PARK     564487.876     66563.011        0.148      8.481    0.000     433915.162     695060.590 \n    PROX_PRIMARY_SCH     186170.524     65515.193        0.072      2.842    0.005      57653.253     314687.795 \nPROX_TOP_PRIMARY_SCH       -477.073     20597.972       -0.001     -0.023    0.982     -40882.894      39928.747 \n  PROX_SHOPPING_MALL    -207721.520     42855.500       -0.109     -4.847    0.000    -291788.613    -123654.427 \n    PROX_SUPERMARKET     -48074.679     77145.257       -0.012     -0.623    0.533    -199405.956     103256.599 \n       PROX_BUS_STOP     675755.044    138551.991        0.133      4.877    0.000     403965.817     947544.272 \n         NO_Of_UNITS       -216.180        90.302       -0.046     -2.394    0.017       -393.320        -39.040 \n     FAMILY_FRIENDLY     142128.272     47055.082        0.056      3.020    0.003      49823.107     234433.438 \n            FREEHOLD     300646.543     77296.529        0.117      3.890    0.000     149018.525     452274.561 \n      LEASEHOLD_99YR     -77137.375     77570.869       -0.030     -0.994    0.320    -229303.551      75028.801 \n-----------------------------------------------------------------------------------------------------------------\n\n\nWe can then use ols_vif_tol() to run the test for multicollinearity using the VIF or variance inflation factor.\n\nfilter(ols_vif_tol(condo.mlr), VIF &gt; 5) # sign of multicollinearity\n\n[1] Variables Tolerance VIF      \n&lt;0 rows&gt; (or 0-length row.names)\n\nfilter(ols_vif_tol(condo.mlr), VIF &lt;= 5) # no sign of multicollinearity\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8601326 1.162611\n2                   AGE 0.7011585 1.426211\n3              PROX_CBD 0.4575471 2.185567\n4        PROX_CHILDCARE 0.2898233 3.450378\n5      PROX_ELDERLYCARE 0.5922238 1.688551\n6  PROX_URA_GROWTH_AREA 0.6614081 1.511926\n7    PROX_HAWKER_MARKET 0.4373874 2.286303\n8     PROX_KINDERGARTEN 0.8356793 1.196631\n9              PROX_MRT 0.4949877 2.020252\n10            PROX_PARK 0.8015728 1.247547\n11     PROX_PRIMARY_SCH 0.3823248 2.615577\n12 PROX_TOP_PRIMARY_SCH 0.4878620 2.049760\n13   PROX_SHOPPING_MALL 0.4903052 2.039546\n14     PROX_SUPERMARKET 0.6142127 1.628100\n15        PROX_BUS_STOP 0.3311024 3.020213\n16          NO_Of_UNITS 0.6543336 1.528272\n17      FAMILY_FRIENDLY 0.7191719 1.390488\n18             FREEHOLD 0.2728521 3.664990\n19       LEASEHOLD_99YR 0.2645988 3.779307\n\n\nAs there is no variable with VIF value above 5, we are ensured that there is no signs of multicollinearity using this criterion."
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#using-olsrr-for-variable-selection-using-stepwise-regression",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#using-olsrr-for-variable-selection-using-stepwise-regression",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Using olsrr for variable selection using stepwise regression",
    "text": "Using olsrr for variable selection using stepwise regression\nForward stepwise regression adds variables one at a time from an empty model by picking the one with the highest rank for a criteria (and within a threshold) and that improves the model. (i.e., adj R squared) The criteria is typically the significance level (e.g., choose minimum p-value below 0.05)\nols_step_forward_p() performs forward stepwise regression using the p-value. Other criteria include AIC, BIC, r-squared. A 0.05 max p-value is defined using the p_val argument. The details argument instructs whether the results are printed out while each step is run.\n\ncondo_fw_mlr &lt;- ols_step_forward_p(condo.mlr, p_val = 0.05, details = FALSE)\n\nWe can pass the ols regression object into plot() to display the results graphically. The charts show the improvement of four of the model metrics with each variable added.\n\nplot(condo_fw_mlr)\n\n\n\n\n\n\n\n\nWe can plot the residuals using ols_plot_resid_fit() and then passing the model object inside the mlr object. This tests for the linearity assumption.\n\nols_plot_resid_fit(condo_fw_mlr$model)\n\n\n\n\n\n\n\n\nWe can plot the residuals using ols_plot_resid_hist() to test for the normality assumption graphically.\n\nols_plot_resid_hist(condo_fw_mlr$model)\n\n\n\n\n\n\n\n\nWe can also do it using the traditional stats using the following\n\nols_test_normality(condo_fw_mlr$model)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the one-sample Kolmogorov-Smirnov test\n\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------"
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#testing-spatial-autocorrelation",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#testing-spatial-autocorrelation",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Testing spatial autocorrelation",
    "text": "Testing spatial autocorrelation\nWe export the residuals of the hedonic pricing model as a dataframe\n\nmlr_output &lt;- as.data.frame(condo_fw_mlr$model$residuals) %&gt;%\n  rename('FW_MLR_RES' = 'condo_fw_mlr$model$residuals')\n\nWe then join the new dataframe to the sf object.\n\ncondo_resale.sf &lt;- cbind(condo_resale.sf,\n                         mlr_output$FW_MLR_RES) %&gt;%\n  rename('MLR_RES' = 'mlr_output.FW_MLR_RES')\n\nWith this, we can produce an interactive map of the residuals using the sf object.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntmap_options(check.and.fix = TRUE) # can be added to the layer with a problem\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\", title = \"Residual\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nThere appears to be clusters with high residuals– there appears to be signs of spatial autocorrelation.\nTo prove our suspicions, we conduct Moran’s I using the sfdep package (without needing to convert and use spdep as in the hands-on exercise)\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(nb = st_knn(geometry, k = 6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nWe can then run the Global Moran’s I permutation test directly on the object\n\nglobal_moran_perm(condo_resale.sf$MLR_RES,\n                  condo_resale.sf$nb,\n                  condo_resale.sf$wt,\n                  alternative = \"two.sided\",\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.32254, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nSince the test p-value is less than 0.05, the result is significant, and as the test statistic is positive, then there are signs of clusters.\nAs there is spatial autocorrelation, we can build the GWR model."
  },
  {
    "objectID": "In-class/In-class_Ex06/In-class_Ex06.html#using-fixed-bandwidth-method",
    "href": "In-class/In-class_Ex06/In-class_Ex06.html#using-fixed-bandwidth-method",
    "title": "Geographically Weighted Regression Model (sf methods)",
    "section": "Using fixed bandwidth method",
    "text": "Using fixed bandwidth method\nThe following code chunk derives the optimal fixed bandwidth for the model from the regression method (have to type in full as it cannot take in the ols regression object.\n\nbw_fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK +\n                  PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\n                  NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                  data = condo_resale.sf,\n                  approach = \"CV\",\n                  kernel = \"gaussian\",\n                  adaptive = FALSE,\n                  longlat = FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe results show that ~971.3405 is the recommended bandwidth.\nThe code below calibrates the gwr model using fixed bandwidth\n\ngwr_fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK +\n                  PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\n                  NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                  data = condo_resale.sf,\n                  bw = bw_fixed,\n                  kernel = \"gaussian\",\n                  longlat = FALSE)\n\nThe output is saved in a list of class gwrm\n\ngwr_fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-14 22:19:43.205686 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf, bw = bw_fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2024-10-14 22:19:43.872735 \n\n\nThe resulting object has a field named SDF which contains the parameter estimates. To visualize elements of it, we transfer this information into the sf object. (only considering a few columns)\n\ngwr_fixed_output &lt;- as.data.frame(gwr_fixed$SDF) %&gt;%\n  select(-c(2:15))\n\ngwr_sf_fixed &lt;- cbind(condo_resale.sf, gwr_fixed_output)\n\nWe can use glimpse() to check the contents of the new object\n\nglimpse(gwr_sf_fixed)\n\nRows: 1,436\nColumns: 63\n$ nb                      &lt;nb&gt; &lt;66, 77, 123, 238, 239, 343&gt;, &lt;21, 162, 163, 19…\n$ wt                      &lt;list&gt; &lt;0.1666667, 0.1666667, 0.1666667, 0.1666667, …\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               &lt;dbl&gt; 1580824.71, 1509406.28, 3583211.16, -444860.49…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2900355.0, 3499796.0, 3628135.3, 5359292.0, 13…\n$ residual                &lt;dbl&gt; 99644.96, 380204.01, -303135.30, -1109292.00, …\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.58164609, 1.00012683, -0.88571524, -2.513378…\n$ Intercept_SE            &lt;dbl&gt; 3395011.7, 1352467.0, 1339841.1, 370353.7, 242…\n$ AREA_SQM_SE             &lt;dbl&gt; 1580.6464, 1221.7304, 1119.0686, 616.3396, 154…\n$ AGE_SE                  &lt;dbl&gt; 15224.251, 9536.861, 7725.609, 5978.954, 9264.…\n$ PROX_CBD_SE             &lt;dbl&gt; 156762.55, 72009.21, 79937.75, 359165.60, 4780…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 568613.2, 463408.9, 398867.7, 305347.0, 806691…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 579655.1, 156842.5, 178208.6, 118670.9, 414128…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 422324.6, 191103.4, 125333.4, 366726.0, 563178…\n$ PROX_MRT_SE             &lt;dbl&gt; 606123.6, 560744.6, 334275.1, 271991.5, 454983…\n$ PROX_PARK_SE            &lt;dbl&gt; 399605.9, 435453.7, 374942.3, 216766.2, 469595…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 512060.9, 268609.3, 238048.9, 226860.0, 278753…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 482696.6, 239509.0, 142155.9, 153273.0, 376752…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 1508504.5, 636969.8, 518721.9, 543058.9, 81911…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 806.8444, 266.4264, 234.1108, 324.0807, 353.41…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 251502.9, 162760.6, 173178.3, 107958.2, 182177…\n$ FREEHOLD_SE             &lt;dbl&gt; 370362.8, 205263.5, 165806.1, 134885.6, 237553…\n$ Intercept_TV            &lt;dbl&gt; 0.4656316, 1.1160393, 2.6743554, -1.2011773, 0…\n$ AREA_SQM_TV             &lt;dbl&gt; 6.162087, 12.196149, 11.585038, 32.977013, 4.3…\n$ AGE_TV                  &lt;dbl&gt; -0.62722500, -4.56438096, -3.26172325, -15.228…\n$ PROX_CBD_TV             &lt;dbl&gt; -0.26572481, -2.24375590, -3.39501541, 3.74802…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 0.314199728, 0.596072954, -0.662701339, 1.8528…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -0.61040359, 1.27422394, 3.26841613, 2.1260488…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -0.51714116, 0.30511964, -2.01288530, -4.31616…\n$ PROX_MRT_TV             &lt;dbl&gt; -0.68841147, -4.18708291, -2.84832358, -6.7173…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.54239001, 0.92304803, 0.76850923, -2.802534…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 0.5897363, 2.7676171, 2.3052085, 13.1016452, 0…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 0.72864945, -1.32424806, -1.12186305, -0.69972…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 0.79146959, 2.90629848, 2.87414548, 11.7683749…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.59112146, -0.80578975, 0.18687430, -0.615698…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; 0.235784421, 0.665313652, -0.356298596, 13.572…\n$ FREEHOLD_TV             &lt;dbl&gt; 0.86274374, 1.82752304, 0.94991015, 8.41398102…\n$ Local_R2                &lt;dbl&gt; 0.9473297, 0.9136782, 0.8989196, 0.8994818, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n$ geometry.1              &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\nWe can then plot using tmap\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_fixed) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this exercise, we are introduced to the sfdep package which is a wrapper on spdep and enables us to work directly with sf objects. It is also written in such a way to fully take advantage of the tidyverse framework."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#data-preparation",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#data-preparation",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualization-of-the-development-indicator",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualization-of-the-development-indicator",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package.\n\ntm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Hunan GDP per capita\")"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-1-computing-deriving-queens-contiguity-weights",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-1-computing-deriving-queens-contiguity-weights",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 1: Computing Deriving Queen’s Contiguity Weights",
    "text": "Step 1: Computing Deriving Queen’s Contiguity Weights\nWe use the code chunk below to compute for the contiguity weight matrix using Queen’s criterion.\n\nwm_q &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style=\"W\"),\n         .before=1)\n\nThe st_weights() function allows three arguments:\n\nnb -\nstyle -\nallow_zero -"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-2a-performing-global-morans-i-test",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-2a-performing-global-morans-i-test",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 2a: Performing Global Moran’s I Test",
    "text": "Step 2a: Performing Global Moran’s I Test\nThe Global Moran’s I test can be performed using global_moran_test() of the sfdep package.\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nAt α=0.05, the test shows that we reject a null hypothesis that the GDPPC values are randomly distributed. As the test statistic is above 0, then the data is showing signs of clustering."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-2b-performing-global-morans-i-permutation-test",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-2b-performing-global-morans-i-permutation-test",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 2b: Performing Global Moran’s I Permutation Test",
    "text": "Step 2b: Performing Global Moran’s I Permutation Test\nMonte Carlo simulation on the (Global Moran’s I) statistic is performed using global_moran_perm() of the sfdep package. The code chunk below performs 100 simulations (nsim + 1)\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nWe get consistent result with the one-time run, but with a lower p-value. (and higher confidence)"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-morans-i",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-morans-i",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Moran’s I",
    "text": "Computing Local Moran’s I\nWe compute for the local Moran’s I statistic for each unit by using local_moran() of sfdep package. The unnest() function expands the elements of list local_moran as separate columns in the lisa object.\n\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe can examine the columns of lisa using the code chunk below.\n\nglimpse(lisa)\n\nRows: 88\nColumns: 21\n$ ii           &lt;dbl&gt; -1.468468e-03, 2.587817e-02, -1.198765e-02, 1.022468e-03,…\n$ eii          &lt;dbl&gt; 0.0017692414, 0.0064149158, -0.0374068734, -0.0000348833,…\n$ var_ii       &lt;dbl&gt; 4.179959e-04, 1.051040e-02, 1.020555e-01, 4.367565e-06, 1…\n$ z_ii         &lt;dbl&gt; -0.15836231, 0.18984794, 0.07956903, 0.50594053, 0.448752…\n$ p_ii         &lt;dbl&gt; 0.874171311, 0.849428289, 0.936580031, 0.612898396, 0.653…\n$ p_ii_sim     &lt;dbl&gt; 0.82, 0.96, 0.76, 0.64, 0.50, 0.82, 0.08, 0.08, 0.02, 0.2…\n$ p_folded_sim &lt;dbl&gt; 0.41, 0.48, 0.38, 0.32, 0.25, 0.41, 0.04, 0.04, 0.01, 0.1…\n$ skewness     &lt;dbl&gt; -0.8122108, -1.0905447, 0.8239085, 1.0401038, 1.6357304, …\n$ kurtosis     &lt;dbl&gt; 0.651875433, 1.889177462, 0.046095140, 1.613439800, 3.960…\n$ mean         &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ median       &lt;fct&gt; High-High, High-High, High-High, High-High, High-High, Hi…\n$ pysal        &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ nb           &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1,…\n$ wt           &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0…\n$ NAME_2       &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"C…\n$ ID_3         &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2…\n$ NAME_3       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ ENGTYPE_3    &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"C…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7…\n$ geometry     &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.228…\n\n\nThe local_moran() function generated 12 columns– which are the first twelve in the lisa dataframe. Key columns are:\n\nii - local Moran i statistic\np_ii_sim - p value from simulation\nFor the clustering / outlier classification, there are three options in different columns: mean, median, pysal."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-local-moran-is",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-local-moran-is",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualising Local Moran I’s",
    "text": "Visualising Local Moran I’s\nThe code chunk below prepares a choropleth map of the statistic in the ii and the p_ii_sim field\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(c(\"ii\", \"p_ii_sim\"), title = c(\"Local Moran's I\",\"P Value\")) +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"Local Moran's I and P-values\")\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#lisa-map",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#lisa-map",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "LISA map",
    "text": "LISA map\nA LISA map is a categorical map showing outliers and clusters.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-gi-statistics",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-gi-statistics",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Gi* Statistics",
    "text": "Computing Local Gi* Statistics\nThe code below computes the weight matrix using inverse distance.\n\nwm_idw &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before=1)\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wts = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nWe then compute the local Gi* by using the code below.\n\nHCSA &lt;- wm_idw %&gt;%\n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0416 Low     0.0114 0.00000641  0.0493 9.61e-1  0.7          0.35    0.875\n 2 -0.333  Low     0.0106 0.00000384 -0.0941 9.25e-1  1            0.5     0.661\n 3  0.281  High    0.0126 0.00000751 -0.151  8.80e-1  0.9          0.45    0.640\n 4  0.411  High    0.0118 0.00000922  0.264  7.92e-1  0.6          0.3     0.853\n 5  0.387  High    0.0115 0.00000956  0.339  7.34e-1  0.62         0.31    1.07 \n 6 -0.368  High    0.0118 0.00000591 -0.583  5.60e-1  0.72         0.36    0.594\n 7  3.56   High    0.0151 0.00000731  2.61   9.01e-3  0.06         0.03    1.09 \n 8  2.52   High    0.0136 0.00000614  1.49   1.35e-1  0.2          0.1     1.12 \n 9  4.56   High    0.0144 0.00000584  3.53   4.17e-4  0.04         0.02    1.23 \n10  1.16   Low     0.0104 0.00000370  1.82   6.86e-2  0.12         0.06    0.416\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-gi",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-gi",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualising Gi*",
    "text": "Visualising Gi*\nThe code chunk\n\ntm_shape(HCSA) +\n  tm_polygons()+\ntm_shape(filter(HCSA,p_sim &lt; 0.05)) +\n  tm_polygons(c(\"cluster\",\"p_sim\"), title=c(\"Cluster\",\"P-Value\"))"
  },
  {
    "objectID": "In-class/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class/In-class_Ex02/In-class_Ex02.html",
    "title": "Spatial Point Pattern Analysis",
    "section": "",
    "text": "The exercise uses the data sources to be used in the upcoming Take Home Exercise:\n\nThailand Road accident data from 2019-2022 from Kaggle\nThailand Roads OpenStreetMap from HDX\nThailand - Subnational Administrative Boudaries shapefile from HDX\n\n\n\n\nThis exercise will make use of four R packages: sf, spatstat, tidyverse, maptools and tmap.\nThe code chunk below imports the already retired. We can still download it from Posit Public Package Manager snapshots by using the code below.\n\ninstall.packages(\"maptools\",\n                 repos = \"https://packagemanager.posit.com/cran/2023-10-13\")\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, tidyverse, tmap, spatstat, maptools)"
  },
  {
    "objectID": "In-class/In-class_Ex02/In-class_Ex02.html#data-sources",
    "href": "In-class/In-class_Ex02/In-class_Ex02.html#data-sources",
    "title": "Spatial Point Pattern Analysis",
    "section": "",
    "text": "The exercise uses the data sources to be used in the upcoming Take Home Exercise:\n\nThailand Road accident data from 2019-2022 from Kaggle\nThailand Roads OpenStreetMap from HDX\nThailand - Subnational Administrative Boudaries shapefile from HDX"
  },
  {
    "objectID": "In-class/In-class_Ex02/In-class_Ex02.html#installing-and-launching-r-packages",
    "href": "In-class/In-class_Ex02/In-class_Ex02.html#installing-and-launching-r-packages",
    "title": "Spatial Point Pattern Analysis",
    "section": "",
    "text": "This exercise will make use of four R packages: sf, spatstat, tidyverse, maptools and tmap.\nThe code chunk below imports the already retired. We can still download it from Posit Public Package Manager snapshots by using the code below.\n\ninstall.packages(\"maptools\",\n                 repos = \"https://packagemanager.posit.com/cran/2023-10-13\")\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, tidyverse, tmap, spatstat, maptools)"
  },
  {
    "objectID": "In-class/In-class_Ex02/In-class_Ex02.html#importing-the-aspatial-data-and-converting-to-sf",
    "href": "In-class/In-class_Ex02/In-class_Ex02.html#importing-the-aspatial-data-and-converting-to-sf",
    "title": "Spatial Point Pattern Analysis",
    "section": "Importing the Aspatial Data and converting to sf",
    "text": "Importing the Aspatial Data and converting to sf\nThe Thailand road accident data is in csv format but contains a field for longitude and another for latitude.\n\nrdacc_sf &lt;- read_csv(\"data/aspatial/thai_road_accident_2019_2022.csv\")  %&gt;%\n  filter(!is.na(longitude) & longitude != \"\",\n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n  st_transform(crs = 32647)\n\nRows: 81735 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): province_th, province_en, agency, route, vehicle_type, presumed_c...\ndbl   (6): acc_code, number_of_vehicles_involved, number_of_fatalities, numb...\ndttm  (2): incident_datetime, report_datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe data appears to have been loaded properly. With the code chunk below, we confirm the data is loaded with a little loss of data given we have filtered out records with invalid coordinates.\n\nrdacc_sf\n\nSimple feature collection with 81376 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -16183190 ymin: -936.228 xmax: 1200243 ymax: 4918525\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 81,376 × 17\n   acc_code incident_datetime   report_datetime     province_th province_en     \n *    &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;       &lt;chr&gt;           \n 1   571905 2019-01-01 00:00:00 2019-01-02 06:11:00 ลพบุรี        Loburi          \n 2  3790870 2019-01-01 00:03:00 2020-02-20 13:48:00 อุบลราชธานี   Ubon Ratchathani\n 3   599075 2019-01-01 00:05:00 2019-01-01 10:35:00 ประจวบคีรีขันธ์ Prachuap Khiri …\n 4   571924 2019-01-01 00:20:00 2019-01-02 05:12:00 เชียงใหม่     Chiang Mai      \n 5   599523 2019-01-01 00:25:00 2019-01-04 09:42:00 นครสวรรค์    Nakhon Sawan    \n 6   571982 2019-01-01 00:30:00 2019-01-07 12:46:00 แม่ฮ่องสอน    Mae Hong Son    \n 7   612782 2019-01-01 00:30:00 2019-10-25 14:25:00 ชุมพร        Chumphon        \n 8   599235 2019-01-01 00:35:00 2019-01-02 16:23:00 สิงห์บุรี       Sing Buri       \n 9   600643 2019-01-01 00:40:00 2019-01-11 10:01:00 สงขลา       Songkhla        \n10   599105 2019-01-01 00:45:00 2019-01-01 10:11:00 ตราด        Trat            \n# ℹ 81,366 more rows\n# ℹ 12 more variables: agency &lt;chr&gt;, route &lt;chr&gt;, vehicle_type &lt;chr&gt;,\n#   presumed_cause &lt;chr&gt;, accident_type &lt;chr&gt;,\n#   number_of_vehicles_involved &lt;dbl&gt;, number_of_fatalities &lt;dbl&gt;,\n#   number_of_injuries &lt;dbl&gt;, weather_condition &lt;chr&gt;, road_description &lt;chr&gt;,\n#   slope_description &lt;chr&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "In-class/In-class_Ex01/data/geospatial/MPSZ-2019.html",
    "href": "In-class/In-class_Ex01/data/geospatial/MPSZ-2019.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on/Hands-On_Ex13/data/geospatial/MPSZ-2019.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/data/geospatial/hexagons.html",
    "href": "Hands-on/Hands-On_Ex12/data/geospatial/hexagons.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 0 0     false"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html",
    "title": "Geographically Weighted Predictive Model",
    "section": "",
    "text": "In this hands-on exercise, we learn about geographically weighted prediction models. In these, occurrences of events are assumed to not be random or uniformly distributed over space.\nThis exercise is based on Chapter 14 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#data-sources",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#data-sources",
    "title": "Geographically Weighted Predictive Model",
    "section": "Data Sources",
    "text": "Data Sources\nTo data for this exercise comes in an rds file and is based on the following sources:\n\nHDB resale data in Singapore from 2017 onwards from data.gov.sg\n2014 Master Plan Planning subzone boundary in shapefile format\nLocational factors with geographic coordinates from data.gov.sg\n\nList and locations of eldercare centres, hawker centres, parks, supermarkets, CHAS clinics, childcare service centres, kindergartens\n\nLocational factors with geographic coordinates from datamall.lto.gov.sg\n\nMRT stations and locations, bus stops and locations\n\nLocational factors without geographic coordinates from data.gov.sg\n\nList of primary schools\n\nLocational factors without geographic coordinates from other sources\n\nCBD coordinates from Google, Shopping malls from Wikipedia, “good or top” primary schools from Local Salary forum"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#installing-and-launching-r-packages",
    "title": "Geographically Weighted Predictive Model",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of eight R packages.\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#reading-from-rds-file",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#reading-from-rds-file",
    "title": "Geographically Weighted Predictive Model",
    "section": "Reading from RDS file",
    "text": "Reading from RDS file\nThe code chunk below uses read_rds() to load the exercise data.\n\nmdata &lt;- read_rds(\"data/rds/mdata.rds\")\n\nWe can use class() to verify the data type, and head() to inspect the first few elements of the object.\n\nclass(mdata)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nhead(mdata)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 28240.06 ymin: 38382.85 xmax: 30637.92 ymax: 39745.94\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 18\n  resale_price floor_area_sqm storey_order remaining_lease_mths PROX_CBD\n         &lt;dbl&gt;          &lt;dbl&gt;        &lt;int&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1       330000             92            1                  684     8.82\n2       360000             91            3                  738     9.84\n3       370000             92            1                  733     9.56\n4       375000             99            2                  700     9.61\n5       380000             92            2                  715     8.35\n6       380000             92            4                  732     9.49\n# ℹ 13 more variables: PROX_ELDERLYCARE &lt;dbl&gt;, PROX_HAWKER &lt;dbl&gt;,\n#   PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;, PROX_GOOD_PRISCH &lt;dbl&gt;, PROX_MALL &lt;dbl&gt;,\n#   PROX_CHAS &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, WITHIN_350M_KINDERGARTEN &lt;int&gt;,\n#   WITHIN_350M_CHILDCARE &lt;int&gt;, WITHIN_350M_BUS &lt;int&gt;,\n#   WITHIN_1KM_PRISCH &lt;int&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#data-sampling",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#data-sampling",
    "title": "Geographically Weighted Predictive Model",
    "section": "Data sampling",
    "text": "Data sampling\nBuilding a predictive model requires splitting at least into training and a test set. We will use a 65:35 ratio. The code chunk below uses initial_split() of the rsample package to perform the split.\n\nset.seed(1234)\nresale_split &lt;- initial_split(mdata, \n                              prop = 6.5/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)\n\nWe can save these into respective rds files to make it easier to reload and replicate the results and model\n\nwrite_rds(train_data, \"data/rds/train_data.rds\")\nwrite_rds(test_data, \"data/rds/test_data.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#converting-the-sf-data-frame-to-spatialpointdataframe",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#converting-the-sf-data-frame-to-spatialpointdataframe",
    "title": "Geographically Weighted Predictive Model",
    "section": "Converting the sf data frame to SpatialPointDataFrame",
    "text": "Converting the sf data frame to SpatialPointDataFrame\nThe first step is to convert the training sf object into a SpatialPointDataFrame format using as_Spatial() in the code chunk below\n\ntrain_data_sp &lt;- as_Spatial(train_data)\ntrain_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 10335 \nextent      : 11597.31, 42623.63, 28217.39, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,          PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       218000,             74,            1,                  555, 0.999393538715878, 1.98943787433087e-08, 0.0333358643817954, 0.0220407324774434, 0.0441643212802781, 0.0652540365486641,                0, 6.20621206270077e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1186888,            133,           17,                 1164,  19.6500691667807,     3.30163731686804,   2.86763031236184,   2.13060636038504,   2.41313695915468,   10.6223726149914, 2.27100643784442,    0.808332738794272,     1.57131703651196,                        7,                    20, ..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-adaptive-bandwidth",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-adaptive-bandwidth",
    "title": "Geographically Weighted Predictive Model",
    "section": "Computing adaptive bandwidth",
    "text": "Computing adaptive bandwidth\nWe then use bw.gwr() of GWmodel package to determine the optimal (adaptive) bandwidth to be used.\n\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\nThe output suggests to use 40 as the adaptive bandwidth. To save on time in the future, (as the code takes some time to run) we can save the results in an RDS file.\n\nwrite_rds(bw_adaptive, \"data/rds/bw_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#constructing-the-adaptive-bandwidth-gwr-model",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#constructing-the-adaptive-bandwidth-gwr-model",
    "title": "Geographically Weighted Predictive Model",
    "section": "Constructing the adaptive bandwidth GWR model",
    "text": "Constructing the adaptive bandwidth GWR model\nWe can then construct the gwr-based hedonic pricing model using adaptive bandwidth and Gaussian kernel with the code chunk below. This uses the gwr.basic() function\n\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\nWe can save the model into an RDS file for future use.\n\nwrite_rds(gwr_adaptive, \"data/rds/gwr_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#retrieve-gwr-output-object",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#retrieve-gwr-output-object",
    "title": "Geographically Weighted Predictive Model",
    "section": "Retrieve GWR output object",
    "text": "Retrieve GWR output object\nThe code chunk below displays the model output by calling the object.\n\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-20 20:39:37.422622 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + storey_order + \n    remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n    PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data_sp, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_order remaining_lease_mths PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_PRISCH\n   Number of data points: 10335\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\n   floor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\n   storey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\n   remaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\n   PROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\n   PROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\n   PROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\n   PROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\n   PROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\n   PROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\n   PROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\n   WITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\n   WITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\n   WITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\n   WITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 61650 on 10320 degrees of freedom\n   Multiple R-squared: 0.7373\n   Adjusted R-squared: 0.737 \n   F-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 3.922202e+13\n   Sigma(hat): 61610.08\n   AIC:  257320.2\n   AICc:  257320.3\n   BIC:  247249\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 40 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.2478e+08 -4.7727e+05 -8.3004e+03  5.5025e+05\n   floor_area_sqm           -2.8714e+04  1.4475e+03  2.3011e+03  3.3900e+03\n   storey_order              3.3186e+03  8.5899e+03  1.0826e+04  1.3397e+04\n   remaining_lease_mths     -1.4431e+03  2.6063e+02  3.9048e+02  5.2865e+02\n   PROX_CBD                 -1.0837e+07 -5.7697e+04 -1.3787e+04  2.6552e+04\n   PROX_ELDERLYCARE         -3.2195e+07 -4.0643e+04  1.0562e+04  6.1054e+04\n   PROX_HAWKER              -2.3985e+08 -5.1365e+04  3.0026e+03  6.4287e+04\n   PROX_MRT                 -1.1632e+07 -1.0488e+05 -4.9373e+04  5.1037e+03\n   PROX_PARK                -6.5961e+06 -4.8671e+04 -8.8128e+02  5.3498e+04\n   PROX_MALL                -1.8112e+07 -7.4238e+04 -1.3982e+04  4.9779e+04\n   PROX_SUPERMARKET         -4.5761e+06 -6.3461e+04 -1.7429e+04  3.5616e+04\n   WITHIN_350M_KINDERGARTEN -4.1823e+05 -6.0040e+03  9.0209e+01  4.7127e+03\n   WITHIN_350M_CHILDCARE    -1.0273e+05 -2.2375e+03  2.6668e+02  2.6388e+03\n   WITHIN_350M_BUS          -1.1757e+05 -1.4719e+03  1.1626e+02  1.7584e+03\n   WITHIN_1KM_PRISCH        -6.6465e+05 -5.5959e+03  2.6916e+02  5.7500e+03\n                                  Max.\n   Intercept                1.6493e+08\n   floor_area_sqm           5.0907e+04\n   storey_order             2.9537e+04\n   remaining_lease_mths     1.8119e+03\n   PROX_CBD                 2.2411e+07\n   PROX_ELDERLYCARE         8.2444e+07\n   PROX_HAWKER              5.9654e+06\n   PROX_MRT                 2.0189e+08\n   PROX_PARK                1.5188e+07\n   PROX_MALL                1.0443e+07\n   PROX_SUPERMARKET         3.8330e+06\n   WITHIN_350M_KINDERGARTEN 6.6799e+05\n   WITHIN_350M_CHILDCARE    1.0802e+05\n   WITHIN_350M_BUS          3.7313e+04\n   WITHIN_1KM_PRISCH        5.0231e+05\n   ************************Diagnostic information*************************\n   Number of data points: 10335 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1730.101 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 8604.899 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 238871.9 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 237036.9 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 238209.1 \n   Residual sum of squares: 4.829191e+12 \n   R-square value:  0.967657 \n   Adjusted R-square value:  0.9611534 \n\n   ***********************************************************************\n   Program stops at: 2024-10-20 20:40:56.413755"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-test-data-predictions-converting-to-spatialpointdataframe",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-test-data-predictions-converting-to-spatialpointdataframe",
    "title": "Geographically Weighted Predictive Model",
    "section": "Computing test data predictions: Converting to SpatialPointDataFrame",
    "text": "Computing test data predictions: Converting to SpatialPointDataFrame\nIn order to compute for the predicted values for the test, we first need to also convert the test data into SpatialPointDataFrame.\n\ntest_data_sp &lt;- test_data %&gt;%\n  as_Spatial()\ntest_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 5566 \nextent      : 11597.31, 42623.63, 28287.8, 48669.59  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,         PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       230888,             74,            1,                  546, 1.00583660772922, 3.34897933104965e-07, 0.0474019664161957, 0.0414043955932523, 0.0502664084494264, 0.0907500295577619,                0, 4.55547870890763e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1050000,            138,           14,                 1151,  19.632402730488,     3.30163731686804,   2.83106651960209,   2.13060636038504,   2.41313695915468,   10.6169590126272, 2.26056404492346,     0.79249074802552,     1.53786629004208,                        7,                    16, ..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-test-data-predictions",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#computing-test-data-predictions",
    "title": "Geographically Weighted Predictive Model",
    "section": "Computing test data predictions",
    "text": "Computing test data predictions\nWe then use gwr.predict() to generate predictions for the test data using a model derived from the training data.\n\ndmat_gwr &lt;- gw.dist(st_coordinates(train_data), st_coordinates(test_data), focus=0, p=2, theta=0, longlat=F)\nwrite_rds(dmat_gwr, \"data/rds/dmat_gwr.rds\")\n\n\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data_sp, \n                        predictdata = test_data_sp, \n                        bw=40, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE,\n                        dMat1 = dmat_gwr)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#extracting-coordinates-data",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#extracting-coordinates-data",
    "title": "Geographically Weighted Predictive Model",
    "section": "Extracting coordinates data",
    "text": "Extracting coordinates data\nWe use the code chunk below to extract the coordinates from the data and the training and test splits. We then write these into RDS files for easy access later.\n\ncoords &lt;- st_coordinates(mdata)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\ncoords &lt;- write_rds(coords, \"data/rds/coords.rds\" )\ncoords_train &lt;- write_rds(coords_train, \"data/rds/coords_train.rds\" )\ncoords_test &lt;- write_rds(coords_test, \"data/rds/coords_test.rds\" )"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#dropping-the-geometry-field",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#dropping-the-geometry-field",
    "title": "Geographically Weighted Predictive Model",
    "section": "Dropping the geometry field",
    "text": "Dropping the geometry field\nNext, we need to drop the geometry field of the training data by using st_drop_geometry() from sf package.\n\ntrain_data_nogeom &lt;- train_data %&gt;% \n  st_drop_geometry()"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#calibrating-using-test-data",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#calibrating-using-test-data",
    "title": "Geographically Weighted Predictive Model",
    "section": "Calibrating using test data",
    "text": "Calibrating using test data\nThe code chunk below calibrates a geographic random forest model using grf() of SpatialML package\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data_nogeom, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\nWe can save the model using the code chunk below.\n\nwrite_rds(gwRF_adaptive, \"data/rds/gwRF_adaptive.rds\")\n\nThen we can reload the object using the following code chunk.\n\ngwRF_adaptive &lt;- read_rds(\"data/rds/gwRF_adaptive.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#predicting-with-test-data",
    "href": "Hands-on/Hands-On_Ex11/Hands-On_Ex11.html#predicting-with-test-data",
    "title": "Geographically Weighted Predictive Model",
    "section": "Predicting with test data",
    "text": "Predicting with test data\n\nPreparing the test data\nIn a similar fashion, we also need to remove the geometry from the test data.\n\ntest_data_nogeom &lt;- cbind(test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\nPredicting with test data\nWe use predict.grf() to predict using the test data in the code chunk below, and then write onto an rds file in the same code block\n\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data_nogeom, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\nwrite_rds(gwRF_pred, \"data/rds/GRF_pred.rds\")\n\n\nhead(gwRF_pred)\n\n[1] 383250.2 352868.8 411256.3 375408.6 419465.5 378760.3\n\n\n\n\nConverting the output into a dataframe\nThe output is in vector form but it is better to convert it into a dataframe so it is easier for analysis and visualizations.\n\nGRF_pred_df &lt;- as.data.frame(gwRF_pred)\n\nWe then add the predicted values into the dataset using the chunk below.\n\ntest_data_p &lt;- cbind(test_data_nogeom, GRF_pred_df)\nwrite_rds(test_data_p, \"data/rds/test_data_p.rds\")\n\n\n\nCalculating Root Mean Square Error (RMSE)\nThe RMSE measures how far predited values are from the actual test values. The code chunk below uses rmse() of Metrics package to compute it for the model against the test data.\n\nrmse(test_data_p$resale_price, \n     test_data_p$gwRF_pred)\n\n[1] 27302.9\n\n\n\n\nVisualizing the predicted values\nWe can also use a scatterplot to visualize the predicted prices against the actual prices\n\nggplot(data = test_data_p,\n       aes(x = gwRF_pred,\n           y = resale_price)) +\n  geom_point()"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "",
    "text": "In this hands-on exercise, we apply hierarchical cluster analysis and spatially constrained cluster analysis to delineate homogeneous regions based on geographically referenced data.\nThis exercise is based on Chapter 12 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#analytical-question",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Analytical Question",
    "text": "Analytical Question\nIn the development of spatial policy and for business, it is often important to segregate homogenous regions using multivariate data. We apply techniques in the study of Shan State in Myanmar by using various indicators."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-sources",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-sources",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on information for Myanmar and for its Shan state:\n\nMyanmar township boundary data in ESRI shapefile format (polygon)\nShan state ICT indicators for 2014 contained in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#installing-and-launching-r-packages",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of thirteen R packages:\n\nsf, rgdal, spdep - for spatial data handling\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\ncoorplot, ggpubr, heatmaply - packages for multivariate data visualization and analysis\ncluster, ClustGeo - packages for performing cluster analysis\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\nWe also define a random seed value for repeatability where of any randmoized results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-boundary",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-boundary",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Data Loading - Shan state boundary",
    "text": "Data Loading - Shan state boundary\nThe code chunk below uses st_read() of the sf package to load the Myanmar township boundary shapefile into an R object. The code chunk includes a pipeline to already filter to the Shan state and include only the relevant columns.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex09\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nWe can inspect the contents of shan_sf using the code chunk below\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nThe sf dataframe conforms to the tidy framework. Given this, we can also use glimpse() to reveal the fields’ data types.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-2014-indicators-aspatial",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-2014-indicators-aspatial",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Data Loading - Shan state 2014 indicators (aspatial)",
    "text": "Data Loading - Shan state 2014 indicators (aspatial)\nThe code chunk below uses read_csv() to load the contents of the csv file into an object ict\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\nRows: 55 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): District Pcode, District Name, Township Pcode, Township Name\ndbl (7): Total households, Radio, Television, Land line phone, Mobile phone,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can use head() to check the first 6 elements of the object,\n\nhead(ict)\n\n# A tibble: 6 × 11\n  `District Pcode` `District Name` `Township Pcode` `Township Name`\n  &lt;chr&gt;            &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;          \n1 MMR014D001       Taunggyi        MMR014001        Taunggyi       \n2 MMR014D001       Taunggyi        MMR014002        Nyaungshwe     \n3 MMR014D001       Taunggyi        MMR014003        Hopong         \n4 MMR014D001       Taunggyi        MMR014004        Hsihseng       \n5 MMR014D001       Taunggyi        MMR014005        Kalaw          \n6 MMR014D001       Taunggyi        MMR014006        Pindaya        \n# ℹ 7 more variables: `Total households` &lt;dbl&gt;, Radio &lt;dbl&gt;, Television &lt;dbl&gt;,\n#   `Land line phone` &lt;dbl&gt;, `Mobile phone` &lt;dbl&gt;, Computer &lt;dbl&gt;,\n#   `Internet at home` &lt;dbl&gt;\n\n\nand summary() to display summary statistics of the numeric columns.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThe dataset contains 11 fields with 55 observations. The numeric fields give the total number of households in each township, and the number of households with the corresponding technology or appliance. (e.g., television, internet connection, etc)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#deriving-new-indicator-variables",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#deriving-new-indicator-variables",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Deriving new indicator variables",
    "text": "Deriving new indicator variables\nUsing the numeric fields directly will be highly biased as it depends on the number of households in the township. (i.e., townships with higher total households are likely to have higher values for all other columns) To overcome this problem, we can derive the penetration rates (PR) of each of the items by computing the number of households with that item per 1000 households. We accomplish this using mutate() from dplyr package in the code below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nWe can use summary() again to display summary statistics on the 6 new columns.\n\nsummary(ict_derived[c(12:17)])\n\n    RADIO_PR          TV_PR         LLPHONE_PR       MPHONE_PR     \n Min.   : 21.05   Min.   :116.0   Min.   :  2.78   Min.   : 36.42  \n 1st Qu.:138.95   1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14  \n Median :210.95   Median :517.2   Median : 37.59   Median :305.27  \n Mean   :215.68   Mean   :509.5   Mean   : 51.09   Mean   :314.05  \n 3rd Qu.:268.07   3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43  \n Max.   :484.52   Max.   :842.5   Max.   :181.49   Max.   :735.43  \n  COMPUTER_PR      INTERNET_PR     \n Min.   : 3.278   Min.   :  1.041  \n 1st Qu.:11.832   1st Qu.:  8.617  \n Median :18.970   Median : 22.829  \n Mean   :24.393   Mean   : 30.644  \n 3rd Qu.:29.897   3rd Qu.: 41.281  \n Max.   :92.402   Max.   :117.985"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#joining-spatial-and-aspatial-data",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#joining-spatial-and-aspatial-data",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Joining spatial and aspatial data",
    "text": "Joining spatial and aspatial data\nFor later map preparations, we need to combine the two datasets (geospatial shan_sf, aspatial ict_derived) into a single object. We do this using the left_join() function of the dplyr package. Both datasets have a common field TS_PCODE which will be treated as the unique identifier or joining key.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")\n\nThe code includes creation of a new rds file so we can use the following code in the future to read this joined dataset without performing all the steps above.\n\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-statistical-graphics",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-statistical-graphics",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "EDA using statistical graphics",
    "text": "EDA using statistical graphics\nWe can use histograms to visualize the overall distribution of data values– e.g., the shape or skewness. The code chunk below produces on for the field RADIO_PR.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20,  color=\"black\", fill=\"light blue\") +\n  xlab(\"Radio Penetration Rate, per K-HH\") +\n  ylab(\"No. of Townships\")\n\n\n\n\n\n\n\n\nWe can also use boxplots for identifying the median, quartiles, and outliers in the data.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")+\n  xlab(\"Radio Penetration Rate, per K-HH\")\n\n\n\n\n\n\n\n\nWe can create multiple histograms side by side by creating objects for each variable’s histogram, and then laying them out in a grid with ggarange() of the ggpubr package.\n\nCreation of Histogram objectsGrid display of multiple histograms\n\n\n\nradio &lt;- ggplot(data=ict_derived, aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20,color=\"black\", fill=\"light blue\") +\n  xlab(\"Radio PR\") +\n  ylab(\"No. of Townships\")\n\ntv &lt;- ggplot(data=ict_derived, aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"TV PR\") +\n  ylab(\"No. of Townships\")\n\nllphone &lt;- ggplot(data=ict_derived, aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Landline Phone PR\") +\n  ylab(\"No. of Townships\")\n\nmphone &lt;- ggplot(data=ict_derived, aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Mobile Phone PR\") +\n  ylab(\"No. of Townships\")\n\ncomputer &lt;- ggplot(data=ict_derived, aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Computer PR\") +\n  ylab(\"No. of Townships\")\n\ninternet &lt;- ggplot(data=ict_derived, aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Internet PR\") +\n  ylab(\"No. of Townships\")\n\n\n\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, nrow = 2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-choropleth-map",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-choropleth-map",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "EDA using choropleth map",
    "text": "EDA using choropleth map\nThe code chunk below prepares a choropleth map of the Shan state and the Radio penetration rate using qtm()\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\nThe above map is based on the derived penetration rate. We can use choropleth maps to go back to the earliest statement that using the raw variables are likely to be biased on the number of households. We can use the code chunk below to look at them side by side. We use the approach of passing multiple arguments instead of using tmap_arrange()\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"TT_HOUSEHOLDS\", \"RADIO\"),\n          n = 5,style = \"jenks\", \n          title = c(\"Total households\",\"Number Radio\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe above map shows that townships with high number of households with radios, also are towns with the high number of households. We can produce a second map to see if the penetration rate and the total number of households are correlated.\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n          n = 5,style = \"jenks\", \n          title = c(\"Total households\",\"Radio Penetration\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe second pair of maps shows no strong correlation between townships having high number of households and having high radio penetration rates.\nFinally, we can show the six derived variables visually using a similar approach in the code chunk below. The viewer needs to be mindful of the data classes. While the darker the shading means a higher value for that derived variable, the range of values are different between pairs of variables.\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\",\n                  \"MPHONE_PR\", \"COMPUTER_PR\", \"INTERNET_PR\"),\n          n = 5,style = \"jenks\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-and-extracting-cluster-variables",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-and-extracting-cluster-variables",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Selecting and extracting cluster variables",
    "text": "Selecting and extracting cluster variables\nThe code chunk below will be used to extract the clustering variables from the shan_sf dataframe. We have chosen to include COMPUTER_PR rather than INTERNET_PR for the cluster analysis\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nThe next step is to change the row names or indices to the township names rather than the row numbers\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nWe see that the row numbers have been replaced with the township names, however, the township names are now duplicated. We solve this by using the code chunk below\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-standardisation",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-standardisation",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Data standardisation",
    "text": "Data standardisation\nMultiple variables will usually have different range of values. If we use them as is for cluster analysis, then the clustering will be biased towards variables with larger values. It is useful to standardise the clustering variables to reduce the risk of this occuring.\n\nMin-max standardisation\nThe code chunk below uses normalize() of heatmaply package to standardise the clustering variables using min-max method. We then use summary() to show that the ranges of each variable have transformed to [0,1]\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\n\n\nZ-score standardisation\nWe can perform z-score standardisation by using scale() of Base R. We use describe() of psych package to display some statistics of the standardised columns. These show that each of the variables have been transformed to have a mean of 1 and a standard deviation of 1\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\n\n\nVisualising the standardised clustering variables\nAside from viewing the statistics of the standardised variables, it is also good practice to visualise their distribution graphically.\nThe code chunk below produces histograms to show the RADIO_PR field without and with standardisation\n\nr &lt;- ggplot(data=ict_derived, aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\nAlternatively, we can view these as density plots using the code below.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-the-proximity-matrix",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-the-proximity-matrix",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing the proximity matrix",
    "text": "Computing the proximity matrix\nThere are many packages in R that provide functions to compute for the distance matrix. We will use dist() for our case.\nThis function supports six distance calculations: euclidean (default), maximum, manhattan, canberra, binary and minkowski. The code chunk below is used to compute the proximity matrix using the euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below displays the content of proxmat for inspection\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-hierarchical-clustering",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-hierarchical-clustering",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing hierarchical clustering",
    "text": "Computing hierarchical clustering\nThere are several packages in R that can perform hierarchical clustering. In this exercise, we use hclust() of R stats.\nhlcust() employs agglomeration method to compute clusters. Eight clustering algorithms are supported: (1) ward.D, (2) ward.D2, (3) single, (4) complete, (5) average(UPGMA), (6) mcquitty(WPGMA), (7) median(WPGMC), and (8) centroid (UPGMC)\nThe code chunk below performs hierarchical clustering using ward.D method. The output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nOnce ran, we can plot the resulting object as tree by using plot()\n\nplot(hclust_ward, cex = 0.6)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-the-optimal-clustering-algorithm",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-the-optimal-clustering-algorithm",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Selecting the optimal clustering algorithm",
    "text": "Selecting the optimal clustering algorithm\nA challenge in performing hierarchical clustering is identifying strong clustering structures. This can be solved by using agnes() of the cluster package. The function acts like hclust(), but can also get the agglomerative coefficients– or the measure of the strength of the clustering structure. (with a value of 1 indicating a strong structure)\nThe code chunk below computes the agglomerative coefficient of all algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nThe output above shows that Ward’s method provides the best coefficient, and therefore the strongest cluster, among the four methods assessed. We will then focus on this method in succeeding analyses."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#determining-optimal-clusters",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#determining-optimal-clusters",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Determining optimal clusters",
    "text": "Determining optimal clusters\nAnother challenge in cluster analysis is determining the number of clusters to retain. For this, there are three commonly used methods to determine the number of clusters:\n\nElbow method\nAverage Silhouette method\nGap Statistic Method\n\n\nGap statistic method\nThe gap statistic compares intra-cluster variation for different values of k with their expected values under null reference data distribution. The optimal cluster will be the one that maximizes the gap statistic– meaning that the optimal cluster is the farthest from representing a random distribution of points.\nWe use the code chunk below to compute the gap statistic using clusGap() of cluster package. One of the arguments, FUN, use the hcut function which comes from factoextra package indicating that hierarchical clustering is used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nWe can then visualize the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\nWhile the chart above shows that k=1 cluster(s) gives the highest gap statistic, it is not logical. Aside from k=1, we see that k=6 clusters gives the largest statistic and would be the best number of clusters to pick.\nIn addition to the above, the NbClust package provides 30 indices for determining the optimal number of clusters."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#interpreting-the-dendograms",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#interpreting-the-dendograms",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Interpreting the dendograms",
    "text": "Interpreting the dendograms\nEach leaf in the dendogram represents one observation. (townships in our example) Moving up the dendogram, leaves are combined into similar ones using branches. The heights of the fusion indicates the dissimilarity between the two observations, i.e., the higher the height the larger the difference between the two observations. The horizontal axis does not provide any information on the similarity or dissimilarity of pairs of observations.\nThe dendogram can be redrawn with a border around selected (number of) clusters by using rect.hclust() of R stat. The border argument specifies the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visually-driven-hierarchical-clustering-analysis",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visually-driven-hierarchical-clustering-analysis",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Visually-driven hierarchical clustering analysis",
    "text": "Visually-driven hierarchical clustering analysis\nIn this section, we use heatmaply package to perform visually driven hierarchical clustering analysis. With this package, we can build interactive or static cluster heatmaps.\n\nTransforming dataframe into a matrix\nTo create a heatmap, the data needs to be in a matrix. We convert the data frame to this format using the code below\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\nPlotting interactive cluster heatmap using heatmaply()\nThe code chunk below uses heatmaply() of heatmap package to produce an interactive cluster heatmap\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#mapping-the-clusters-formed",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#mapping-the-clusters-formed",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Mapping the clusters formed",
    "text": "Mapping the clusters formed\nWe can use cutree() for R base to derive a 6-cluster model. The code below outputs a list object.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nIn order to visualize the clusters, the list first needs to be appended to the shan_sf simple feature object.\nThis is accomplished in the following code chunk in three steps:\n\nThe object is converted to a matrix\ncbind() is used to append the matrix object onto shan_sf as a new sf object\nrename() is then used to rename the appended field as.matrix.groups into CLUSTER\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nWe then use qtm() of tmap package to produce a quick map of Shan state with the clusters\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\nThe plot shows that the resulting clusters are quite fragmented. This is a limitation of performing non-spatial clustering algorithm."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#converting-into-spatialpolygons-dataframe",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#converting-into-spatialpolygons-dataframe",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Converting into SpatialPolygons DataFrame",
    "text": "Converting into SpatialPolygons DataFrame\nThe skater() function can only support sp objects, so conversion into the appropriate type is required\nThe code chunk performs the conversion using as_Spatial() of sf package.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\nclass(shan_sp)\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\""
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-neighbor-list",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-neighbor-list",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing Neighbor list",
    "text": "Computing Neighbor list\nWe then use poly2nd() of spdep package to generate the neighbor list from the polygon dataframe.\n\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can visualize the neighbor lists using the chunks below. We first plot the boundaries based on shan_sf. We follow this with the neighbor list object shan.nd and use the shape centroids to represent nodes for the graph representation. The add=TRUE argument specifies plotting the network on top of the plot of the boundaries.\n\ncoords &lt;- st_coordinates(\n  st_centroid(st_geometry(shan_sf)))\n\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\nplot(shan.nb,\n     coords, \n     col=\"blue\", \n     add=TRUE)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-minimum-spanning-tree",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-minimum-spanning-tree",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing minimum spanning tree",
    "text": "Computing minimum spanning tree\n\nCalculating edge costs\nWe then use nbcosts() of spdep package to compute the cost of each edge. The cost will be the “distance” between nodes.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nThe function computes the dissimilarity between pairs of neighbors across the value of the five variables.\nWe then incorporate these costs into a weights object which is equivalent to converting the neighbor list into a list weights object by specifying lcosts as weights. We achieve this by using nb2listw() of spdep package in the code chunk below. We specify style=\"B\" to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\nComputing the minimum spanning tree\nWe use mstree() of spdep package to compute the minimal spanning tree.\n\nshan.mst &lt;- mstree(shan.w)\n\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the resulting minimum spanning tree has a dimension n-1 (55-1=54) as it consists of n-1 links to connect the nodes that allow traversal from any any pair of nodes.\nWe can display the content of the spanning tree using head() where we see the first six links and their respective weight\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\nThe plot method for MST includes a way to show the observation (numbers/index) in addition to the connecting edge. The resulting plot will be similar to the neighbor list’s except there will only be at most two edges connecting to any node\n\nplot(st_geometry(shan_sf), \n                 border=gray(.5))\nplot.mst(shan.mst, \n         coords, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-spatially-constrained-clusters-using-skater-method",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-spatially-constrained-clusters-using-skater-method",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Computing spatially constrained clusters using SKATER method",
    "text": "Computing spatially constrained clusters using SKATER method\nWe can use skater() of spdep package to compute spatially constrained clusters as in the code chink below\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe function requires three mandatory arguments:\n\nThe first two columes of the MST (the edge, or pair of nodes)\nthe data matrix\nthe number of cuts which is equal to the number of clusters minus 1\n\nThe resulting object is of class skater. We can examine the contents using the code chunk below\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe first field groups contains the label of the cluster membership for that observation. The next object contains details of the different clusters which include the nodes and edges in that cluster.\nWe can check the cluster assignments using the following code chunk\n\nccs6 &lt;- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can check how many observations are in each cluster using the table() function.\n\ntable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nWe can also plot the pruned tree showing the six clusters. Note that two of the clusters have only one township each so they will not produce a colored edge in the plot (Group 5 with node 23, group 6 with node 3)\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(clust6, \n     coords, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\", \"black\"),\n     cex.circles=0.005, \n     add=TRUE)\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visualizing-the-clusters-in-a-choropleth-map",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visualizing-the-clusters-in-a-choropleth-map",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Visualizing the clusters in a choropleth map",
    "text": "Visualizing the clusters in a choropleth map\nThe code chunk below can be used to plot the clusters derived from SKATER method using tmap package\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\nFor easier comparison, we can put the clusters generated using hierarchical clustering and spatially constrained hierarchical clustering side by side.\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#ward-like-hierarchical-clustering-clustgeo",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#ward-like-hierarchical-clustering-clustgeo",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Ward-like hierarchical clustering: ClustGeo",
    "text": "Ward-like hierarchical clustering: ClustGeo\nThe package contains a function hclustgeo() to perform Ward-like hierarchical clustering similar to hclust()\nThe function only requires the dissimilarity matrix to perform non-spatially constrained clustering as in the code chunk below. Note that the dissimilarity matrix needs to be of class dist, an object generated by dist()\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#mapping-the-clusters-formed-1",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#mapping-the-clusters-formed-1",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Mapping the clusters formed",
    "text": "Mapping the clusters formed\nWe can plot the generated clusters in a shaded map using the code chunks below\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\nAs it is not spatially-constrained, the resulting clusters are quite fragmented"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#spatially-constrained-hierarchical-clustering",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#spatially-constrained-hierarchical-clustering",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Spatially-constrained hierarchical clustering",
    "text": "Spatially-constrained hierarchical clustering\nBefore performing spatially-constrained hierarchical clustering, we use st_distance() of sf package to generate a spatial distance matrix. We use the code chunk below which also include as.dist() to convert the dataframe into a matrix.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNext, we determine a suitable value for the mixing parameter alpha using choicealpha()\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the charts above, we select alpha = 0.3 as the input to hclustgeo()\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nWe then use cutree() to derive the cluster object\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe then join the group list back into the shan_sf polygon feature dataframe using the code chunk below\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can then use qtm() to map the spatially constrained clusters.\n\nqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visualising-individual-clustering-variable",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#visualising-individual-clustering-variable",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Visualising individual clustering variable",
    "text": "Visualising individual clustering variable\nThe code chunk below reveals the distribution of a clustering variable (RADIO_PR) based on one of the clustering approaches (non-spatially constrained ClustGeo)\n\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplot reveals that cluster 3 (in this method) has the highest mean radio penetration rate while clusters 4,5, and 6 have the lowest"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#multi-variate-visualisation",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#multi-variate-visualisation",
    "title": "Geographic Segmentation with Spatially Constrained Clustering Techniques",
    "section": "Multi-variate visualisation",
    "text": "Multi-variate visualisation\nParallel coordinate plots can be used to reveal insights on clustering variables by clusters effectively, We can perform this using ggparcoord() of GGally package.\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThe scale argument of ggparcorr() provides several options to scale the variables:\n\nstd - univariately, scaled as Z value\nrobust - univariately, subtract median and divide by mean absolute deviation\nuniminmax - univariately, scale to [0-1]\nglobalminmax - no scaling is done, range is defined by global min and max values\ncenter - use uniminmax to standardize vertical height, then center at a specified value (scaleSummary parameter)\ncenterObs - use uniminmax to standardize vertical height, then center at a specified observation’s value (centerObsIDparameter)\n\nWe can also compare the clusters by computing for statistics that will complement the visual interpretation. The code chunk below uses group_by() and summarise() of dplyr to derive the mean values for the clustering variables by cluster\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we learn how to compute and interpret global measures of spatial autocorrelation (GMSA) using the spdep package.\nThis exercise is based on Chapter 9 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#analytical-question",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Analytical Question",
    "text": "Analytical Question\nOne of the main development objective in spatial policy is for local governments and planners to ensure that there is equal distribution of development in the province. We then need to apply the appropriate spatial methods to verify if there is indeed even distribution of wealth geographically. If there is uneven distribution, then the next step is to identify if and where clusters are happening.\nWe continue studying the Hunan Province in China and focus on GDP per capita as the key indicator of development."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-sources",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-sources",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on the Hunan county coming from two files:\n\nHunan county boundary layer in ESRI shapefile format\nHunan local development indicators for 2012 stored in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#installing-and-launching-r-packages",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, tmap, and spdep.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspdep - functions to create spatial weights, autocorrelation statistics\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\nWe also define a random seed value for repeatability of any simulation results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-loading",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-loading",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Hunan shapefile into an R object.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe then use the code chunk below to load the csv file with the indicators into R using read_csv()\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-preparation",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-preparation",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-the-development-indicator",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-the-development-indicator",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package. We present these uas two maps using classes of equal intervals and equal quantiles.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#computing-contiguity-spatial-weights",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#computing-contiguity-spatial-weights",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nPrior to computing GMSA’s, we need t construct spatial weights of the study area. Spatial weights are used to define the neighborhood relationship between units. (i.e., neighbors or adjacent units)\nThe code chunk below uses poly2nb() of the spdep package to compute contiguity weight matrices for the study area. The function builds a neighbor list based on regions with shared boundaries. The queen argument takes TRUE (default) or FALSE as options. This instructs the function if Queen criteria should be used in defining neighbors. For the code below, we use the Queen criteria to build the contiguity matrix\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output shows that there are 88 units in the hunan dataset, The most connected unit has 11 neighbors and two units have only one neighbor."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#row-standardised-weights-matrix",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#row-standardised-weights-matrix",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Row-standardised weights matrix",
    "text": "Row-standardised weights matrix\nThe next step is assigning weights to each neighbor. For our case, we assign equal weight (using style=\"W\") to each neighboring polygon. This assigns the fraction 1/n, where n is the number of neighbors a unit has, as the weight of each unit’s neighbor. The drawback of this approach is that polygons in the edge of the study area will base their value on a smaller number of neighbors. This means that we may be potentially over- or under-estimating the true nature of spatial autocorrelation. The alternative more robust style=\"B\" can address this.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-test",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-test",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Moran’s I test",
    "text": "Moran’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of the spdep package\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nThe p-value does not support CSR for the GDPPC, while a positive statistic indicates signs of clustering. If the statistic value were below 0, or negative, then it would indicate signs of dispersion."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-morans-i",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-morans-i",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Monte Carlo Simulation for Moran’s I",
    "text": "Monte Carlo Simulation for Moran’s I\nWe use the code chunk below to perform permutation test for the statistic by using moran.mc() of spdep. The nsim argument is set so that 1000 simulations will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-morans-i",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-morans-i",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of Monte Carlo Simulation Results (Moran’s I)",
    "text": "Visualization of Monte Carlo Simulation Results (Moran’s I)\nIt is good practice to analyse and visualize the simulation results in more detail. We can do this by checking the values and distribution of the statistic numerically and graphically.\nWe can use the code chunk below to show individual statistics of the simulated value.\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\npaste(\"Standard Dev:\", var(bperm$res[1:999]))\n\n[1] \"Standard Dev: 0.00437157393477615\"\n\n\nWe can visualize graphically using hist() and abline() from R Graphics.\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-test",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-test",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Geary’s C test",
    "text": "Geary’s C test\nThe code chunk below uses geary.test() to perform Geary’s C test for spatial autocorrelation.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nGeary’s C test uses a different interpretation compared to Moran’s I. A statistic value less than one, as in the case above, indicates signs of clustering, while a value of greater than one indicates dispersion. The very low p-value means that any hypothesis of compete spatial randomness (with α &gt; 0.015%) is not supported by the observed data."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-gearys-c",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-gearys-c",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Monte Carlo Simulation for Geary’s C",
    "text": "Monte Carlo Simulation for Geary’s C\nWe use the code chunk below to perform permutation test for the statistic by using geary.mc() of spdep. The nsim argument is set so that 1000 simulations will be performed.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-gearys-c",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-gearys-c",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of Monte Carlo Simulation Results (Geary’s C)",
    "text": "Visualization of Monte Carlo Simulation Results (Geary’s C)\nIt is good practice to analyse and visualize the simulation results in more detail. We can do this by checking the values and distribution of the statistic numerically and graphically.\nWe can use the code chunk below to show individual statistics of the simulated value.\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\npaste(\"Standard Dev:\", var(bperm$res[1:999]))\n\n[1] \"Standard Dev: 0.00743649278244122\"\n\n\nWe can visualize graphically using hist() and abline() from R Graphics.\n\nhist(bperm$res,\n     freq=TRUE,\n     breaks=20,\n     xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-correlogram-and-plot",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-correlogram-and-plot",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Moran’s I Correlogram and Plot",
    "text": "Moran’s I Correlogram and Plot\nThe code chunk below uses sp.correlogram() of spdep package to compute a 6-lag (order=6) spatial correlogram of GDPPC using Moran’s I. (method=\"I\") We then plot() to produce the visualization.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\nAside from the output, we can also display the full content of the analysis using the code below. This lets us see the result for each lag in more detail.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-correlogram-and-plot",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-correlogram-and-plot",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Geary’s C Correlogram and Plot",
    "text": "Geary’s C Correlogram and Plot\nThe code chunk below uses sp.correlogram() of spdep package to compute a 6-lag (order=6) spatial correlogram of GDPPC using Geary’s C. (method=\"C\") We then plot() to produce the visualization.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\nWe can also examine the results in more detail using the code chunk below\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html",
    "href": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "In this hands-on exercise, we learn some more SPPA with Network Constrained Spatial Point Patterns Analysis or NetSPPA. This is a collection of SPPA methods that is used to analyse spatial point events occuring on or alongside a network– which can be a road network, river network, etc.\nWe will be using the spNetwork package to derive the network kernel density estimation (NKDE) and then we perform analysis on the G-function and K-Function.\nThis exercise is based on Chapter 7 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#data-sources",
    "href": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#data-sources",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are from public sources and will be used to analyse the distribution of childcare centers in the Punggol planning area. Two datasets in ESRI shapefile format will be used:\n\nA line feature geospatial dataset which includes the road network of Punggol planning area\nA point feature geospatial dataset which includes the location of childcare centers in the Punggol planning area"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#installing-and-launching-r-packages",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, spNetwork, tidyverse, and tmap.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nsPNetwork - provides functions for performing SPPA methods like KDE and K-function on a network. The package can also be used to build spatial matrices to conduct traditional spatial analyses with spatial weights based on reticular distances\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spNetwork, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#preparing-the-lixels-objects",
    "href": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#preparing-the-lixels-objects",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Preparing the lixels objects",
    "text": "Preparing the lixels objects\nBefore computing the NKDE, SpatialLines objects need to be cut into lixels with a specified minimal distance. We do this using lixelize_lines() of spNetwork as shown in the code chunk below.\n\nlixels &lt;- lixelize_lines(network, \n                         700, \n                         mindist = 375)\n\nThe second argument of the function is lx_length which stands for the lixel length and was set to 700. The minimum lixel lngth is set to 375 by the mindist argument. If the length of the final lixel is shorter than mindist then it is added to the previous lixel. If mindist is NULL, then it is set to maxdist / 10."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#generating-line-centre-points",
    "href": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#generating-line-centre-points",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Generating line centre points",
    "text": "Generating line centre points\nNext, we use line_center() of spNetwork to generate a SpatialPointsDataFrame with line centre points as in the code chunk below. The centres will be located in the middle of the line based on the length.\n\nsamples &lt;- lines_center(lixels)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#performing-nkde",
    "href": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#performing-nkde",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Performing NKDE",
    "text": "Performing NKDE\nThe code chunk below computes the NKDE\n\ndensities &lt;- nkde(network, \n                  events = st_zm(childcare),\n                  w = rep(1, nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nA few points on the arguments in the code chunk above:\n\nThe st_zm() function drops the z coordinate of childcare since the function requires two-dimensional inputs, while childcare has 3\nkernel_name is set to quartic but can be also set to other kernel methods like: triangle, gaussian, scaled gaussian, tricube, cosine, triweight, epanechnikov or uniform\nmethod is currently set to use simple method to be used to calculate the NKDE. There are three popular methods that are used\n\nsimple - distances between events and sampling points are replaced by network distances. The formula of the kernel is adapted to calculate the density over a linear instead of an areal unit\ndiscontinuous - equally divides the mass density of an event at the intersection of the lixels.\ncontinuous - divides the mass of the density at the intersection but adjusts the density before the intersection to make the function continuous"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#visualizing-nkde",
    "href": "Hands-on/Hands-On_Ex05/Hands-On_Ex05.html#visualizing-nkde",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Visualizing NKDE",
    "text": "Visualizing NKDE\nWe first insert the computed density values into the samples and lixels objects as a field density\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\nBefore plotting, we rescale the distances of the objects from the current SVY21 default of meter to kilometer, by using a multiple of 1000\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\nThe code chunk below produces an interactive map using tmap. The resulting map reveals road segments with high density of childcare centres based on the intensity of the color (i.e., darker shading is more dense)\n\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(childcare)+\n  tm_dots()\n\n\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "",
    "text": "For this hands-on exercise, we start learning about Spatial Point pattern analysis, starting with First Order effects. (based on an underlying property or location)\nWe will be using the functions of the spatstat package, and applying it to an analysis on the location of childcare centres in Singapore.\nThis exercise is based on Chapter 4 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#data-sources",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#data-sources",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are from public sources and include:\n\nLocation and attribute information of childcare centres in Singapore from data.gov.sg\nMaster Plan 2014 Subzone Boundary from data.gov.sg\nNational boundary of Singapore provided in SLA and ESRI shapefile format"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#installing-and-launching-r-packages",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, spatstat, raster, maptools and tmap. Among these, the new ones we are using are:\n\nspatstat - offers a wide range of functions for point pattern analysis (PPA)\nraster - used to read, write, manipulate and analyse models of gridded spatial data\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, tidyverse, tmap, spatstat, raster)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#importing-the-geospatial-data",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#importing-the-geospatial-data",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Importing the Geospatial Data",
    "text": "Importing the Geospatial Data\nWe use the st_read() function of the sf package to load the different geospatial datasets into R.\nThe code below loads the preschool location geoJSON file into the dataframe childcare_sf and projects it into SVY21.\n\nchildcare_sf &lt;- st_read(\"data/geospatial/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex03\\data\\geospatial\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe code chunk below loads the Singapore National boundary shapefile into the dataframe sg_sf.\n\nsg_sf &lt;- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\nThe code chunk below loads the Master Plan 2014 subzone boundary shapefile into another dataframe called mpsz_sf\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWe can use st_crs() to check what coordinate systems are used in each of the three sf dataframes.\nchildcare_sf is in SVY21 using EPSG code 3414 after our load and transform operation above.\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nWhile sg_sf and mpsz_sf appeared to be in SVY21, running st_crs() reveals that they are not using the correct EPSG code.\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nUsing st_set_crs() we can convert these and correct the crs information.\n\nsg_sf = st_set_crs(sg_sf, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\nmpsz_sf = st_set_crs(mpsz_sf, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nRerunning st_crs() shows that the correct EPSG code is now reflected.\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#mapping-the-geospatial-data-sets",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#mapping-the-geospatial-data-sets",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Mapping the Geospatial Data Sets",
    "text": "Mapping the Geospatial Data Sets\nWith all three datasets in the same coordinate system, we can draw them in a single map as different layers using tm_shape() as in the following code chunk. See how the planning subzones extend beyond the defined coastal borders.\n\ntm_shape(sg_sf)+\n    tm_fill(\"lightblue\") +\n    tm_borders(lwd = 0.1,  alpha = 1)+\n    tm_shape(mpsz_sf) +\n    tm_fill(\"grey\", alpha = 0.5) +\n    tm_borders(lwd = 0.1,  alpha = 1) +\n    tm_shape(childcare_sf) +\n    tm_dots(col = \"darkgreen\")\n\n\n\n\n\n\n\n\nThe previous map shows the importance of using the same reference system across three different data sets for mapping and for analysis.\nAlternatively, we can prepare a pin map using the code below which switches to interactive mode using tmap_mode()\n\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\nInteractive maps allow the user to navigate and zoom in and out of the map freely. Features can also be queried by clicking on them. The background map layer is defaulted to ESRI.WorldGrayCanvas. There are two other available background map layers (ESRI.WorldToolMap and OpenStreetMap)\nIt is important to switch back to static mode (using the code below) when interactive maps are not required. This is as each interactive map uses a connection. The use of of interactive maps should be limited when publishing.\n\ntmap_mode('plot')\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#converting-sf-dataframe-sp-spatial-class",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#converting-sf-dataframe-sp-spatial-class",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Converting sf dataframe sp spatial class",
    "text": "Converting sf dataframe sp spatial class\nThe code chunk below converts the three dataframes using as_Spatial() from the sf package.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\nWe can check the contents of the new dataframes by calling them. This confirms that they are in the spatial* class format.\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#converting-the-spatial-class-to-generic-sp-format",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#converting-the-spatial-class-to-generic-sp-format",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Converting the spatial class to generic sp format",
    "text": "Converting the spatial class to generic sp format\nspatstat requires that the data is in ppp object form. There is no direct way to do this from spatial* class. We need to convert spatial* class to a spatial object first.\nThe code chunk below transform two of the spatial* objects into generic sp objects.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nCalling childcare_sp and sg_sp lets us check their properties.\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#converting-generic-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#converting-generic-sp-format-into-spatstats-ppp-format",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Converting generic sp format into spatstat’s ppp format",
    "text": "Converting generic sp format into spatstat’s ppp format\nWe will then use as.ppp() from spatstat package to convert the (spatial) data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\nWarning in as.ppp.sf(childcare_sf): only first attribute column is used for\nmarks\n\nchildcare_ppp\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nWe see the difference of this format when we use plot() (from R Graphics) to produce a quick map of the data.\n\nplot(childcare_ppp)\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1545 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\nWe can see summary information on the new ppp object using the code chunk below.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#handling-duplicated-points",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#handling-duplicated-points",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Handling duplicated points",
    "text": "Handling duplicated points\nWe can check duplication in a ppp object using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] FALSE\n\n\nTo count the number of coincident points, we can use the multiplicity() function.\n\nmultiplicity(childcare_ppp)\n\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWe can wrap this in sum() to count the number of locations with more than one event.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 0\n\n\nThe outputs show that there are no duplication in childcare_ppp\nIf there are any duplicates, there are three possible approaches to handle them:\n\nDelete the duplicates. The downside of this is that some (useful) information will be lost\nJittering. Adding a small perturbation to the duplicate points so they do not occupy the exact same space\nMake each point unique, then attach the duplicates of the points as marks or attributes to the point\n\nThe code chunk below shows how jittering can be applied using rjitter()\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#creating-owin-object",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#creating-owin-object",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Creating owin object",
    "text": "Creating owin object\nWhen analysing spatial point patterns, it is best to confine the analysis within a geographical area. In spatstat, the object that represents the bounded region is called an owin.\nThe code chunk below creates an owin based on the sg SpatialPolygon object.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe owin object can be displayed graphically using plot() and summarized using summary()\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#combining-point-events-object-and-owin-object",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Combining point events object and owin object",
    "text": "Combining point events object and owin object\nIn this last step, we extract childcare events (locations) that are within Singapore, as depicted by the owin, using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output is a combination of the point feature and the polygon feature into a single ppp object:\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nRunning plot() on this shows both the owin and the preschool locations in a single map.\n\nplot(childcareSG_ppp)\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1545 symbols are shown in the symbol map"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#kernel-density-estimation-kde",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#kernel-density-estimation-kde",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Kernel Density Estimation (KDE)",
    "text": "Kernel Density Estimation (KDE)\n\nComputing KDE using automatic bandwidth selection method\nThe following code chunk computes the KDE for childcare services using density() from spatstat with the following parameters:\n\nbw.diggle() for automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl()\ngaussian as the smoothing kernel. This is already the default. Other smoothing methods that can be used are epanechnikov, quartic and disc\nThe intensity estimate is corrected for edge effect bias by using the method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nRunning this new object into plot() will display the derived KDE for the childcare events/locations\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\nThe values stand for the density and currently range from 0 to 0.000035. These numbers are small and hard to visualize, but is a result of the unit of measurements used in the reference system, which is svy21. As svy21 uses meter as the unit of measure, the figures stand for number of points per square meter.\nWe can also retrieve the bandwidth used to compute the kde layer using the code chunk below\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n298.4095 \n\n\n\n\nRescaling KDE values\nThe code chunk below uses rescale.ppp() to convert the unit of measure from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\nWe can then recompute the KDE and plot the new map with the rescaled data. The map will be the same but the KDE values will now represent the number of points per square kilometer\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                              kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n\nWorking with different automatic bandwidth methods\nThere are three other spatstat functions that can be used to determine bandwidth: bw.CvL(), bw.scott(), and bw.ppl()\nWe can observe the different computed values by using the code chunks below:\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\n\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2984095 \n\n\nBaddeley et al (2016) suggested the use of the bw.ppl() because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. They also insist that if the purpose of one’s study is to detect a single tight cluster in the midst of random noise then bw.diggle() seems to work best.\nThe code chunk beow will be used to compare the output of using bw.diggle() and bw.ppl().\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\nWorking with different kernel methods\nThe default kernel method used is gaussian, but, as mentioned, there are three other options.\nThe code chunk below can be used to compare the KDE of the four different methods based on the output of plot()\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#fixed-and-adaptive-kde",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#fixed-and-adaptive-kde",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Fixed and Adaptive KDE",
    "text": "Fixed and Adaptive KDE\n\nComputing the KDE by using a fixed bandwidth\nThe code chunk below computes a KDE layer with a bandwidth of 600 meters. This is done by setting a sigma value of 0.6 as the unit of measurement is kilometer. (600m = 0.6km)\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km,\n                               sigma=0.6,\n                               edge=TRUE,\n                               kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\nComputing KDE by using an adaptive bandwidth\nFixed bandwidth will be sensitive to highly skewed distribution over geographic units– for example, in rural vs urban areas. One way to overcome this problem is by using adaptive bandwidths.\nThe code chunk below uses adaptive.density() to derive adaptive KDE.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can use the code chunk below to compare the outputs of using fixed and adaptive bandwidths\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\nConverting KDE output into grid object\nWe can convert the KDE output into a grid object using the code below\n\ngridded_kde_childcareSG_bw &lt;- as(kde_childcareSG.bw, \"SpatialGridDataFrame\")\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nConverting gridded output into raster\nWe then convert the gridded KDE object into a RasterLayer object using raster() of the raster package\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\nWe can take a look at the properties of thee new raster object by calling it as below. Note that the CRS property of this object is NA\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\n\nAssigning projection systems\nThe code chunk below will add CRS information into kde_childcare_SG_bw_raster raster layer\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : v \nvalues     : -8.476185e-15, 28.51831  (min, max)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#visualising-the-output-in-tmap",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#visualising-the-output-in-tmap",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Visualising the output in tmap",
    "text": "Visualising the output in tmap\nThe code chunk below will display a raster object into a cartographic quality map using the tmap package. Notice that the raster values are encoded explicitly onto the raster pixel using the values in the “v” field.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(palette = \"viridis\", title = \"layer\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#comparing-spatial-point-patterns-using-kde",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#comparing-spatial-point-patterns-using-kde",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Comparing spatial point patterns using KDE",
    "text": "Comparing spatial point patterns using KDE\nIn this section, we will compare the KDE of childcare facilities in Punggol, Tampines, Chua Chu Kang and Jurong West (planning areas)\n\nExtracting study area\nThe code chunk below extracts the target planning areas from mpsz_sf\n\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nThe code chunks below plot the different target planning areas using tmap_arrange().\n\npunggol &lt;- tm_shape(pg) + \n  tm_polygons() + tm_layout(title = \"Punggol\")\n\ncck &lt;- tm_shape(ck) + \n  tm_polygons() + tm_layout(title = \"Chua Chu Kang\")\n\ntampines &lt;- tm_shape(tm) + \n  tm_polygons() + tm_layout(title = \"Tampines\")\n\njwest &lt;- tm_shape(jw) + \n  tm_polygons() + tm_layout(title = \"Jurong West\")\n\ntmap_arrange(punggol, tampines, cck, jwest, asp=2, ncol=2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nCreating owin object\nThe code chunk below converts the four objeects into owin which is a requirement for analysing with spatstat\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\nCombining childcare points and the study area\nThe code chunk below extracts the childcare points/events that are within each of the target areas\n\nchildcare_pg_ppp = childcare_ppp[pg_owin]\nchildcare_tm_ppp = childcare_ppp[tm_owin]\nchildcare_ck_ppp = childcare_ppp[ck_owin]\nchildcare_jw_ppp = childcare_ppp[jw_owin]\n\nWe use rescale.ppp() in the next code chunk to transform the unit of measure from meter to kilometer\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below s used to plot the four target areas and the locations of their childcare centres\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\nplot(childcare_tm_ppp.km, main=\"Tampines\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 89 symbols are shown in the symbol map\n\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 88 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\n\n\nComputing KDE\nThe code chunk below computes the KDE of the four target areas. bw.diggle() is the method used to compute for the bandwidths\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\n\n\n\n\n\n\nComputing fixed bandwidth KDE\nTo enable comparisons, we can set the bandwidth to 250m using the code chunk below\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#nearest-neighbour-analysis",
    "href": "Hands-on/Hands-On_Ex03/Hands-On_Ex03.html#nearest-neighbour-analysis",
    "title": "First Order Spatial Points Analysis Methods",
    "section": "Nearest Neighbour Analysis",
    "text": "Nearest Neighbour Analysis\nFor the succeeding section, we perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() from spatstat.\nThe test hypotheses for these are:\n\n\\(H_0\\) - The distribution of event locations (childcard service centres) is random\n\\(H_1\\) - The distribution of event locations is not random\n\nFor our testing, 95% confidence interval will be used\n\nTesting spatial point patterns using Clark-Evans test\nThe code chunk below runs the test on childcareSG_ppp\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.55631, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nThe resulting p value ( \\(2.2 * 10^{-16}\\)) is very small and is below our significance level of 5%. Based on this, we reject the null hypothesis and conclude that the childcare centres are not randomly distributed.\n\n\nClarke-Evans test on individual planning areas\nThe code chunks below runs clarkevans.test() on the target areas separately.\nThe outputs show that the test rejects the null hypothesis for Tampines and Jurong West, but not for the other two. (i.e., it concludes that the childcare centres in Tampines and in Jurong West are not randomly distributed)\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.91415, p-value = 0.1996\nalternative hypothesis: two-sided\n\n\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.77989, p-value = 7.112e-05\nalternative hypothesis: two-sided\n\n\n\nclarkevans.test(childcare_pg_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_pg_ppp\nR = 0.88576, p-value = 0.08783\nalternative hypothesis: two-sided\n\n\n\nclarkevans.test(childcare_jw_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_jw_ppp\nR = 0.88937, p-value = 0.04711\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Geospatial Data Wrangling with R",
    "section": "",
    "text": "For this hands-on exercise, we performed basic data wrangling tasks using the sf package in R.\nThis exercise is based on Chapter 1 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#data-sources",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#data-sources",
    "title": "Geospatial Data Wrangling with R",
    "section": "Data Sources",
    "text": "Data Sources\nThe exercise will use the following publicly available datasets:\n\nMaster Plan 2014 Subzone Boundary from data.gov.sg\nPre-Schools location from data.gov.sg\nCycling Path from LTA DataMall\nSingapore AirBNB listing data from Inside AirBNB\n\nThe files from the first three are loaded into a folder named geospatial, while the last one (AirBNB listings) is loaded into a folder named aspatial."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-launching-r-packages",
    "title": "Geospatial Data Wrangling with R",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of two R packages: sf and tidyverse. Tidyverse is a family of R packages used for data wrangling and visualization. Sf is used for importing, managing and processing geospatial data.\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#importing-polygon-feature-data-in-shapefile-format",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#importing-polygon-feature-data-in-shapefile-format",
    "title": "Geospatial Data Wrangling with R",
    "section": "Importing polygon feature data in shapefile format",
    "text": "Importing polygon feature data in shapefile format\nMP14_SUBZONE_WEB_PL is a polygon feature layer in ESRI shapefile format from the first data source. (Master Plan 2014 Subzone Boundary from data.gov.sg) This will get loaded into R as a polygon feature data frame.\nThe st_read() function call for ESRI shapefiles requires two arguments: dsn which defines the path, and layer which defines the shapefile name. The path only requires a folder and therefore does not require a file (with an extension) to be named. We load this data into a dataframe mpsz.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe message confirms that the load is successful and that the objects are multipolygon features. It also gives information on the number of features (323), fields (15) and the coordinate system. (SVY21) The bounding box value defines the extent of the data."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#importing-polyline-feature-data-in-shapefile-format",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#importing-polyline-feature-data-in-shapefile-format",
    "title": "Geospatial Data Wrangling with R",
    "section": "Importing polyline feature data in shapefile format",
    "text": "Importing polyline feature data in shapefile format\nCyclingPath is a line feature layer in ESRI shapefile format from the third data source. (Cycling Path from LTA) This will get loaded into R as a line feature data frame.\nA similar function call is used to load the data into R as a dataframe cyclingpath.\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message confirms the type (Multistring), the number of features (3138) and fields (2) among other information."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#importing-gis-data-in-kml-format",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#importing-gis-data-in-kml-format",
    "title": "Geospatial Data Wrangling with R",
    "section": "Importing GIS data in kml format",
    "text": "Importing GIS data in kml format\nPreSchoolsLocation is a point feature layer in kml format from the second data source. (Preschools Location from data.gov.sg)\nThe st_read() function call for KML files requires one parameter, which is the complete path, including the kml filename. We load this data into a dataframe preschool.\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-on_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message confirms the type (Point), the number of features (2290) and fields (2) among other information."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#working-with-st_geometry",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#working-with-st_geometry",
    "title": "Geospatial Data Wrangling with R",
    "section": "Working with st_geometry()",
    "text": "Working with st_geometry()\nThe geometry column in the sf dataframe is a list of class sfc which contains the geometries. The contents of the column can be retrieved by:\n\ncalling the column using mpsz$geometry , mpsz$geom , or mpsz[[1]]\nusing the function st_geometry()\n\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303..."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#working-with-glimpse",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#working-with-glimpse",
    "title": "Geospatial Data Wrangling with R",
    "section": "Working with glimpse()",
    "text": "Working with glimpse()\nThe glimpse() function from dplyr reveals the data type of each field and gives the first few observations.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…"
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#working-with-head",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#working-with-head",
    "title": "Geospatial Data Wrangling with R",
    "section": "Working with head()",
    "text": "Working with head()\nThe Base R head() function reveals the first elements of the dataframe. The number of elements can be set by specifying an n argument. It also displays information for an sf dataframe like the geometry type, bounding box and projection system.\n\nhead(mpsz, n = 5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#assigning-epsg-code-to-a-simple-featured-data-frame",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#assigning-epsg-code-to-a-simple-featured-data-frame",
    "title": "Geospatial Data Wrangling with R",
    "section": "Assigning EPSG code to a simple featured data frame",
    "text": "Assigning EPSG code to a simple featured data frame\nA common issue that can happen during importation of the data is that the coordinate system is missing or wrongly assigned during the process.\nThe st_crs() function can be used to display information on the coordinate system of an sf dataframe.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nWhile mpsz appears to be projected in svy21 as expected, the output shows that it using EPSG code 9001 instead of 3414, which is the correct one for svy21. We can use the st_set_crs() function of sf to assign the correct EPSG code to mpsz. (as a new dataframe mpsz3414)\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nRunning st_crs() on mpsz3414 confirms that the new dataframe has the correct EPSG code assigned.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21",
    "title": "Geospatial Data Wrangling with R",
    "section": "Transforming the projection of preschool from wgs84 to svy21",
    "text": "Transforming the projection of preschool from wgs84 to svy21\nGeographic coordinate systems are not appropriate for geospatial analysis if the analysis requires distance or area measurements. Because of this, transforming data from geographic to projected coordinate systems is a common task.\nThe code block and output below shows that preschool is in the wgs84 coordinate system.\n\nst_geometry(preschool)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.8072 1.299333 0)\n\n\nPOINT Z (103.826 1.312839 0)\n\n\nPOINT Z (103.8409 1.348843 0)\n\n\nPOINT Z (103.8048 1.435024 0)\n\n\nPOINT Z (103.839 1.33315 0)\n\n\nSince reprojection is required, st_transform() from the sf package will be used. (st_set_crs() will not do the job) The following code chunk performs the reprojection into a new dataframe preschool3414.\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\nChecking the contents confirms that preschool3414 is now using the svy21 projected coordinate system.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nPOINT Z (25089.46 31299.16 0)\n\n\nPOINT Z (27189.07 32792.54 0)\n\n\nPOINT Z (28844.56 36773.76 0)\n\n\nPOINT Z (24821.92 46303.16 0)\n\n\nPOINT Z (28637.82 35038.49 0)"
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-aspatial-data",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-aspatial-data",
    "title": "Geospatial Data Wrangling with R",
    "section": "Importing the aspatial data",
    "text": "Importing the aspatial data\nThe appropriate function from the readr package should be used depending on the file format. For csv files, the read_csv() function loads our file into a tibble dataframe named listings\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nRows: 3540 Columns: 75\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (26): listing_url, source, name, description, neighborhood_overview, pi...\ndbl  (38): id, scrape_id, host_id, host_listings_count, host_total_listings_...\nlgl   (6): host_is_superhost, host_has_profile_pic, host_identity_verified, ...\ndate  (5): last_scraped, host_since, calendar_last_scraped, first_review, la...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe list() function from Base R displays the contents of the dataframe and also shows that there are 3540 rows and 75 columns.\n\nlist(listings)\n\n[[1]]\n# A tibble: 3,540 × 75\n       id listing_url            scrape_id last_scraped source name  description\n    &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;      \n 1  71609 https://www.airbnb.co…   2.02e13 2024-06-29   previ… Ensu… For 3 room…\n 2  71896 https://www.airbnb.co…   2.02e13 2024-06-29   city … B&B … &lt;NA&gt;       \n 3  71903 https://www.airbnb.co…   2.02e13 2024-06-29   city … Room… Like your …\n 4 275343 https://www.airbnb.co…   2.02e13 2024-06-29   city … 10mi… **IMPORTAN…\n 5 275344 https://www.airbnb.co…   2.02e13 2024-06-29   city … 15 m… Lovely hom…\n 6 289234 https://www.airbnb.co…   2.02e13 2024-06-29   previ… Book… This whole…\n 7 294281 https://www.airbnb.co…   2.02e13 2024-06-29   city … 5 mi… I have 3 b…\n 8 324945 https://www.airbnb.co…   2.02e13 2024-06-29   city … Comf… **IMPORTAN…\n 9 330095 https://www.airbnb.co…   2.02e13 2024-06-29   city … Rela… **IMPORTAN…\n10 344803 https://www.airbnb.co…   2.02e13 2024-06-29   city … Budg… Direct bus…\n# ℹ 3,530 more rows\n# ℹ 68 more variables: neighborhood_overview &lt;chr&gt;, picture_url &lt;chr&gt;,\n#   host_id &lt;dbl&gt;, host_url &lt;chr&gt;, host_name &lt;chr&gt;, host_since &lt;date&gt;,\n#   host_location &lt;chr&gt;, host_about &lt;chr&gt;, host_response_time &lt;chr&gt;,\n#   host_response_rate &lt;chr&gt;, host_acceptance_rate &lt;chr&gt;,\n#   host_is_superhost &lt;lgl&gt;, host_thumbnail_url &lt;chr&gt;, host_picture_url &lt;chr&gt;,\n#   host_neighbourhood &lt;chr&gt;, host_listings_count &lt;dbl&gt;, …\n\n\nScanning through the columns reveals that there are columns named longitude and latitude which appear to be in decimal degree format. We will assume that these are recorded based on the wgs84 geographic coordinate system."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#creating-a-simple-feature-from-an-aspatial-data-frame",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#creating-a-simple-feature-from-an-aspatial-data-frame",
    "title": "Geospatial Data Wrangling with R",
    "section": "Creating a simple feature from an aspatial data frame",
    "text": "Creating a simple feature from an aspatial data frame\nWe use the code chunk below to create an sf dataframe listings_sf from the aspatial data in listings.\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nWe used the following arguments in the above function call:\n\ncoords requires the column names of the x-coordinates followed by the y-coordinates\ncrs indicates the epsg format used in the data. EPSG 4326 corresponds to the wgs84 geographic coordinate system\n%&gt;% is used to nest st_transform() and transform the newly created sf dataframe into the svy21 coordinate system (EPSG 3414)\n\nWe can then use glimpse() on the new dataframe to examine the contents.\n\nglimpse(listings_sf)\n\nRows: 3,540\nColumns: 74\n$ id                                           &lt;dbl&gt; 71609, 71896, 71903, 2753…\n$ listing_url                                  &lt;chr&gt; \"https://www.airbnb.com/r…\n$ scrape_id                                    &lt;dbl&gt; 2.024063e+13, 2.024063e+1…\n$ last_scraped                                 &lt;date&gt; 2024-06-29, 2024-06-29, …\n$ source                                       &lt;chr&gt; \"previous scrape\", \"city …\n$ name                                         &lt;chr&gt; \"Ensuite Room (Room 1 & 2…\n$ description                                  &lt;chr&gt; \"For 3 rooms.Book room 1 …\n$ neighborhood_overview                        &lt;chr&gt; NA, NA, \"Quiet and view o…\n$ picture_url                                  &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_id                                      &lt;dbl&gt; 367042, 367042, 367042, 1…\n$ host_url                                     &lt;chr&gt; \"https://www.airbnb.com/u…\n$ host_name                                    &lt;chr&gt; \"Belinda\", \"Belinda\", \"Be…\n$ host_since                                   &lt;date&gt; 2011-01-29, 2011-01-29, …\n$ host_location                                &lt;chr&gt; \"Singapore\", \"Singapore\",…\n$ host_about                                   &lt;chr&gt; \"Hi My name is Belinda -H…\n$ host_response_time                           &lt;chr&gt; \"within an hour\", \"within…\n$ host_response_rate                           &lt;chr&gt; \"100%\", \"100%\", \"100%\", \"…\n$ host_acceptance_rate                         &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"99%…\n$ host_is_superhost                            &lt;lgl&gt; FALSE, FALSE, FALSE, FALS…\n$ host_thumbnail_url                           &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_picture_url                             &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_neighbourhood                           &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ host_listings_count                          &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49…\n$ host_total_listings_count                    &lt;dbl&gt; 11, 11, 11, 73, 73, 11, 8…\n$ host_verifications                           &lt;chr&gt; \"['email', 'phone']\", \"['…\n$ host_has_profile_pic                         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ host_identity_verified                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ neighbourhood                                &lt;chr&gt; NA, NA, \"Singapore, Singa…\n$ neighbourhood_cleansed                       &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ neighbourhood_group_cleansed                 &lt;chr&gt; \"East Region\", \"East Regi…\n$ property_type                                &lt;chr&gt; \"Private room in villa\", …\n$ room_type                                    &lt;chr&gt; \"Private room\", \"Private …\n$ accommodates                                 &lt;dbl&gt; 3, 1, 2, 1, 1, 4, 2, 1, 1…\n$ bathrooms                                    &lt;dbl&gt; NA, 0.5, 0.5, 2.0, 2.5, N…\n$ bathrooms_text                               &lt;chr&gt; \"1 private bath\", \"Shared…\n$ bedrooms                                     &lt;dbl&gt; 2, 1, 1, 1, 1, 3, 2, 1, 1…\n$ beds                                         &lt;dbl&gt; NA, 1, 2, 1, 1, NA, 1, 1,…\n$ amenities                                    &lt;chr&gt; \"[\\\"Free parking on premi…\n$ price                                        &lt;chr&gt; NA, \"$80.00\", \"$80.00\", \"…\n$ minimum_nights                               &lt;dbl&gt; 92, 92, 92, 180, 180, 92,…\n$ maximum_nights                               &lt;dbl&gt; 365, 365, 365, 999, 999, …\n$ minimum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 180, 180, 92,…\n$ maximum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 180, 180, 92,…\n$ minimum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ maximum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ minimum_nights_avg_ntm                       &lt;dbl&gt; 92, 92, 92, 180, 180, 92,…\n$ maximum_nights_avg_ntm                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ calendar_updated                             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ has_availability                             &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ availability_30                              &lt;dbl&gt; 30, 30, 30, 28, 0, 29, 30…\n$ availability_60                              &lt;dbl&gt; 59, 53, 60, 58, 0, 58, 60…\n$ availability_90                              &lt;dbl&gt; 89, 83, 90, 62, 0, 88, 90…\n$ availability_365                             &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 3…\n$ calendar_last_scraped                        &lt;date&gt; 2024-06-29, 2024-06-29, …\n$ number_of_reviews                            &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 1…\n$ number_of_reviews_ltm                        &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1…\n$ number_of_reviews_l30d                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ first_review                                 &lt;date&gt; 2011-12-19, 2011-07-30, …\n$ last_review                                  &lt;date&gt; 2020-01-17, 2019-10-13, …\n$ review_scores_rating                         &lt;dbl&gt; 4.44, 4.16, 4.41, 4.40, 4…\n$ review_scores_accuracy                       &lt;dbl&gt; 4.37, 4.22, 4.39, 4.16, 4…\n$ review_scores_cleanliness                    &lt;dbl&gt; 4.00, 4.09, 4.52, 4.26, 4…\n$ review_scores_checkin                        &lt;dbl&gt; 4.63, 4.43, 4.63, 4.47, 4…\n$ review_scores_communication                  &lt;dbl&gt; 4.78, 4.43, 4.64, 4.42, 4…\n$ review_scores_location                       &lt;dbl&gt; 4.26, 4.17, 4.50, 4.53, 4…\n$ review_scores_value                          &lt;dbl&gt; 4.32, 4.04, 4.36, 4.63, 4…\n$ license                                      &lt;chr&gt; NA, NA, NA, \"S0399\", \"S03…\n$ instant_bookable                             &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE…\n$ calculated_host_listings_count               &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49…\n$ calculated_host_listings_count_entire_homes  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ calculated_host_listings_count_private_rooms &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 6, 49…\n$ calculated_host_listings_count_shared_rooms  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reviews_per_month                            &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0…\n$ geometry                                     &lt;POINT [m]&gt; POINT (41972.5 3639…\n\n\nThe output shows that a new column geometry has been introduced in the data, while the coordinate columns longitude and latitude have been dropped."
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#buffering",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#buffering",
    "title": "Geospatial Data Wrangling with R",
    "section": "Buffering",
    "text": "Buffering\nSCENARIO\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nSOLUTION\nFirst, use st_buffer() to compute a 5m buffer around the cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nSecond, calculate the areas of the buffers with st_area().\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLast, summing the values of the new column using sum() will give the total land required.\n\nsum(buffer_cycling$AREA)\n\n2218855 [m^2]\n\n\nDone! 2.2 million square meters are required"
  },
  {
    "objectID": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "href": "Hands-on/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "title": "Geospatial Data Wrangling with R",
    "section": "Point-in-polygon count",
    "text": "Point-in-polygon count\nSCENARIO 1\nA group wants to fund out the number of pre-schools in each planning subzone\nSOLUTION 1\nFirst, use st_intersects to identify preschools located in each planning subzone. (stored as a list) Then use lengths() from Base R to calculate the number of preschools in each planning subzone. This is stored in a new column PreSch Count\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nThe new column can be summarized using summary() as shown. The output shows that the median number of preschools ranges from 0 to 72 and the median is 4.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nThe planning subzones with the most number of preschools can be displayed using the top_n() function of the dplyr package. This shows that Tampines East has the maximum number of 72 preschools.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nSCENARIO 2\nThe group also wants to understand the density of preschools. Larger subzones are expected to have more preschools so density might be a more appropriate measure to compare\nSOLUTION 2\nWe again use st_area to compute areas. This time we do this to compute for each subzone’s.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNext, the mutate() function from the dplyr package is used to compute the density\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nThe top_n() function can be used to fetch the subzone with the highest density, which is Cecil.\n\ntop_n(mpsz3414, 1, `PreSch Density`)\n\nSimple feature collection with 1 feature and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29501.64 ymin: 28623.75 xmax: 29976.93 ymax: 29362.03\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO SUBZONE_N SUBZONE_C CA_IND    PLN_AREA_N PLN_AREA_C\n1       27          8     CECIL    DTSZ08      Y DOWNTOWN CORE         DT\n        REGION_N REGION_C          INC_CRC FMEL_UPD_D  X_ADDR   Y_ADDR\n1 CENTRAL REGION       CR 65AA82AF6F4D925D 2014-12-05 29730.2 29011.33\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   2116.095   196619.9 MULTIPOLYGON (((29808.18 28...            7\n            Area   PreSch Density\n1 196619.9 [m^2] 35.60169 [1/m^2]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! I’m Derek (Federico Jose Rodriguez) and I built this website as I started taking my Geospatial Analytics and Applications course in the MITB Program of Singapore Management University in 2024."
  },
  {
    "objectID": "about.html#about-me-and-this-site",
    "href": "about.html#about-me-and-this-site",
    "title": "About",
    "section": "",
    "text": "Hi there! I’m Derek (Federico Jose Rodriguez) and I built this website as I started taking my Geospatial Analytics and Applications course in the MITB Program of Singapore Management University in 2024."
  },
  {
    "objectID": "about.html#quarto-websites",
    "href": "about.html#quarto-websites",
    "title": "About",
    "section": "Quarto Websites",
    "text": "Quarto Websites\nThis is a Quarto website.\nTo learn more about Quarto websites and start creating your own, please visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html",
    "title": "Choropleth Mapping with R",
    "section": "",
    "text": "For this hands-on exercise, we learned how to plot choropleth maps by using an R package called tmap\nThis exercise is based on Chapter 2 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#data-sources",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#data-sources",
    "title": "Choropleth Mapping with R",
    "section": "Data Sources",
    "text": "Data Sources\nThe exercise will use the following publicly available datasets:\n\nMaster Plan 2014 Subzone Boundary from data.gov.sg\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 from singstat.gov\n\nThe first one is geospatial data and was also used in the previous hands-on exercise. The second source is for aspatial data but the PA and SZ fields in it allows geocoding into the shapefile."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#installing-and-launching-r-packages",
    "title": "Choropleth Mapping with R",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of three R packages: sf, tidyverse and tmap. We have already introduced the first two in the last exercise: tidyverse is a family of R packages used for data wrangling and visualization, while sf is used for importing, managing and processing geospatial data. Tidyverse is made up of multiple packages which include tidyr and dplyr which will be the specific packages where the functions we use will come from.\nTmap stands for thematic map and will enable us to create the functional choropleth maps that go beyond the capabilities of plot()\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, tidyverse, tmap)\ntmap_options(show.messages = FALSE)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#importing-geospatial-data-into-r",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#importing-geospatial-data-into-r",
    "title": "Choropleth Mapping with R",
    "section": "Importing Geospatial Data into R",
    "text": "Importing Geospatial Data into R\nWe first import MP14_SUBZONE_WEB_PL using st_read() function by providing the path and the layer name as parameters.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWe can examine the contents by calling the dataframe like in the code chunk below. This function call only shows the first 10 features or rows of the dataframe.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#importing-attribute-data-into-r",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#importing-attribute-data-into-r",
    "title": "Choropleth Mapping with R",
    "section": "Importing Attribute Data into R",
    "text": "Importing Attribute Data into R\nOur remaining data is in a csv file which we will load into a dataframe called popdata using read_csv() which comes from the readr package that is included in tidyverse.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesexfa2011to2020.csv\")\n\nRows: 738492 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, FA\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can quickly examine the data by calling the dataframe name. This shows that popdata consists of 738K records or rows with 7 attributes or columns.\n\npopdata\n\n# A tibble: 738,492 × 7\n   PA         SZ                     AG     Sex     FA              Pop  Time\n   &lt;chr&gt;      &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   &lt;= 60             0  2011\n 2 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   &gt;60 to 80        10  2011\n 3 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   &gt;80 to 100       30  2011\n 4 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   &gt;100 to 120      80  2011\n 5 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   &gt;120             20  2011\n 6 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Not Available     0  2011\n 7 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females &lt;= 60             0  2011\n 8 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females &gt;60 to 80        10  2011\n 9 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females &gt;80 to 100       40  2011\n10 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females &gt;100 to 120      90  2011\n# ℹ 738,482 more rows\n\n\nWe will use the 2020 information from this to build a new data table popdata2020 which includes the following variables:\n\nPA and SZ give information on the location (planning area and township)\nYOUNG is the population for those aged 0 to 24\nECONOMY ACTIVE is the population for those aged 25 to 64\nAGED is the population for those aged 65 and above\nTOTAL is the total population across all age groups\nDEPENDENCY is the ratio between young and aged against the economy active group"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#data-wrangling",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#data-wrangling",
    "title": "Choropleth Mapping with R",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nTo generate the required table from popdata, we will use pivot_wider() of tidyr package, mutate(), filter(), group_by() and select() of dplyr package. All of these are included in tidyverse.\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\nWe can check that the transformation has been executed properly by displaying the new dataframe.\n\npopdata2020\n\n# A tibble: 332 × 7\n   PA         SZ                   YOUNG `ECONOMY ACTIVE`  AGED TOTAL DEPENDENCY\n   &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo Kio Town Cen…  1440             2640   770  4850      0.837\n 2 Ang Mo Kio Cheng San             6660            15380  6080 28120      0.828\n 3 Ang Mo Kio Chong Boon            6150            13970  6450 26570      0.902\n 4 Ang Mo Kio Kebun Bahru           5500            12040  5080 22620      0.879\n 5 Ang Mo Kio Sembawang Hills       2130             3390  1270  6790      1.00 \n 6 Ang Mo Kio Shangri-La            3970             8430  3540 15940      0.891\n 7 Ang Mo Kio Tagore                2220             4160  1520  7900      0.899\n 8 Ang Mo Kio Townsville            4720            11430  5050 21200      0.855\n 9 Ang Mo Kio Yio Chu Kang             0                0     0     0    NaN    \n10 Ang Mo Kio Yio Chu Kang East     1190             2230   740  4160      0.865\n# ℹ 322 more rows"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#joining-the-attribute-and-geospatial-data",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#joining-the-attribute-and-geospatial-data",
    "title": "Choropleth Mapping with R",
    "section": "Joining the attribute and geospatial data",
    "text": "Joining the attribute and geospatial data\nTo use this attribute data for our analysis, we need to join it with the geospatial data. The first step will be to convert the PA and SZ values to uppercase to make sure that they follow the same convention as the geospatial data. We use the mutate_at() function to apply this transformation.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nThe next step is to use left_join() from dplyr to merge the two tables using SZ.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nAs the left side is an sf dataframe, the resulting object is also an sf dataframe and we write this into a file to store for future use without rerunning all the transformations so far.\n\nwrite_rds(mpsz_pop2020, \"data/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#plotting-a-choropleth-map-quickly-using-qtm",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#plotting-a-choropleth-map-quickly-using-qtm",
    "title": "Choropleth Mapping with R",
    "section": "Plotting a choropleth map quickly using qtm()",
    "text": "Plotting a choropleth map quickly using qtm()\nqtm() is a convenient way to produce a thematic or choropleth map. It provides a good default or initial visualization.\nThe code chunk below draws a standard choropleth map using the values from DEPENDENCY as the fill.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThe tmap_mode(\"plot\") produces a static plot. To produce an interactive plot, tmap_mode(\"view\") should be used instead.\nqtm() takes the sf dataframe as its first argument. The fill argument defines which attribute to map."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "title": "Choropleth Mapping with R",
    "section": "Creating a choropleth map by using tmap’s elements",
    "text": "Creating a choropleth map by using tmap’s elements\nWhile convenient, the downside of qtm() is that it makes controlling the aesthetics of individual layers harder to control. Using tmap’s drawing elements allows more customization, and allows the production of higher quality maps.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nWe go through the different elements plotted and the respective functions below.\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elements.\nIn the code chunk below, tm_shape() is used to define the input data and then tm_polygons() draws the planning subzone polygons.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nThe code chunk below generates a choropleth map of the dependency ratio by planning subzone by passing DEPENDENCY to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nA few points on tm_polygons():\n\nThe default binning used is called “pretty”. We will go more into data classification methods later\nThe default color scheme is YlOrRd (Yellow Orange Red) of ColorBrewer. We will also go more into color schemes later\nBy default, missing values are shaded grey\n\n\n\nDrawing a choropleth map using tm_fill() and tm_border()\ntm_polygons() is actually a wrapper of two other functions: tm_fill() and tm_border().\ntm_fill() shades the polygons using the default color scheme. The code chunk below draws the choropleth map with just this element.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\ntm_borders() adds the borders of the shapefile onto the map. We add this element to the previous code to produce a similar plot to the one generated with tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\ntm_borders() has the following (optional) arguments:\n\nalpha defines the transparency and takes a value of 0 (totally transparent) to 1 (totally opaque)\ncol defines the border colour\nlwd defines the border line width (default 1)\nlty defines the border line type (default solid)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#data-classification-methods-on-tmap",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#data-classification-methods-on-tmap",
    "title": "Choropleth Mapping with R",
    "section": "Data Classification Methods on tmap",
    "text": "Data Classification Methods on tmap\nData classification refers to grouping (a large number of) observations into data ranges or classes. tmap provides ten data classification methods which include: fixed, sd, equal, pretty, quantile, kmeans, hclust, bclust, fisher, and jenks\nThe data classification method can be defined by using the style argument in tm_fill() or tm_polygons()\n\nPlotting choropleth maps with built-in classification methods\nThe code below produces a plot using quantile data classification with 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe code below produces a plot using equal data classification with 5 classes. Note that the previous chart produces evenly distributed classes, while this new chart has almost all but one zone in the first class.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe code below produces a plot using jenks data classification. By changing the style argument, the other data classification methods can be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nPlotting Choropleth Maps with custom break\nThe builtin styles compute the class breaks or boundaries internally. To override this, the breakpoints can be set explicitly using the breaks argument of tm_fill()\nThe breaks defined will include the minimum and maximum. Therefore, to define n classes, n+1 breakpoints should be defined in the breaks argument.\nWe first run some descriptive statistics on DEPENDENCY before we define the breakpoints\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.7113  0.7926  0.8561  0.8786 19.0000      92 \n\n\nBased on the distribution, say we define the breakpoints for five classes to be 0.6, 0.7, 0.8 and 0.9 to split. We need to include a minimum, 0, and a maximum, 20, for the breaks argument.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 20)) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#colour-schemes",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#colour-schemes",
    "title": "Choropleth Mapping with R",
    "section": "Colour Schemes",
    "text": "Colour Schemes\ntmap supports user-defined colour maps, and pre-defined colour maps from the RColorBrewer package.\n\nUsing ColourBrewer palette\nTo change the colour map, we assign a value to the palette argument of tm_fill() as shown in the code below\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe choropleth map is shaded in blue where a darker shade corresponds to a higher value of DEPENDENCY. To reverse this behavior, we can add a “-” prefix to the passed value.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Blues\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#map-layouts",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#map-layouts",
    "title": "Choropleth Mapping with R",
    "section": "Map Layouts",
    "text": "Map Layouts\nMap layout refers to the combination of all map elements into a cohesive map. Aside from the objects, the other elements include the title, the scale bar, a compass, margins and aspect ratios.\n\nMap Legend\nSeveral options are provided to adjust the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nMap Style\nA wide variety of layout settings can be changed by calling tmap_style()\nThe codeblock below shows the map using the classic style.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\nCartographic Furniture\ntmap provides options to add other elements like a compass, scale, and grid lines. In the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add these elements.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nNote that the above map still uses the classic style. To revert to the default style, use the code chunk below.\n\ntmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\""
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#drawing-small-multiple-choropleth-maps",
    "title": "Choropleth Mapping with R",
    "section": "Drawing Small Multiple Choropleth Maps",
    "text": "Drawing Small Multiple Choropleth Maps\nSmall multiple maps or facet maps are composed of multiple maps arranged side-by-side. (in a grid, stacked vertically, lined up) Small multiple maps allow visualization of how spatial relationships change with respect to one variable. (e.g., time)\nThere are three ways to define small multiples in tmap:\n\nby assigning multiple values to at least one aesthetic argument,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange()\n\n\nAssigning multiple values to at least one of the aesthetic arguments\nIn the code below, small multiple maps are created by passing a list of (numeric) columns as the col argument of tm_fill() Each map depicts a different column or attribute.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\nIn the next code, two parameters are passed to palette argument to result to different colour maps for the each small multiple.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nBy defining a group-by variable in tm_facets()\nIn the code below, a small multiple of each region is created by using tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.units=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nBy creating multiple stand-alone maps and using tmap_arrange()\nIn the following code, two maps are defined individually and then displayed side by side using tmap_arrange()\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#mapping-spatial-objects-meeting-selection-criterion",
    "href": "Hands-on/Hands-On_Ex02/Hands-On_Ex02.html#mapping-spatial-objects-meeting-selection-criterion",
    "title": "Choropleth Mapping with R",
    "section": "Mapping Spatial Objects Meeting Selection Criterion",
    "text": "Mapping Spatial Objects Meeting Selection Criterion\nInstead of using small multiples to look at a particular subset of the data, you can also use a selection criteria or a mask to only map objects meeting a particular condition. The code below produces a map for only the Central Region.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\nWarning in pre_process_gt(x, interactive = interactive, orig_crs =\ngm$shape.orig_crs): legend.width controls the width of the legend within a map.\nPlease use legend.outside.size to control the width of the outside legend"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "",
    "text": "In this hands-on exercise, we continue learning about Spatial Point pattern analysis with the help of spatstat package. We continue looking at the dataset using the location of childcare centres in Singapore.\nThis exercise is based on Chapter 5 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#data-sources",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#data-sources",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are from public sources and are the same as the previous hands-on exercise. This includes:\n\nLocation and attribute information of childcare centres in Singapore from data.gov.sg\nMaster Plan 2014 Subzone Boundary from data.gov.sg\nNational boundary of Singapore provided in SLA and ESRI shapefile format"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#installing-and-launching-r-packages",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, spatstat, raster and tmap.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspatstat - offers a wide range of functions for point pattern analysis (PPA)\nraster - for reading, writing, manipulating and analysing models of gridded spatial data\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#importing-the-spatial-data",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#importing-the-spatial-data",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Importing the spatial data",
    "text": "Importing the spatial data\nThe code chunk below uses st_read() from the sf package to import the geospatial datasets.\n\nchildcare_sf &lt;- st_read(\"data/geospatial/child-care-services-geojson.geojson\") %&gt;% st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex04\\data\\geospatial\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf &lt;- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex04\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex04\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#checking-and-converting-projection-systems",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#checking-and-converting-projection-systems",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Checking and converting projection systems",
    "text": "Checking and converting projection systems\nWe can use st_crs() to check what coordinate systems are used in each of the three sf dataframes.\nchildcare_sf is in SVY21 using EPSG code 3414 after our load and transform operation above. However, sg_sf and mpsz_sf reveals that they are not using the correct EPSG code.\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nWe use the code chunk below to update the EPSG code for the last two objects\n\nsg_sf = st_set_crs(sg_sf, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\nmpsz_sf = st_set_crs(mpsz_sf, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#mapping-the-geospatial-datasets",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#mapping-the-geospatial-datasets",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Mapping the geospatial datasets",
    "text": "Mapping the geospatial datasets\nIt is always good practice to plot or display imported data to check that they have been loaded and transformed properly. The code chunk below creates a map using tmap with all three objects where they appear to be mapped properly.\n\ntm_shape(sg_sf)+\n    tm_fill(\"lightblue\") +\n    tm_borders(lwd = 0.1,  alpha = 1)+\n    tm_shape(mpsz_sf) +\n    tm_fill(\"grey\", alpha = 0.5) +\n    tm_borders(lwd = 0.1,  alpha = 1) +\n    tm_shape(childcare_sf) +\n    tm_dots(col = \"darkgreen\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#converting-sf-dataframe-sp-spatial-class",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#converting-sf-dataframe-sp-spatial-class",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Converting sf dataframe sp spatial class",
    "text": "Converting sf dataframe sp spatial class\nThe code chunk below converts the three dataframes using as_Spatial() from the sf package.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\nWe can check the contents of the new dataframes by calling them. This confirms that they are in the spatial* class format.\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#converting-the-spatial-class-to-generic-sp-format",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#converting-the-spatial-class-to-generic-sp-format",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Converting the spatial class to generic sp format",
    "text": "Converting the spatial class to generic sp format\nspatstat requires that the data is in ppp object form. There is no direct way to do this from spatial* class. We need to convert spatial* class to a spatial object first.\nThe code chunk below transform two of the spatial* objects into generic sp objects.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nCalling childcare_sp and sg_sp lets us check their properties.\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#converting-generic-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#converting-generic-sp-format-into-spatstats-ppp-format",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Converting generic sp format into spatstat’s ppp format",
    "text": "Converting generic sp format into spatstat’s ppp format\nWe will then use as.ppp() from spatstat package to convert the (spatial) data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\nWarning in as.ppp.sf(childcare_sf): only first attribute column is used for\nmarks\n\nchildcare_ppp\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nWe see the difference of this format when we use plot() (from R Graphics) to produce a quick map of the data.\n\nplot(childcare_ppp)\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1545 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\nWe can see summary information on the new ppp object using the code chunk below.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#handling-duplicated-points",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#handling-duplicated-points",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Handling duplicated points",
    "text": "Handling duplicated points\nWe can check duplication in a ppp object using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] FALSE\n\n\nTo count the number of coincident points, we can use the multiplicity() function.\n\nmultiplicity(childcare_ppp)\n\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWe can wrap this in sum() to count the number of locations with more than one event.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 0\n\n\nThe outputs show that there are no duplication in childcare_ppp"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#creating-owin-object",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#creating-owin-object",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Creating owin object",
    "text": "Creating owin object\nWhen analysing spatial point patterns, it is best to confine the analysis within a geographical area. In spatstat, the object that represents the bounded region is called an owin.\nThe code chunk below creates an owin based on the sg SpatialPolygon object.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe owin object can be displayed graphically using plot() and summarized using summary()\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#combining-point-events-object-and-owin-object",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Combining point events object and owin object",
    "text": "Combining point events object and owin object\nIn this last step, we extract childcare events (locations) that are within Singapore, as depicted by the owin, using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output is a combination of the point feature and the polygon feature into a single ppp object:\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nRunning plot() on this shows both the owin and the preschool locations in a single map.\n\nplot(childcareSG_ppp)\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1545 symbols are shown in the symbol map"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#extracting-study-areas",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#extracting-study-areas",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Extracting study areas",
    "text": "Extracting study areas\nThe code chunk below extracts the target planning areas from mpsz_sf\n\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nThe code chunks below plot the different target planning areas using tmap_arrange().\n\npunggol &lt;- tm_shape(pg) + \n  tm_polygons() + tm_layout(title = \"Punggol\")\n\ncck &lt;- tm_shape(ck) + \n  tm_polygons() + tm_layout(title = \"Chua Chu Kang\")\n\ntampines &lt;- tm_shape(tm) + \n  tm_polygons() + tm_layout(title = \"Tampines\")\n\njwest &lt;- tm_shape(jw) + \n  tm_polygons() + tm_layout(title = \"Jurong West\")\n\ntmap_arrange(punggol, tampines, cck, jwest, asp=2, ncol=2, nrow = 2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#converting-target-areas-into-owin-objects",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#converting-target-areas-into-owin-objects",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Converting target areas into owin objects",
    "text": "Converting target areas into owin objects\nThe code chunk below converts the four objeects into owin which is a requirement for analysing with spatstat\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#combining-childcare-points-and-the-study-area",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#combining-childcare-points-and-the-study-area",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Combining childcare points and the study area",
    "text": "Combining childcare points and the study area\nThe code chunk below extracts the childcare points/events that are within each of the target areas\n\nchildcare_pg_ppp = childcare_ppp[pg_owin]\nchildcare_tm_ppp = childcare_ppp[tm_owin]\nchildcare_ck_ppp = childcare_ppp[ck_owin]\nchildcare_jw_ppp = childcare_ppp[jw_owin]\n\nWe use rescale.ppp() in the next code chunk to transform the unit of measure from meter to kilometer\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot the four target areas and the locations of their childcare centres\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\nplot(childcare_tm_ppp.km, main=\"Tampines\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 89 symbols are shown in the symbol map\n\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 88 symbols are shown in the symbol map"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#analysing-spatial-point-process-using-g-function",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Analysing spatial point process using G function",
    "text": "Analysing spatial point process using G function\nThe G function measures the distribution of distances from an arbitrary event to the nearest event. We will compute the G function using Gest() of the spatstat package. We will also perform monte carlo simulation using the envelope() function of the same package.\n\nG-Function Estimation for Choa Chu Kang Planning Area\n\nComputing the G function estimation\nThe code chunk below computes the G-function for the planning area using Gest()\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial pattern, we conduct hypothesis testing. The test hypotheses for these are:\n\n\\(H_0\\) - The locations of childcare service centres in Choa Chu Kang are randomly distributed\n\\(H_1\\) - The locations of childcare service centres in Choa Chu Kang are not randomly distributed\n\nFor our testing, we will use a p-value smaller than α = 0.001 to reject the null hypothesis.\nThe code chunk below produces monte carlo simulation for the G-function values using envelope()\n\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\nWe then plot the results using the code chunk below\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\nG Function Estimation for Tampines Planning Area\n\nComputing the G function estimation\nThe code chunk below computes the G-function for the planning area using Gest()\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial pattern, we conduct hypothesis testing. The test hypotheses for these are:\n\n\\(H_0\\) - The locations of childcare service centres in Tampines are randomly distributed\n\\(H_1\\) - The locations of childcare service centres in Tampines are not randomly distributed\n\nFor our testing, we will use a p-value smaller than α = 0.001 to reject the null hypothesis.\nThe code chunk below produces monte carlo simulation for the G-function values using envelope()\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#analysing-spatial-point-process-using-f-function",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Analysing Spatial Point Process using F-Function",
    "text": "Analysing Spatial Point Process using F-Function\nThe F-Function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will compute the F-function using Fest() of the spatstat package. We will again use monte carlo simulation using envelope()\n\nF-Function Estimation for Choa Chu Kang Planning Area\n\nComputing the F-function estimation\nThe code chunk below computes the F-function for the planning area using Fest()\n\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial pattern, we conduct hypothesis testing. The test hypotheses for these are:\n\n\\(H_0\\) - The locations of childcare service centres in Choa Chu Kang are randomly distributed\n\\(H_1\\) - The locations of childcare service centres in Choa Chu Kang are not randomly distributed\n\nFor our testing, we will use a p-value smaller than α = 0.001 to reject the null hypothesis.\nThe code chunk below produces monte carlo simulation for the F-function values using envelope()\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\nF-Function Estimation for Tampines Planning Area\n\nComputing the F-function estimation\nThe code chunk below computes the F-function for the planning area using Fest()\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial pattern, we conduct hypothesis testing. The test hypotheses for these are:\n\n\\(H_0\\) - The locations of childcare service centres in Tampines are randomly distributed\n\\(H_1\\) - The locations of childcare service centres in Tampines are not randomly distributed\n\nFor our testing, we will use a p-value smaller than α = 0.001 to reject the null hypothesis.\nThe code chunk below produces monte carlo simulation for the F-function values using envelope()\n\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#analysing-spatial-point-process-using-k-function",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Analysing Spatial Point Process using K-Function",
    "text": "Analysing Spatial Point Process using K-Function\nThe K-function measures the number of events found up to a given distance of any particular event. In this section, we will use Kest() of the spatstat package to estimate the K-function. We will again perform monte carlo simulations using envelope()\n\nK-Function Estimation for Choa Chu Kang Planning Area\nThe code chunk below computes the K-function for the planning area using Kest()\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\nTo confirm the observed spatial pattern, we conduct hypothesis testing. The test hypotheses for these are:\n\n\\(H_0\\) - The locations of childcare service centres in Choa Chu Kang are randomly distributed\n\\(H_1\\) - The locations of childcare service centres in Choa Chu Kang are not randomly distributed\n\nFor our testing, we will use a p-value smaller than α = 0.001 to reject the null hypothesis.\nThe code chunk below produces monte carlo simulation for the K-function values using envelope()\n\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\nK-Function Estimation for Tampines Planning Area\nThe code chunk below computes the K-function for the planning area using Kest()\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\nTo confirm the observed spatial pattern, we conduct hypothesis testing. The test hypotheses for these are:\n\n\\(H_0\\) - The locations of childcare service centres in Tampines are randomly distributed\n\\(H_1\\) - The locations of childcare service centres in Tampines are not randomly distributed\n\nFor our testing, we will use a p-value smaller than α = 0.001 to reject the null hypothesis.\nThe code chunk below produces monte carlo simulation for the K-function values using envelope()\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on/Hands-On_Ex04/Hands-On_Ex04.html#analysing-spatial-point-process-using-l-function",
    "title": "Second Order Spatial Point Pattern Analysis Methods",
    "section": "Analysing Spatial Point Process using L-Function",
    "text": "Analysing Spatial Point Process using L-Function\nIn this section, we will use Lest() of the spatstat package to estimate the L-function. We will again perform monte carlo simulations using envelope()\n\nL-Function Estimation for Choa Chu Kang Planning Area\nThe code chunk below computes the L-function for the planning area using Lest()\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\nTo confirm the observed spatial pattern, we conduct hypothesis testing. The test hypotheses for these are:\n\n\\(H_0\\) - The locations of childcare service centres in Choa Chu Kang are randomly distributed\n\\(H_1\\) - The locations of childcare service centres in Choa Chu Kang are not randomly distributed\n\nFor our testing, we will use a p-value smaller than α = 0.001 to reject the null hypothesis.\nThe code chunk below produces monte carlo simulation for the L-function values using envelope()\n\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\nL-Function Estimation for Tampines Planning Area\nThe code chunk below computes the L-function for the planning area using Lest()\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\nTo confirm the observed spatial pattern, we conduct hypothesis testing. The test hypotheses for these are:\n\n\\(H_0\\) - The locations of childcare service centres in Tampines are randomly distributed\n\\(H_1\\) - The locations of childcare service centres in Tampines are not randomly distributed\n\nFor our testing, we will use a p-value smaller than α = 0.001 to reject the null hypothesis.\nThe code chunk below produces monte carlo simulation for the L-function values using envelope()\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html",
    "title": "Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, we learn how to compute spatial weights and spatially lagged in R using the spdep package.\nThis exercise is based on Chapter 8 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#data-sources",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#data-sources",
    "title": "Spatial Weights and Applications",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on the Hunan county coming from two files:\n\nHunan county boundary layer in ESRI shapefile format\nHunan local development indicators for 2012 stored in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#installing-and-launching-r-packages",
    "title": "Spatial Weights and Applications",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, tmap, and spdep.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspdep - functions to create spatial weights\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#data-loading",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#data-loading",
    "title": "Spatial Weights and Applications",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Hunan shapefile into an R object.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nThe following code chunk imports the second data source, a csv file, into an R object using read_csv() of the readr package.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can examine the contents of the two objects by calling them.\n\nhunan sf dataframehunan2012 dataframe\n\n\n\nhunan\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3   ENGTYPE_3 Shape_Leng Shape_Area    County\n1   Changde 21098   Anxiang      County   1.869074 0.10056190   Anxiang\n2   Changde 21100   Hanshou      County   2.360691 0.19978745   Hanshou\n3   Changde 21101    Jinshi County City   1.425620 0.05302413    Jinshi\n4   Changde 21102        Li      County   3.474325 0.18908121        Li\n5   Changde 21103     Linli      County   2.289506 0.11450357     Linli\n6   Changde 21104    Shimen      County   4.171918 0.37194707    Shimen\n7  Changsha 21109   Liuyang County City   4.060579 0.46016789   Liuyang\n8  Changsha 21110 Ningxiang      County   3.323754 0.26614198 Ningxiang\n9  Changsha 21111 Wangcheng      County   2.292093 0.13049161 Wangcheng\n10 Chenzhou 21112     Anren      County   2.240739 0.13343936     Anren\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\nhunan2012\n\n# A tibble: 88 × 29\n   County    City   avg_wage deposite    FAI Gov_Rev Gov_Exp    GDP GDPPC    GIO\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Anhua     Yiyang    30544   10967   6832.    457.   2703  13225  14567  9277.\n 2 Anren     Chenz…    28058    4599.  6386.    221.   1455.  4941. 12761  4189.\n 3 Anxiang   Chang…    31935    5517.  3541     244.   1780. 12482  23667  5109.\n 4 Baojing   Hunan…    30843    2250   1005.    193.   1379.  4088. 14563  3624.\n 5 Chaling   Zhuzh…    31251    8241.  6508.    620.   1947  11585  20078  9158.\n 6 Changning Hengy…    28518   10860   7920     770.   2632. 19886  24418 37392 \n 7 Changsha  Chang…    54540   24332  33624    5350    7886. 88009  88656 51361 \n 8 Chengbu   Shaoy…    28597    2581.  1922.    161.   1192.  2570. 10132  1681.\n 9 Chenxi    Huaih…    33580    4990   5818.    460.   1724.  7755. 17026  6644.\n10 Cili      Zhang…    33099    8117.  4498.    500.   2306. 11378  18714  5843.\n# ℹ 78 more rows\n# ℹ 19 more variables: Loan &lt;dbl&gt;, NIPCR &lt;dbl&gt;, Bed &lt;dbl&gt;, Emp &lt;dbl&gt;,\n#   EmpR &lt;dbl&gt;, EmpRT &lt;dbl&gt;, Pri_Stu &lt;dbl&gt;, Sec_Stu &lt;dbl&gt;, Household &lt;dbl&gt;,\n#   Household_R &lt;dbl&gt;, NOIP &lt;dbl&gt;, Pop_R &lt;dbl&gt;, RSCG &lt;dbl&gt;, Pop_T &lt;dbl&gt;,\n#   Agri &lt;dbl&gt;, Service &lt;dbl&gt;, Disp_Inc &lt;dbl&gt;, RORP &lt;dbl&gt;, ROREmp &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#performing-relational-join",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#performing-relational-join",
    "title": "Spatial Weights and Applications",
    "section": "Performing relational join",
    "text": "Performing relational join\nThe code chunk below will be used to import columns from hunan2012 into hunan using left_join() of the dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#computing-queen-contiguity-based-neighbours",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#computing-queen-contiguity-based-neighbours",
    "title": "Spatial Weights and Applications",
    "section": "Computing (QUEEN) contiguity based neighbours",
    "text": "Computing (QUEEN) contiguity based neighbours\nThe code chunk below computes for a Queen contiguity weight matrix and displays a summary.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output shows that:\n\nThere are 88 units in the dataset.\nThe most connected unit has 11 neighbours (and only one unit has 11 neighbours)\nThere are two units with only one neighbour.\n\nThe resulting polygon object wm_q lists all neighboring polygons for each polygon. For example, the following code will show the neighbors of the first polygon:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nThis shows that there are 5 neighbors for the first polygon. The numbers denote the id of those neighbors as they are stored in hunan.\nWe can retrieve the names of those polygons or units using the code chunk below. The columns County and NAME_3 contain the same value so either may be used to return the names\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these countries using the code below (for polygon 1 and then for its five neighbours)\n\nhunan$GDPPC[1]\n\n[1] 23667\n\nhunan$GDPPC[wm_q[[1]]]\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe complete weight matrix can be displayed by using str(), i.e., str(wm_q)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#computing-rook-contiguity-based-neighbours",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#computing-rook-contiguity-based-neighbours",
    "title": "Spatial Weights and Applications",
    "section": "Computing (ROOK) contiguity based neighbours",
    "text": "Computing (ROOK) contiguity based neighbours\nThe code chunk below computes the Rook contiguity weight matrix by setting the queen argument to FALSE\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe report shows a few differences compared to the earlier QUEEN contiguity matrix. The most connected area has 10 instead of 11 neighbors, and there are differences in the details from the number of nonzero links to the average number of links."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#visualizing-contiguity-weights",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#visualizing-contiguity-weights",
    "title": "Spatial Weights and Applications",
    "section": "Visualizing contiguity weights",
    "text": "Visualizing contiguity weights\nIn this section, we introduce connectivity graphs which displays lines between neighboring points. As we are working with a polygon object at the moment, we would need to convert or define points to represent them first before attempting to build a connectivity graph. The most common method to do this is by choosing the centroid as the point for the polygon\n\nGetting longitude and latitude of polygon centroids\nThe process is slightly complicated as we cannot immediately simply run st_centroid() on the object.\nFirst, we need to get the coordinates of the polygons in separate dataframe by using a mapping function. The code chunk below create a dataframe for the centroids along the longitude by using st_centroid() on the geometry longitude using double bracket notation.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nFor the latitudes, we use a similar code with the only difference being the index referenced by the double bracket notation.\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nWe can then use cbind() to combine the two objects into a single object for the centroid locations.\n\ncoords &lt;- cbind(longitude, latitude)\n\nWe can confirm that the points are formatted correctly by checking the first few records with head()\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\nPlotting Queen contiguity based neighbours map\nThe code below creates the connectivity graph based on the matrix in wm_q\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n\n\n\n\nPlotting Rook contiguity based neighbours map\nThe code below creates the connectivity graph based on the matrix in wm_r\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nPlotting Queen and Rook contiguity based neighbours map\nThe code below creates the connectivity graph for both queen and rook based contiguity and show theem side by side\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#determining-the-cut-off-distance",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#determining-the-cut-off-distance",
    "title": "Spatial Weights and Applications",
    "section": "Determining the cut-off distance",
    "text": "Determining the cut-off distance\nThe first step is to determine the upper limit for the distance bands by using the following steps:\n\nUsing knearneigh() of spdep package to produce a matrix of the (indices) of the k-nearest neighbors (knn) of each unit\nUsing knn2nb() to convert the resulting knn object into a neighbors list of class nb with a list of integer vectors containing the neighbor region number ids\nUsing nbdists() to return the length of neighbor relationship edges. Note that this function returns in the same units if the source is projected. Otherwise, it uses km\nUsing unlist() to remove the list structure of the returned object\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary shows that the largest first nearest neighbor distance is 61.79km, so this is a good upper threshold that ensures that all units will have at least one neighbor"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#computing-fixed-distance-weight-matrix",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#computing-fixed-distance-weight-matrix",
    "title": "Spatial Weights and Applications",
    "section": "Computing fixed distance weight matrix",
    "text": "Computing fixed distance weight matrix\nWe use dnearneigh() in the code chunk below to compute the distance weight matrix\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nWe can use str() to display the contents of the wm_d62 weight matrix\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAlternatively, we can also display the matrix in another form using table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\nPlotting the fixed distance weight matrix\nThe code chunk below plots the distance weight matrix\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\nThe red lines show the links for first nearest neighbors while black ones show neighbors based on a cut-off distance of 62km. We can use the code chunk below to show these two set of links separately.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#computing-adaptive-distance-weight-matrix",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#computing-adaptive-distance-weight-matrix",
    "title": "Spatial Weights and Applications",
    "section": "Computing adaptive distance weight matrix",
    "text": "Computing adaptive distance weight matrix\nA fixed distance weight matrix will produce more neighbours for areas that are more densely packed compared to areas that are less densely packed.\nk-nearest neighbors can be used to control the number of neighbors directly. This is done using knn2nb() in the code below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nThis code guarantees that each unit has exactly six neighbors.\n\nPlotting distance based neighbors\nThe code chunk below plots the weight matrix based on knn,\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#spatial-lag-with-row-standardized-weights",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#spatial-lag-with-row-standardized-weights",
    "title": "Spatial Weights and Applications",
    "section": "Spatial lag with row-standardized weights",
    "text": "Spatial lag with row-standardized weights\nWe compute the average neighbor GDPPC for each unit using the code below. These are referred to as spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nIn one of the previous sections, we retrieved the GDPPC of the five neighboring counties of the first one using the code below\n\nnb1 &lt;- wm_q[[1]]\nnb1 # neighbors of county 1\n\n[1]  2  3  4 57 85\n\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\nmean(nb1)\n\n[1] 24847.2\n\n\nThe average of these corresponds to the first value in GDPPC.lag\nWe can append the GDPPC values into hunan using the code chunk below\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nThe code chunk below shows the average neighbor GDPPC for the first counties as the new added column (lag GDPPC)\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we can plot the individual and spatial lag GDPPC side by side to compare, using the code chunk below\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#spatial-lag-as-sum-of-neighboring-values",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#spatial-lag-as-sum-of-neighboring-values",
    "title": "Spatial Weights and Applications",
    "section": "Spatial lag as sum of neighboring values",
    "text": "Spatial lag as sum of neighboring values\nAnother approach is by assigning inary weights. This requires applying a function of assigning binary weights on the neighbor list using glist= in the nb2listw() function to assign weights\nWe start by assigning a value of 1 to each neighbor using lapply() which applies a function to each value in the object\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith weights assigned, we can then use lag.listw() to compute a lag variable from our weights and the GDDPPC\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nWe can examine the results using the code below\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nThe resulting variables are the sum of the neighboring counties’ GDPPC. For example, the first value 124236 is the same as:\n\nsum(nb1)\n\n[1] 124236\n\n\nWe can append these new values into hunan using left_join()\n\nhunan &lt;- left_join(hunan, lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nWe end this section by plotting the GDPPC and the new Spatial Lag Sum GDPPC values using qtm()similar to the previous section\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#spatial-window-average",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#spatial-window-average",
    "title": "Spatial Weights and Applications",
    "section": "Spatial window average",
    "text": "Spatial window average\nSpatial window average uses row=standardized weights and includes the diagonal element. This means we need to include the diagonal element in the neighbor list (i.e., include the current unit)\nWe can accomplish this by using include.self() from spdep\n\nwm_qs &lt;- include.self(wm_q)\n\nIf we inspect the first county using the code chunk below, we see that “1” or itself is now included in the list with previously five neighbors\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nWe then reobtain wights by using nb2listw() on this new matrix\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nWe use the next code chunk to create the lag variable using lag.listw()\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nThe code chunk below converts this into a dataframe object using as.data.frame() The code includes a relabeling of the columns as seen on the last line\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nWe append the computed average values into hunan using left_join() the code chunk below\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nWe can clearly compare the lag and spatial window averages using kable() of the knitr package.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nWe again end by using qtm() to compare the individual and the computed average GDPPC values\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#spatial-window-sum",
    "href": "Hands-on/Hands-On_Ex06/Hands-On_Ex06.html#spatial-window-sum",
    "title": "Spatial Weights and Applications",
    "section": "Spatial window sum",
    "text": "Spatial window sum\nSimilar to the other section, we can alternatively use a sum instead of a (weighted) average for aggregating the neighbor values\nWe again need to use the neighbor matrix with the added the diagonal element.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we assign binary weights to each neighbor (including itself in this case) using the code below\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nWe then use nb2listw() and glist() to explcitly assign weights\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWe can now compute the lag variable using lag.listw()\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nThe code below then converts this into a dataframe object using as.data.frame(), and then appends it to hunan using left_join()\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nWe can again compare the differently computed sum variables using kable() from knitr\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nFinally, we can use qtm() and tmap_arrange() to show the two different sum variables visually side by side.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we learn how to compute and interpret local measures of spatial autocorrelation or local indicators of spatial association (LISA) using the spdep package.\nThis exercise is based on Chapter 10 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#analytical-question",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Analytical Question",
    "text": "Analytical Question\nOne of the main development objective in spatial policy is for local governments and planners to ensure that there is equal distribution of development in the province. We then need to apply the appropriate spatial methods to verify if there is indeed even distribution of wealth geographically. If there is uneven distribution, then the next step is to identify if and where clusters are happening.\nWe continue studying the Hunan Province in China and focus on GDP per capita as the key indicator of development."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-sources",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-sources",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on the Hunan county coming from two files:\n\nHunan county boundary layer in ESRI shapefile format\nHunan local development indicators for 2012 stored in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#installing-and-launching-r-packages",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, tmap, and spdep.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspdep - functions to create spatial weights, autocorrelation statistics\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\nWe also define a random seed value for repeatability of any simulation results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-loading",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-loading",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Hunan shapefile into an R object.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe then use the code chunk below to load the csv file with the indicators into R using read_csv()\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-preparation",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-preparation",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#visualization-of-the-development-indicator",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#visualization-of-the-development-indicator",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package. We present these uas two maps using classes of equal intervals and equal quantiles.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-contiguity-spatial-weights",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-contiguity-spatial-weights",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nPrior to computing LISA’s, we need t construct spatial weights of the study area. Spatial weights are used to define the neighborhood relationship between units. (i.e., neighbors or adjacent units)\nThe code chunk below uses poly2nb() of the spdep package to compute contiguity weight matrices for the study area. The function builds a neighbor list based on regions with shared boundaries. The queen argument takes TRUE (default) or FALSE as options. This instructs the function if Queen criteria should be used in defining neighbors. For the code below, we use the Queen criteria to build the contiguity matrix\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output shows that there are 88 units in the hunan dataset, The most connected unit has 11 neighbors and two units have only one neighbor."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#row-standardised-weights-matrix",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#row-standardised-weights-matrix",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Row-standardised weights matrix",
    "text": "Row-standardised weights matrix\nThe next step is assigning weights to each neighbor. For our case, we assign equal weight (using style=\"W\") to each neighboring polygon. This assigns the fraction 1/n, where n is the number of neighbors a unit has, as the weight of each unit’s neighbor. The drawback of this approach is that polygons in the edge of the study area will base their value on a smaller number of neighbors. This means that we may be potentially over- or under-estimating the true nature of spatial autocorrelation. The alternative more robust style=\"B\" can address this.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-local-morans-i",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-local-morans-i",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Moran’s I",
    "text": "Computing Local Moran’s I\nWe use localmoran() of the spdep package to compute for the local Moran’s I statistic. The function computes for a set of Ii values based on a set of zi values and a listw object which provides the neighbor weight information.\nThe code chunk below computes for the local Moran’s I of the GDPPC variable.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nThe function returns a matrix with the following columns:\n\nIi - the local Moran’s statistic\nE.Ii - the expected value of the statistic under randomisation hypothesis\nV.Ii - the variance of the statistic under randomisation hypothesis\nZ.Ii - the standard deviate of the statistic\nPr(z != E(Ii)) - the p-value of the local Moran statistic\n\nThe code chunk below displays the content of the local Moran matrix by using printCoefmat()\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-the-local-morans-i",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-the-local-morans-i",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Mapping the local Moran’s I",
    "text": "Mapping the local Moran’s I\nBefore mapping, we append the local Moran’s I dataframe to the hunan SpatialPolygonDataFrame using the code chunk below.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nThe code chunk below uses the tmap package to plot the local Moran’s I statistic values.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe map shows evidence of positive and negative Ii values. It is good to consider the p-values for these regions. We use the code chunk below to plot the p-values\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nFor better interpretation, we should consider having these two maps side by side like in the code below.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-moran-scatterplot",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-moran-scatterplot",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting the Moran Scatterplot",
    "text": "Plotting the Moran Scatterplot\nThe Moran scatterplot illustrates the relationship between the value of a chosen attribute against the average of that value across neighbors. The code chunk below uses moran.plot() of spdep package to produce the Moran scatterplot of GDPPC\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\nThe chart is split into quadrants based on the region’s GDPPC and their neighbors’ average or their lagged GDPPC."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-moran-scatterplot-with-standardised-variables",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-moran-scatterplot-with-standardised-variables",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting Moran scatterplot with standardised variables",
    "text": "Plotting Moran scatterplot with standardised variables\nWe can use scale() to center and scale the variable as in the code chunk below. The final function in the code, as.vector(), ensures that we get a vector out of this transformation, that we can then map into our target dataframe.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nWe can then rerun the scatterplot with standardised variables using the code chunk below\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#preparing-lisa-map-classes",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#preparing-lisa-map-classes",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Preparing LISA map classes",
    "text": "Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nWe then derive the spatially lagged variable of interest, GDPPC, and center it around its mean by using the code below.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nWe then center the local Moran’s statistics around the mean\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nWe set a 5% statistical significance level for the local Moran\n\nsignif &lt;- 0.05       \n\nThe code chunk below defines the four different quadrants or categories\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, the code chunk below places the non-significant Moran in category 0 (zero)\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nThe previous steps can be rewritten into a single code chunk below\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-lisa-map",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-lisa-map",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting the LISA map",
    "text": "Plotting the LISA map\nThe code chunk below builds the LISA map using tmap package\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nFor better and more effective interpretation, we can again plot the LISA map and the original GDPPC values side by side using the code chunk below\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#getis-and-ords-g-statistics",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#getis-and-ords-g-statistics",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Getis and Ord’s G-statistics",
    "text": "Getis and Ord’s G-statistics\nThe Getis and Ord’s G-statistics looks at neighbors based on a defined proximity to identify high (hot spots) or low (cold spots) value clusters.\nThe analysis consists of three steps:\n\nDeriving a spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#deriving-distance-based-weight-matrix",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#deriving-distance-based-weight-matrix",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Deriving distance-based weight matrix",
    "text": "Deriving distance-based weight matrix\nFor Getis-Ord, we need to define neighbors based on distance, which can be done by using fixed distance weights or adaptive distance.\n\nDeriving the Centroid\nWe need to define the centroids of each polygon. This consists of multiple steps as we cannot directly use st_centroid() directly on our object for our problem.\nWe first get the longitude values and then map the st_centroid() function on them.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for the latitude values using the code chunk below\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nWith the centroid for the longitude and latitude calculated, we can bind them into a single object using cbind()\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\nDetermining the cut-off distance\nWe then determine the upper limit for the distance bands using the following steps\n\nCreate a matrix with indices of points belonging to k-nearest neighbors using knearneigh() of the spdep package\nConvert the matrix into a neighbors list of class nb by using knn2nb()\nReturn the length of neighbor relationship edges by using nbdists()\nRemove the list structure of the returned object using unlist()\n\nThe code chunk below executes these steps\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\nComputing fixed-distance weight matrix\nWe then compute the distance matrix using dnearneigh() in the code chunk below\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nWe then convert th nb object into a spatial weights object using nb2listw() in the chunk below\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\nComputing adaptive distance matrix\nFixed distance weight matrices will result to units in densely packed areas having more neighbors than less densely packed areas.\nWe can control the numbers of neighbors directly using knn, either accepting asymmetric neighbors or imposing symmetry using the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nWe then convert the nb object into a spatial weights object using the code below\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-fixed-distance",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-fixed-distance",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Gi statistics using fixed distance",
    "text": "Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or G* values.\nThe Gi statistics are represented as Z-scores. Greater values represent grester clustering intensity while the sign indicates the high (positive) or low (negative) clusters.\nWe then join the Gi values with the corresponding units in hunan using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-gi-values-with-fixed-distance-weights",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-gi-values-with-fixed-distance-weights",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Mapping Gi values with fixed distance weights",
    "text": "Mapping Gi values with fixed distance weights\nThe code below maps the Gi values using a fixed distance weight matrix\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap_fix &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap_fix, asp=1, ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-adaptive-distance",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-adaptive-distance",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Gi statistics using adaptive distance",
    "text": "Gi statistics using adaptive distance\nThe code chunk below computes the Gi values for GDPPC by suing an adaptive distance matrix\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\nWe then visualize this (also beside the original GDPPC values) using the code below.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap_ad &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap_ad, \n             asp=1, \n             ncol=2)\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWe show the fixed and adaptive maps side-by-side using the chunk below\n\ntmap_arrange(Gimap_fix,Gimap_ad, asp=1,ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html",
    "title": "Geographically Weighted Regression Model",
    "section": "",
    "text": "In this hands-on exercise, we learn to use GWR or geographically weighted regression. GWR is a technique that takes non-stationary variables and models their relationships to an outcome of interest. We use GWR to build hedonic pricing models for the resale prices of condominiums in Singapore. (from 2015)\nThis exercise is based on Chapter 13 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#data-sources",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#data-sources",
    "title": "Geographically Weighted Regression Model",
    "section": "Data Sources",
    "text": "Data Sources\nTo datasets will be used for this exercise:\n\n2014 Master Plan subzone boundary in shapefile format\n2015 condo resale prices in csv format"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#installing-and-launching-r-packages",
    "title": "Geographically Weighted Regression Model",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of the following R packages:\n\nolsrr - for building OLS (ordinary least squares) regression models and performing diagnostic tests\nGWmodel - for calibrating geographically weighted family of models\ntmap - for plotting cartographic quality maps\ncorrplot - for multivariate data visualization and analysis\nsf - spatial data handling\ntidyverse - attribute data handling\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#geospatial-data-loading-and-preparation",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#geospatial-data-loading-and-preparation",
    "title": "Geographically Weighted Regression Model",
    "section": "Geospatial data loading and preparation",
    "text": "Geospatial data loading and preparation\nThe code chunk below uses st_read() of the sf package to load the geospatial data. (master plan boundaries) This data is in svy21 projected coordinate systems.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex10\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nAs the new object does not have EPSG information, we will use the following code with st_transform() to apply the correct code of 3414.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\nmpsz_svy21 &lt;- st_make_valid(mpsz_svy21)\n\nWe can use st_crs() to verify that the operation was successful.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nWe can use st_bbox() to reveal the limits of the bounding box or the extent of the sf object.\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#aspatial-data-loading",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#aspatial-data-loading",
    "title": "Geographically Weighted Regression Model",
    "section": "Aspatial data loading",
    "text": "Aspatial data loading\nThe code chunk below uses read_csv() of readr to import the 2015 condo resale prices from the csv file.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can verify that the load is successful and get an idea of the data structure by using a function like glimpse()\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nWe can use head() to inspect the first few (default 6) elements. We can use it for select columns/fields, as we do in the next code chunk for longitude and latitude.\n\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nWe can use summary() of base R to display summary statistics across columns in the same dataframe.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#converting-aspatial-dataframe-into-an-sf-object",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#converting-aspatial-dataframe-into-an-sf-object",
    "title": "Geographically Weighted Regression Model",
    "section": "Converting aspatial dataframe into an sf object",
    "text": "Converting aspatial dataframe into an sf object\nTo convert the condo_resale object into a spatial object, we can use the following code chunk that utilizes st_as_sf() from sf package. The final line of the code chunk converts the data frame from wgs84 to svy21 using the indicated crs values.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nWe can again use head() to inspect the first few elements of the new object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#eda-using-statistical-graphics",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#eda-using-statistical-graphics",
    "title": "Geographically Weighted Regression Model",
    "section": "EDA using statistical graphics",
    "text": "EDA using statistical graphics\nWe can produce a histogram of the selling price by using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Distribution of Resale Selling Price\") +\n  labs(x = \"Selling Price\", y = \"Records\")\n\n\n\n\n\n\n\n\nThe figure shows a right-skewed distribution for price– that more units were sold at lower prices.\nSkewed distributions are undesirable for modeling variables but can be solved through methods like log transformation. The code chunk below creates a new variable which is the log transformation of the original selling price variable. It utilizes the function log() to perform this.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nWe can now replot the transformed variable in a similar method using ggplot.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Distribution of log of Resale Selling Price\") +\n  labs(x = \"log(Selling Price)\", y = \"Records\")\n\n\n\n\n\n\n\n\nThe new variable has less skewness compared to the original one."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#multiple-histogram-plots-of-variables",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#multiple-histogram-plots-of-variables",
    "title": "Geographically Weighted Regression Model",
    "section": "Multiple histogram plots of variables",
    "text": "Multiple histogram plots of variables\nWe will use ggarrange() of the ggpubr package to produce small multiple histograms or trellis plots.\nThe code chunk below uses ggarrange() to produce 12 small histograms arranged in columns of 4 rows.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#drawing-statistical-point-map",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#drawing-statistical-point-map",
    "title": "Geographically Weighted Regression Model",
    "section": "Drawing statistical point map",
    "text": "Drawing statistical point map\nWe can show the geospatial distribution of resale prices using the tmap package.\nThe code chunk below produces an interactive map (by toggling with tmap_mode(\"view\")) of the selling price. The set.zoom.limits argument of tm_view() constrains the minimum and the maximum zoom levels. The code chunk ends by turning interactive mode off to ensure that there is no active connection.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#simple-linear-regression-method",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#simple-linear-regression-method",
    "title": "Geographically Weighted Regression Model",
    "section": "Simple linear regression method",
    "text": "Simple linear regression method\nWe build a simple linear regression model by using SELLING_PRICE as the dependent variable and then AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nNote that lm() returns an lm object (or c(mlm, lm) for multiple responses)\nThe summary and output can be obtained by using summary() and anova() functions.\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output includes the estimate of the best fit line based on the coefficients table displayed. In this case it is:\n\\[\nSELLINGPRICE = -258181.1 + 1.4719 (AREA)\n\\]\nThe R-squared value of 0.4518 states that the model is able to explain 45% of the values of the selling/resale price.\nThe p-value of less than 0.01 indicates that the regression model is a good estimator of the resale price.\nTo visualize the best fit line graphically, we can produce the scatterplot and then incorporate lm() function for the smoothed line in ggplot as below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  ggtitle(\"Fl0or area vs Resale Price\") +\n  labs(x = \"Floor Area\", y = \"Resale Price\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#multiple-linear-regression-method",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#multiple-linear-regression-method",
    "title": "Geographically Weighted Regression Model",
    "section": "Multiple linear regression method",
    "text": "Multiple linear regression method\n\nVisualizing the relationship of the independent variables\nBefore building a multiple LM model, it is important to ensure that the independent variables used are not highly correlated with each other. A correlation matrix is commonly used to visually inspect the relationships between these variables.\nThe pairs() function of R as well as other packages can be used. For this section, we will use the corrplot package.\nThe code chunk below uses corrplot() from that package to show the correlation coefficient between every pair of independent variable.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\n\n\n\n\nMatrix reorder, controlled by the order argument, is important to uncover hidden structures or patterns. There are four methods available: AOE, FPC, hclust and alphabet. AOE is used in the code above and uses the angular order of the eigenvectors method.\nInspecting the output above, it is clear that FREEHOLD is highly correlated with LEASE_99YEAR– so it is best to only include one of these. For our model, we will just keep the first variable.\n\n\nBuilding a hedonic pricing model using multiple linear regression method\nThe code chunk below uses lm() to calbrate a multiple linear regression model. It also produces the summary of the model using summary()\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nPreparing publication quality table using olsrr\nWith reference to the results above, it is clear that some of the variables are not statistically significant. We revise the model to exclude such variables and then produce the summary using ols_regress().\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.591 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\nPreparing publication quality table using gtsummary\nThe gtsummary package provides an alternative way to produce publication-grade summaries in R.\nThe code chunk below uses tbl_regression() to create a formatted regression report.\n\ngtsummary::tbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWith the gtsummary package, model statistics can also be added by appending them to the output using add_glance_table() or as a source not by using add_glance_source_note() as in the code chunk below.\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nChecking for multicolllinearity\nIn the code chunk below, we use ols_vif_tol() of olsrr package to check for signs of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nAs the VIF of each of the independent variables is less than 10, we can safely assume that there is no multicollinearity in our model.\n\n\nTesting for non-linearity\nWhen performing multiple linear regression, we need to check whether the assumptions of linearity and additivity are not violated.\nFor linearity, we use the ols_plot_resid_fit() of olsrr package in the code chunk below.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\n\n\n\nAs the residuals / points lie around the zero line, we have confidence that the linearity assumption is not violated.\n\n\nTest for normality\nThe code chunk below uses ols_plot_resid_hist() of olsrr package to check for normality.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\n\n\n\nThe output reveals that the residuals follow a normal distribution.\nThe oslrr package can also perform regular statistical tests for normality and display in a tabular format using ols_test_normality()\n\nols_test_normality(condo.mlr1)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the one-sample Kolmogorov-Smirnov test\n\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\n\n\nTesting for spatial autocorrelation\nTo test for spatial autocorrelation, we need to convert the resell prices sf data frame into a SpatialPointsDataFrame.\nWe first need to export the residuals of the regression model and save it as a dataframe.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nWe then include this as a new field in the condo_resale.sf object by using the code chunk below\n\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf,\n                             condo.mlr1$residuals) %&gt;%\n  rename(`MLR_RES` = `condo.mlr1.residuals`)\n\nWe then use the code chunk below to convert the object into SpatialPointsDataFrame format to be able to use spdep package functions on it.\n\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nThe code chunk below creates an interactive map using tmap to visualize the data.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nThe map reveals no clear signs of autocorrelation as there are no clear clusters with high or low residual values.\nTo verify this conclusion, we can perform Moran’s I test.\nFirst, we generate the distance-based weight matrix by using dnearneigh() of spdep package.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, we use nb2listw() to convert the neighbours into spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, we use lm.morantest() of spdep package to perform Moran’s I test for the residual spatial autocorrelation.\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nAs the p-value is less than our level of confidence α = 0.05, we reject the hypothesis of spatial randomness. As the test statistic I is positive, we infer that the residuals exhibit clustering."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#building-fixed-bandwidth-gwr-model",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#building-fixed-bandwidth-gwr-model",
    "title": "Geographically Weighted Regression Model",
    "section": "Building fixed bandwidth GWR model",
    "text": "Building fixed bandwidth GWR model\n\nComputing fixed bandwidth\nIn the code chunk below, we use bw.gwr() of the GWR package to determine an optimal fixed bandwidth. The adaptive=\"FALSE\" argument value indicates that we are computing for a fixed bandwidth.\nWe use the approach argument to define the stopping rule which can either be \"CV\" or cross-validation approach, or \"AICc\" or AIC corrected approach. We use the former in the code chunk.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe output shows that the recommended bandwidth is 971.3405 (meters)\n\n\nGWModel method - fixed bandwidth\nWe can calibrate the gwr model using fixed bandwidth and a gaussian kernel using the code chunk below.\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe object contains the output and is in class gwrm. Calling the object displays the model output.\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-14 16:05:46.792674 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2024-10-14 16:05:47.512285 \n\n\nThe report shows that the AICc of the gwr is signigicantly smaller than that of the global multiple lm (42263.61 &lt; 42967.1)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#building-adaptive-bandwidth-gwr-model",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#building-adaptive-bandwidth-gwr-model",
    "title": "Geographically Weighted Regression Model",
    "section": "Building adaptive bandwidth GWR model",
    "text": "Building adaptive bandwidth GWR model\n\nComputing adaptive bandwidth\nWe again use bw.gwr() of the GWR package to determine the bandwidth. This time adaptive=\"TRUE\" argument value indicates that we are computing for an adaptive bandwidth.\n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe output shows that 30 is the recommended data points to be used.\n\n\nGWModel method - adaptive bandwidth\nWe can calibrate the gwr model using adaptive bandwidth and a gaussian kernel using the code chunk below.\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nThe object contains the output and is in class gwrm. Calling the object displays the model output.\n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-14 16:05:52.861739 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-10-14 16:05:53.74023"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#visualizing-gwr-output",
    "href": "Hands-on/Hands-On_Ex10/Hands-On_Ex10.html#visualizing-gwr-output",
    "title": "Geographically Weighted Regression Model",
    "section": "Visualizing GWR Output",
    "text": "Visualizing GWR Output\nThe output table includes various fields aside from the residuals and are all stored in the SpatialPointsDataFrame or SpatialPolygonsDataFrame object in an object called SDF.\n\nConverting SDF into SF dataframe\nTo visualize the fields in SDF, we first convert it into an sf dataframe using the code chunks below\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\n\ncondo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414)\n\n\ngwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nWe then use glimpse() to check the contents of the last object.\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       &lt;dbl&gt; 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               &lt;dbl&gt; 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               &lt;dbl&gt; 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\nThere are 77 fields that are included in the dataframe. We can use summary() to check the statistics of the yhat field as below.\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\nVisualizing local R2\nThe code chunk below is used to create an interactive point symbol map based on the Local_R2 values.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\n\n\nVisualizing coefficient estimates\nThe code chunk below creates side-by-side interactive map of the standard error and t-value of the AREA_SQM variable.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nWe can also focus on a particular region like the central region and show the R2 values using the code chunk below\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/data/geospatial/ELDERCARE.html",
    "href": "Hands-on/Hands-On_Ex12/data/geospatial/ELDERCARE.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  ELDERCARE  ENG dataset\n\nELDERCARE\n\n                 0 0     false"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html",
    "title": "Modelling Geographic Accessibility",
    "section": "",
    "text": "In this hands-on exercise, we model geographic accessibility using R.\nThis exercise is based on Chapter 17 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#data-sources",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#data-sources",
    "title": "Modelling Geographic Accessibility",
    "section": "Data Sources",
    "text": "Data Sources\nThe data for this exercise comes in the form of four files:\n\n2014 Master Plan Planning subzone boundary in shapefile format sourced from data.gov.sg\nSingapore GIS data with hexagons of 250m radius generated by using st_make_grid() of the sf package\nLocation of eldercare centres sourced from data.gov.sg and in shapefile format\nA distance matrix in csv format from the hexagons to the eldercare centres. The data also contains fields for the entry, exit and network costs which give the distance between roads and the hexagon, between roads and eldercare centres, and between the network points of the hexagon and eldercare centre.\n\nAside from the first dataset, the balance are already processed datasets c/o Prof Kam to be used by his students for this exercise."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#installing-and-launching-r-packages",
    "title": "Modelling Geographic Accessibility",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of six R packages. For this exercise, we will use spatialAcc which is used to model geographic accessibility.\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(tmap, SpatialAcc, sf, \n               ggstatsplot, reshape2,\n               tidyverse)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#importing-geospatial-data",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#importing-geospatial-data",
    "title": "Modelling Geographic Accessibility",
    "section": "Importing geospatial data",
    "text": "Importing geospatial data\nWe use st_read() of sf package to load the three geospatial datasets into R.\n\nLoading Planning SubzoneLoading HexagonsLoading Eldercare Centre Location\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\nReading layer `MP14_SUBZONE_NO_SEA_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex12\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\nhexagons &lt;- st_read(dsn = \"data/geospatial\", layer = \"hexagons\") \n\nReading layer `hexagons' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex12\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3125 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21506.33 xmax: 50010.26 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\n\neldercare &lt;- st_read(dsn = \"data/geospatial\", layer = \"ELDERCARE\") \n\nReading layer `ELDERCARE' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex12\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 120 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14481.92 ymin: 28218.43 xmax: 41665.14 ymax: 46804.9\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\n\nThe outputs show that all the objects are in sf format. The object mpsz is both in multipolygon class and currently does not have EPSG information."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#updating-crs-information",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#updating-crs-information",
    "title": "Modelling Geographic Accessibility",
    "section": "Updating CRS information",
    "text": "Updating CRS information\nThe code chunk below assigns and ensures that all objects have the same EPSG code of 3414.\n\nmpsz &lt;- st_transform(mpsz, 3414)\neldercare &lt;- st_transform(eldercare, 3414)\nhexagons &lt;- st_transform(hexagons, 3414)\n\nWe can then use st_crs() of sf package to confirm the CRS information of an object.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#cleaning-and-updating-attribute-fields-of-the-geospatial-data",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#cleaning-and-updating-attribute-fields-of-the-geospatial-data",
    "title": "Modelling Geographic Accessibility",
    "section": "Cleaning and updating attribute fields of the geospatial data",
    "text": "Cleaning and updating attribute fields of the geospatial data\nIf we inspect the last two objects, we see that there are a number of redundant or unnecessary fields. We use the code chunk below to only keep the necessary ones using select(). We also add a new field called capacity to both objects using mutate().\n\neldercare &lt;- eldercare %&gt;%\n  select(fid, ADDRESSPOS) %&gt;%\n  mutate(capacity = 100)\n\nhexagons &lt;- hexagons %&gt;%\n  select(fid) %&gt;%\n  mutate(demand = 100)\n\nFor this exercise we use a dummy value of 100 for the capacity, but, in practice, this number needs to be updated with the actual capacity of the location."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#importing-the-distance-matrix",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#importing-the-distance-matrix",
    "title": "Modelling Geographic Accessibility",
    "section": "Importing the Distance Matrix",
    "text": "Importing the Distance Matrix\nThe code chunk below uses read_csv() to load the distance matrix into R\n\nODMatrix &lt;- read_csv(\"data/aspatial/OD_Matrix.csv\", skip = 0)\n\nRows: 375000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): origin_id, destination_id, entry_cost, network_cost, exit_cost, tot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can inspect the object using head()\n\nhead(ODMatrix)\n\n# A tibble: 6 × 6\n  origin_id destination_id entry_cost network_cost exit_cost total_cost\n      &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1         1              1       668.       19847.      47.6     20562.\n2         1              2       668.       45027.      31.9     45727.\n3         1              3       668.       17644.     173.      18486.\n4         1              4       668.       36010.      92.2     36770.\n5         1              5       668.       31068.      64.6     31801.\n6         1              6       668.       31195.     117.      31980."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#tidying-the-distance-matrix",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#tidying-the-distance-matrix",
    "title": "Modelling Geographic Accessibility",
    "section": "Tidying the distance matrix",
    "text": "Tidying the distance matrix\nBased on our inspection, we see that the distance matrix is not yet in the typical matrix format where the origin is on one axis and the destination is on another. Instead, the origins and the destinations are in their respective columns and there are fields for the distances.\nIn order to transform the object into matrix format, we use spread() of tidyr in the code chunk below. pivot_wider() can also be used to achieve the same result\n\ndistmat &lt;- ODMatrix %&gt;%\n  select(origin_id, destination_id, total_cost) %&gt;%\n  spread(destination_id, total_cost)%&gt;%\n  select(c(-c('origin_id')))\n\n\nclass(distmat)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe matrix is still in dataframe format and is in meters. We use the code below to convert it to km and turn it into a matrix.\n\ndistmat_km &lt;- as.matrix(distmat/1000)\n\n\nclass(distmat_km)\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#computing-hansen-accessibility",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#computing-hansen-accessibility",
    "title": "Modelling Geographic Accessibility",
    "section": "Computing Hansen Accessibility",
    "text": "Computing Hansen Accessibility\nTo compute for the Hansen accessibility we use ac() of the SpatialAcc package. The code chunk below does this and also uses data.frame() to convert the output into dataframe format.\n\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\n\nglimpse(acc_Hansen)\n\nRows: 3,125\nColumns: 1\n$ ac.hexagons.demand..eldercare.capacity..distmat_km..power...2.. &lt;dbl&gt; 1.6483…\n\n\nWe see that the function returns one field and its default field name is a unreadable. We can fix this by assigning a new name to colnames()\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"\n\n\nglimpse(acc_Hansen)\n\nRows: 3,125\nColumns: 1\n$ accHansen &lt;dbl&gt; 1.648313e-14, 1.096143e-16, 3.865857e-17, 1.482856e-17, 1.05…\n\n\nWe then convert the accessibility measure to tibble format and then bind it with the hexagon data frame using the two lines of code in the following code chunk\n\nacc_Hansen &lt;- tibble::as_tibble(acc_Hansen)\n\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#visualising-hansens-accessibility",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#visualising-hansens-accessibility",
    "title": "Modelling Geographic Accessibility",
    "section": "Visualising Hansen’s Accessibility",
    "text": "Visualising Hansen’s Accessibility\nWe first extract the extent of the hexagons object using st_bbox() of sf package\n\nmapex &lt;- st_bbox(hexagons)\n\nWe use tmap package in the code chunk below to create a visualization of the accessibility of eldercare centres across Singapore using Hansen method.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#statistical-graphical-representation",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#statistical-graphical-representation",
    "title": "Modelling Geographic Accessibility",
    "section": "Statistical graphical representation",
    "text": "Statistical graphical representation\nWe can compare the distribution of the accessibility values (using Hansen method) across planning regions.\nFirst, we need to include the planning region field into hexagon_Hansen object by using st_join() in the code chunk below.\n\nhexagon_Hansen &lt;- st_join(hexagon_Hansen, mpsz, \n                          join = st_intersects)\n\nWe then use ggplot() to produce a box plot of the accessibility value by planning region.\n\nggplot(data=hexagon_Hansen, \n       aes(y = log(accHansen), \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#computing-kd2sfca-accessibility",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#computing-kd2sfca-accessibility",
    "title": "Modelling Geographic Accessibility",
    "section": "Computing KD2SFCA Accessibility",
    "text": "Computing KD2SFCA Accessibility\nTo compute for the KD2SFCA accessibility we use ac() of the SpatialAcc package. The code chunk below does this and also uses data.frame() to convert the output into dataframe format. Compared to the previous section, we use a different value for the family argument.\n\nacc_KD2SFCA &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            d0 = 50,\n                            power = 2, \n                            family = \"KD2SFCA\"))\n\ncolnames(acc_KD2SFCA) &lt;- \"accKD2SFCA\"\nacc_KD2SFCA &lt;- tibble::as_tibble(acc_KD2SFCA)\nhexagon_KD2SFCA &lt;- bind_cols(hexagons, acc_KD2SFCA)\n\n\nglimpse(acc_KD2SFCA)\n\nRows: 3,125\nColumns: 1\n$ accKD2SFCA &lt;dbl&gt; 1.745751e-149, 1.849596e-192, 4.442757e-202, 3.938161e-211,…\n\n\nThe object already includes updating the measure’s column name and binds the measure values to the hexagon sf dataframe."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#visualising-kd2sfcas-accessibility",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#visualising-kd2sfcas-accessibility",
    "title": "Modelling Geographic Accessibility",
    "section": "Visualising KD2SFCA’s Accessibility",
    "text": "Visualising KD2SFCA’s Accessibility\nWe again use tmap package in the code chunk below to create a visualization of the accessibility of eldercare centres across Singapore using KD2SFCA method.Note that we reuse the mapex object from the previous section.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hexagon_KD2SFCA,\n         bbox = mapex) + \n  tm_fill(col = \"accKD2SFCA\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: KD2SFCA method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#statistical-graphical-representation-1",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#statistical-graphical-representation-1",
    "title": "Modelling Geographic Accessibility",
    "section": "Statistical graphical representation",
    "text": "Statistical graphical representation\nWe can compare the distribution of the accessibility values (using Hansen method) across planning regions.\nFirst, we need to include the planning region field into the last object by using st_join() in the code chunk below.\n\nhexagon_KD2SFCA &lt;- st_join(hexagon_KD2SFCA, mpsz, \n                          join = st_intersects)\n\nWe then use ggplot() to produce a box plot of the accessibility value by planning region.\n\nggplot(data=hexagon_KD2SFCA, \n       aes(y = accKD2SFCA, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#computing-sams-accessibility",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#computing-sams-accessibility",
    "title": "Modelling Geographic Accessibility",
    "section": "Computing SAM’s Accessibility",
    "text": "Computing SAM’s Accessibility\nTo compute for the SAM accessibility we use ac() of the SpatialAcc package. The code chunk below does this and also uses data.frame() to convert the output into dataframe format. Compared to the previous section, we use a different value for the family argument.\n\nacc_SAM &lt;- data.frame(ac(hexagons$demand,\n                         eldercare$capacity,\n                         distmat_km, \n                         d0 = 50,\n                         power = 2, \n                         family = \"SAM\"))\n\ncolnames(acc_SAM) &lt;- \"accSAM\"\nacc_SAM &lt;- tibble::as_tibble(acc_SAM)\nhexagon_SAM &lt;- bind_cols(hexagons, acc_SAM)\n\n\nglimpse(acc_SAM)\n\nRows: 3,125\nColumns: 1\n$ accSAM &lt;dbl&gt; 0.11941792, 0.10101549, 0.09775876, 0.09490965, 0.09391923, 0.0…\n\n\nThe object already includes updating the measure’s column name and binds the measure values to the hexagon sf dataframe."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#visualising-sams-accessibility",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#visualising-sams-accessibility",
    "title": "Modelling Geographic Accessibility",
    "section": "Visualising SAM’s Accessibility",
    "text": "Visualising SAM’s Accessibility\nWe again use tmap package in the code chunk below to create a visualization of the accessibility of eldercare centres across Singapore using SAM method. Note that we reuse the mapex object from the previous section.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hexagon_SAM,\n         bbox = mapex) + \n  tm_fill(col = \"accSAM\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: SAM method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 3),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#statistical-graphical-representation-2",
    "href": "Hands-on/Hands-On_Ex12/Hands-On_Ex12.html#statistical-graphical-representation-2",
    "title": "Modelling Geographic Accessibility",
    "section": "Statistical graphical representation",
    "text": "Statistical graphical representation\nWe can compare the distribution of the accessibility values (using Hansen method) across planning regions.\nFirst, we need to include the planning region field into the last object by using st_join() in the code chunk below.\n\nhexagon_SAM &lt;- st_join(hexagon_SAM, mpsz, \n                       join = st_intersects)\n\nWe then use ggplot() to produce a box plot of the accessibility value by planning region.\n\nggplot(data=hexagon_SAM, \n       aes(y = accSAM, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n\n\nThe boxes are not clear due to the outliers. Note that we can hide outliers in ggplot boxplots using the outlier.shape argument. In addition, the y-axis needs to be reset using coord_cartesian() as removing the outliers does not aautomatically adjust the axis.\n\nggplot(data=hexagon_SAM, \n       aes(y = accSAM, \n           x= REGION_N)) +\n  geom_boxplot(outlier.shape = NA) +\n  coord_cartesian(ylim = c(0,8)) + \n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html",
    "title": "Spatial Interaction Models",
    "section": "",
    "text": "In this hands-on exercise, we apply functions for modeling spatial interaction using R. We cover both the processing and visualizing of flow data and the calibration of spatial interaction models in this exercise.\nThis exercise is based on Chapters 15 and 16 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#data-sources",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#data-sources",
    "title": "Spatial Interaction Models",
    "section": "Data Sources",
    "text": "Data Sources\nFor this exercise, we will be using the following two sources:\n\nPassenger volume by origin and destination bus stops from the LTA Data Mall\nBus stop locations based on data from the last quarter of 2022\nURA Masterplan 2019 Planning Subzone boundary which is already converted into sf dataframe format and saved in an rds file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#installing-and-launching-r-packages",
    "title": "Spatial Interaction Models",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of nine R packages.\n\nsf - for importing and processing geospatial data\ntidyverse - for data importing and wrangling\ntmap - for creating thematic maps\nstplanr - for solving common problems in transport planning\nDT - provides an R interface for JavaScript linrary DataTables\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(tmap, sf, sp,\n               performance, reshape2,\n               ggpubr, tidyverse,\n               DT, stplanr)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#preparing-the-flow-data",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#preparing-the-flow-data",
    "title": "Spatial Interaction Models",
    "section": "Preparing the Flow Data",
    "text": "Preparing the Flow Data\n\nImporting the OD Data\nWe first load the passenger volume data by using read_csv() of readr package.\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202210.csv\")\n\nRows: 5122925 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): YEAR_MONTH, DAY_TYPE, PT_TYPE\ndbl (4): TIME_PER_HOUR, ORIGIN_PT_CODE, DESTINATION_PT_CODE, TOTAL_TRIPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can use glimpse() to inspect the contents of the odbus object.\n\nglimpse(odbus)\n\nRows: 5,122,925\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2022-10\", \"2022-10\", \"2022-10\", \"2022-10\", \"2022-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 10, 10, 7, 11, 16, 16, 20, 7, 7, 11, 11, 8, 11, 11…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;dbl&gt; 65239, 65239, 23519, 52509, 54349, 54349, 43371, 8…\n$ DESTINATION_PT_CODE &lt;dbl&gt; 65159, 65159, 23311, 42041, 53241, 53241, 14139, 9…\n$ TOTAL_TRIPS         &lt;dbl&gt; 2, 1, 2, 1, 1, 4, 1, 3, 1, 5, 2, 5, 15, 40, 1, 1, …\n\n\nThe origin and destination codes are imported as numeric data type. We use the following code chunk to convert them into characters or categorical type.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\nExtracting the study data\nFor this study, we focus on the activity on weekdays between 6pm and 9pm. The code chunk below extracts the relevant data based on that.\n\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n`summarise()` has grouped output by 'ORIGIN_PT_CODE'. You can override using\nthe `.groups` argument.\n\n\nWe can display the contents as a table using the following code which uses the DT package.\n\ndatatable(odbus6_9)\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\nWe can save the output for future use using the code chunk below\n\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\n\nThe following code chunk reloads the same data.\n\nodbus6_9 &lt;- read_rds(\"chap15/data/rds/odbus6_9.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#importing-geospatial-data",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#importing-geospatial-data",
    "title": "Spatial Interaction Models",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\nWe use the code chunk below to load the bus stop locations into R.\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex13\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5159 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nThe following code chunk loads the masterplan subzone boudaries\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex13\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#geospatial-data-wrangling",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#geospatial-data-wrangling",
    "title": "Spatial Interaction Models",
    "section": "Geospatial Data wrangling",
    "text": "Geospatial Data wrangling\nThe code chunk below combines the busstop and mpsz data by populating the planning subzone code into the busstop object.\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\ndatatable(busstop_mpsz)\n\n\n\n\n\nWe can save the output into an rds file to save our work up to this point.\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \n\nNext, we use the code chunk below to append the planning subzone code of the origin onto odbus6_9\n\nod_data &lt;- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\nWarning in left_join(odbus6_9, busstop_mpsz, by = c(ORIGIN_PT_CODE = \"BUS_STOP_N\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 55491 of `x` matches multiple rows in `y`.\nℹ Row 161 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nWe check if there are any duplicated records using the code chunk below\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\ndatatable(duplicate)\n\n\n\n\n\nWe see that there are quite a number of duplicated records. We use the code chunk below to remove duplicates\n\nod_data &lt;- unique(od_data)\n\nWe then update the object with the planning subzone code of the destination using the code chunk below and then remove any duplicates\n\nod_data &lt;- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\nWarning in left_join(od_data, busstop_mpsz, by = c(DESTIN_BS = \"BUS_STOP_N\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 74 of `x` matches multiple rows in `y`.\nℹ Row 1379 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\nod_data &lt;- unique(od_data)\n\n\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\n`summarise()` has grouped output by 'ORIGIN_SZ'. You can override using the\n`.groups` argument.\n\n\nWe can save this into an rds file to be able to preserve our work so far.\n\nwrite_rds(od_data, \"data/rds/od_data_fii.rds\")\n\n\nod_data_fii &lt;- read_rds(\"data/rds/od_data.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#visualizing-spatial-interaction",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#visualizing-spatial-interaction",
    "title": "Spatial Interaction Models",
    "section": "Visualizing Spatial Interaction",
    "text": "Visualizing Spatial Interaction\nIn this section, we learn about using the stplanr package to prepare desire lines\n\nRemoving intra-zonal flows\nWe are not interested in intra-zonal flows. As such, we use the code below to remove them\n\nod_data_fij &lt;- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\n\n\n\nCreating desire lines\nIn the code below, we use od2line() of stplanr package to create the desire lines\n\nflowLine &lt;- od2line(flow = od_data_fii, \n                    zones = mpsz,\n                    zone_code = \"SUBZONE_C\")\n\nCreating centroids representing desire line start and end points.\n\n\nVisualizing the desire lines\nTo visualize the desire lines, the code chunk below can be used.\n\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\nWarning in g$scale * (w_legend/maxW): longer object length is not a multiple of\nshorter object length\n\n\nWarning in g$scale * (x/maxW): longer object length is not a multiple of\nshorter object length\n\n\n\n\n\n\n\n\n\nWhen there are too many flow lines rendering the visualization ineffective, it is wise to just focus on a subset of flows. The code chunk below just shows the flows with value of at least 5000\n\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\nWarning in g$scale * (w_legend/maxW): longer object length is not a multiple of\nshorter object length\n\n\nWarning in g$scale * (x/maxW): longer object length is not a multiple of\nshorter object length"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#computing-the-distance-matrix",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#computing-the-distance-matrix",
    "title": "Spatial Interaction Models",
    "section": "Computing the Distance Matrix",
    "text": "Computing the Distance Matrix\n\nConverting the sf data table to SpatialPolygonsDataFrame\nThe distance matrix can be computed using sf or sp. Previous runs have shown that computing the distance matrix using sp rather than sf is faster so we will be using that here.\nFirst, we convert the subzone boundaries into a SpatialPolygonsDataFrame using the following code chunk\n\nmpsz_sp &lt;- as(mpsz, \"Spatial\")\nmpsz_sp\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\n\n\nComputing the distance matrix\nWe can use spDists() of sp package in the code chunk below to compute for the Euclidean distance between centroids of the subzones.\n\ndist &lt;- spDists(mpsz_sp, \n                longlat = FALSE)\n\n\nhead(dist, n=c(10, 10))\n\n           [,1]       [,2]      [,3]      [,4]       [,5]      [,6]      [,7]\n [1,]     0.000  3926.0025  3939.108 20252.964  2989.9839  1431.330 19211.836\n [2,]  3926.003     0.0000   305.737 16513.865   951.8314  5254.066 16242.523\n [3,]  3939.108   305.7370     0.000 16412.062  1045.9088  5299.849 16026.146\n [4,] 20252.964 16513.8648 16412.062     0.000 17450.3044 21665.795  7229.017\n [5,]  2989.984   951.8314  1045.909 17450.304     0.0000  4303.232 17020.916\n [6,]  1431.330  5254.0664  5299.849 21665.795  4303.2323     0.000 20617.082\n [7,] 19211.836 16242.5230 16026.146  7229.017 17020.9161 20617.082     0.000\n [8,] 14960.942 12749.4101 12477.871 11284.279 13336.0421 16281.453  5606.082\n [9,]  7515.256  7934.8082  7649.776 18427.503  7801.6163  8403.896 14810.930\n[10,]  6391.342  4975.0021  4669.295 15469.566  5226.8731  7707.091 13111.391\n           [,8]      [,9]     [,10]\n [1,] 14960.942  7515.256  6391.342\n [2,] 12749.410  7934.808  4975.002\n [3,] 12477.871  7649.776  4669.295\n [4,] 11284.279 18427.503 15469.566\n [5,] 13336.042  7801.616  5226.873\n [6,] 16281.453  8403.896  7707.091\n [7,]  5606.082 14810.930 13111.391\n [8,]     0.000  9472.024  8575.490\n [9,]  9472.024     0.000  3780.800\n[10,]  8575.490  3780.800     0.000\n\n\n\n\nLabeling row and column names\nWe first create a sorted list according to the distance matrix based on the planning zone subcode\n\nsz_names &lt;- mpsz$SUBZONE_C\n\nNext we attach the subzone names to the row and column headers\n\ncolnames(dist) &lt;- paste0(sz_names)\nrownames(dist) &lt;- paste0(sz_names)\n\n\n\nPivoting distance value by subzone name\nWe use the code chunk below to unpivot the distance matrix into a long table by using the subzone codes\n\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value)\nhead(distPair, 10)\n\n     Var1   Var2      dist\n1  MESZ01 MESZ01     0.000\n2  RVSZ05 MESZ01  3926.003\n3  SRSZ01 MESZ01  3939.108\n4  WISZ01 MESZ01 20252.964\n5  MUSZ02 MESZ01  2989.984\n6  MPSZ05 MESZ01  1431.330\n7  WISZ03 MESZ01 19211.836\n8  WISZ02 MESZ01 14960.942\n9  SISZ02 MESZ01  7515.256\n10 SISZ01 MESZ01  6391.342\n\n\nWe see that intrazone distances appear as zero here\n\n\nUpdating intrazonal distances\nWe use this section to raplce the intrazonal distance with another constant\nWe first use summary() to select and fin the minim value of the distance\n\ndistPair %&gt;%\n  filter(dist &gt; 0) %&gt;%\n  summary()\n\n      Var1             Var2             dist        \n MESZ01 :   331   MESZ01 :   331   Min.   :  173.8  \n RVSZ05 :   331   RVSZ05 :   331   1st Qu.: 7149.5  \n SRSZ01 :   331   SRSZ01 :   331   Median :11890.0  \n WISZ01 :   331   WISZ01 :   331   Mean   :12229.4  \n MUSZ02 :   331   MUSZ02 :   331   3rd Qu.:16401.7  \n MPSZ05 :   331   MPSZ05 :   331   Max.   :49894.4  \n (Other):107906   (Other):107906                    \n\n\nNext, we replace the distance with a value of 50 if its current is zero\n\ndistPair$dist &lt;- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\nThe code chunk below checks the resulting dataframe\n\ndistPair %&gt;%\n  summary()\n\n      Var1             Var2             dist      \n MESZ01 :   332   MESZ01 :   332   Min.   :   50  \n RVSZ05 :   332   RVSZ05 :   332   1st Qu.: 7097  \n SRSZ01 :   332   SRSZ01 :   332   Median :11864  \n WISZ01 :   332   WISZ01 :   332   Mean   :12193  \n MUSZ02 :   332   MUSZ02 :   332   3rd Qu.:16388  \n MPSZ05 :   332   MPSZ05 :   332   Max.   :49894  \n (Other):108232   (Other):108232                  \n\n\nWe then use the following to rename the origin and the destination fields\n\ndistPair &lt;- distPair %&gt;%\n  rename(orig = Var1,\n         dest = Var2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#preparing-the-flow-data-1",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#preparing-the-flow-data-1",
    "title": "Spatial Interaction Models",
    "section": "Preparing the flow data",
    "text": "Preparing the flow data\nWe will start with the od_data_fii object for this step.\nWe compute the total number of passenger trips between and within subzone using the code chunk below\n\nflow_data &lt;- od_data_fii %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;% \n  summarize(TRIPS = sum(MORNING_PEAK)) \n\n`summarise()` has grouped output by 'ORIGIN_SZ'. You can override using the\n`.groups` argument.\n\n\nWe show the first ten records using the code chunk below\n\nhead(flow_data, 10)\n\n# A tibble: 10 × 3\n# Groups:   ORIGIN_SZ [1]\n   ORIGIN_SZ DESTIN_SZ TRIPS\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 AMSZ01    AMSZ01     1998\n 2 AMSZ01    AMSZ02     8289\n 3 AMSZ01    AMSZ03     8971\n 4 AMSZ01    AMSZ04     2252\n 5 AMSZ01    AMSZ05     6136\n 6 AMSZ01    AMSZ06     2148\n 7 AMSZ01    AMSZ07     1620\n 8 AMSZ01    AMSZ08     1925\n 9 AMSZ01    AMSZ09     1773\n10 AMSZ01    AMSZ10       63\n\n\n\nSeparating intra-flow from passenger volume\nWe use the code chunk below to create three new fields in the dataframe\n\nflow_data$FlowNoIntra &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0, flow_data$TRIPS)\nflow_data$offset &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0.000001, 1)\n\n\n\nCombining passenger volume data with distance value\nWe first need to convert the data type of the origin and destinations into factors using the code chunk below\n\nflow_data$ORIGIN_SZ &lt;- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ &lt;- as.factor(flow_data$DESTIN_SZ)\n\nNext, we combine flow_data and distPair using left_join()\n\nflow_data1 &lt;- flow_data %&gt;%\n  left_join (distPair,\n             by = c(\"ORIGIN_SZ\" = \"orig\",\n                    \"DESTIN_SZ\" = \"dest\"))"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#preparing-origin-and-destination-attributes",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#preparing-origin-and-destination-attributes",
    "title": "Spatial Interaction Models",
    "section": "Preparing Origin and Destination Attributes",
    "text": "Preparing Origin and Destination Attributes\n\nImporting population data\n\npop &lt;- read_csv(\"data/aspatial/pop.csv\")\n\nRows: 332 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): PA, SZ\ndbl (3): AGE7_12, AGE13_24, AGE25_64\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\npop1 &lt;- pop %&gt;%\n  left_join(st_drop_geometry(mpsz), by = c(SZ = \"SUBZONE_N\"))\n\n\n\nPreparing origin and destination attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop1,\n            by = c(ORIGIN_SZ = \"SUBZONE_C\")) %&gt;%\n  rename(ORIGIN_AGE7_12 = AGE7_12,\n         ORIGIN_AGE13_24 = AGE13_24,\n         ORIGIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA))\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop1,\n            by = c(DESTIN_SZ = \"SUBZONE_C\")) %&gt;%\n  rename(DESTIN_AGE7_12 = AGE7_12,\n         DESTIN_AGE13_24 = AGE13_24,\n         DESTIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA))"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#calibrating-spatial-interaction-models",
    "href": "Hands-on/Hands-On_Ex13/Hands-On_Ex13.html#calibrating-spatial-interaction-models",
    "title": "Spatial Interaction Models",
    "section": "Calibrating Spatial Interaction models",
    "text": "Calibrating Spatial Interaction models\n\nImporting the modelling data\nWe rename the last object to indicate it as the modeling data\n\nSIM_data &lt;- flow_data1\n\n\n\nVisualizing the dependent variable\nWe can plot the distribution of the dependent variable using ggplot package\n\nggplot(data = SIM_data,\n       aes(x = TRIPS)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe distribution is highly skewed and far from normal\nWe can visualize the relationship between the dependent variable and an independent variable like distance using the code chunk below\n\nggplot(data = SIM_data,\n       aes(x = dist,\n           y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe plot doesn’t show a linear relationship between these variables\nAlternatively, we can use a log transformed version of these variables and see the relationship of those\n\nggplot(data = SIM_data,\n       aes(x = log(dist),\n           y = log(TRIPS))) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nChecking for variables with zero values\nSince the log of a zero value is underfined, it is important to ensure that there are no zeros especially for Poisson regression.\nThe code chunk below displays summary statistics for all numeric variables.\n\nsummary(SIM_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS         FlowNoIntra      \n Length:14734       Length:14734       Min.   :     1   Min.   :     0.0  \n Class :character   Class :character   1st Qu.:    14   1st Qu.:    13.0  \n Mode  :character   Mode  :character   Median :    76   Median :    70.0  \n                                       Mean   :  1021   Mean   :   839.9  \n                                       3rd Qu.:   426   3rd Qu.:   379.0  \n                                       Max.   :232187   Max.   :148274.0  \n     offset              dist           SZ.x           ORIGIN_AGE7_12\n Min.   :0.000001   Min.   :   50   Length:14734       Min.   :   0  \n 1st Qu.:1.000000   1st Qu.: 3346   Class :character   1st Qu.: 240  \n Median :1.000000   Median : 6067   Mode  :character   Median : 700  \n Mean   :0.982150   Mean   : 6880                      Mean   :1032  \n 3rd Qu.:1.000000   3rd Qu.: 9729                      3rd Qu.:1480  \n Max.   :1.000000   Max.   :26136                      Max.   :6340  \n ORIGIN_AGE13_24 ORIGIN_AGE25_64 PLN_AREA_N.x       PLN_AREA_C.x      \n Min.   :    0   Min.   :    0   Length:14734       Length:14734      \n 1st Qu.:  440   1st Qu.: 2200   Class :character   Class :character  \n Median : 1350   Median : 6810   Mode  :character   Mode  :character  \n Mean   : 2269   Mean   :10487                                        \n 3rd Qu.: 3260   3rd Qu.:15770                                        \n Max.   :16380   Max.   :74610                                        \n  REGION_N.x         REGION_C.x            SZ.y           DESTIN_AGE7_12\n Length:14734       Length:14734       Length:14734       Min.   :   0  \n Class :character   Class :character   Class :character   1st Qu.: 240  \n Mode  :character   Mode  :character   Mode  :character   Median : 720  \n                                                          Mean   :1033  \n                                                          3rd Qu.:1500  \n                                                          Max.   :6340  \n DESTIN_AGE13_24 DESTIN_AGE25_64 PLN_AREA_N.y       PLN_AREA_C.y      \n Min.   :    0   Min.   :    0   Length:14734       Length:14734      \n 1st Qu.:  460   1st Qu.: 2200   Class :character   Class :character  \n Median : 1420   Median : 7030   Mode  :character   Mode  :character  \n Mean   : 2290   Mean   :10574                                        \n 3rd Qu.: 3260   3rd Qu.:15830                                        \n Max.   :16380   Max.   :74610                                        \n  REGION_N.y         REGION_C.y       \n Length:14734       Length:14734      \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nThe report shows some variables that have zero values. We use the code chunk below to replace any zero values for those variables with 0.99\n\nSIM_data$DESTIN_AGE7_12 &lt;- ifelse(\n  SIM_data$DESTIN_AGE7_12 == 0,\n  0.99, SIM_data$DESTIN_AGE7_12)\nSIM_data$DESTIN_AGE13_24 &lt;- ifelse(\n  SIM_data$DESTIN_AGE13_24 == 0,\n  0.99, SIM_data$DESTIN_AGE13_24)\nSIM_data$DESTIN_AGE25_64 &lt;- ifelse(\n  SIM_data$DESTIN_AGE25_64 == 0,\n  0.99, SIM_data$DESTIN_AGE25_64)\nSIM_data$ORIGIN_AGE7_12 &lt;- ifelse(\n  SIM_data$ORIGIN_AGE7_12 == 0,\n  0.99, SIM_data$ORIGIN_AGE7_12)\nSIM_data$ORIGIN_AGE13_24 &lt;- ifelse(\n  SIM_data$ORIGIN_AGE13_24 == 0,\n  0.99, SIM_data$ORIGIN_AGE13_24)\nSIM_data$ORIGIN_AGE25_64 &lt;- ifelse(\n  SIM_data$ORIGIN_AGE25_64 == 0,\n  0.99, SIM_data$ORIGIN_AGE25_64)\n\n\n\nUnconstrained spatial interaction model\nThe code chunk below uses glm() to calibrate a spatial interaction model\n\nuncSIM &lt;- glm(formula = TRIPS ~ \n                log(ORIGIN_AGE25_64) + \n                log(DESTIN_AGE25_64) +\n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nuncSIM\n\n\nCall:  glm(formula = TRIPS ~ log(ORIGIN_AGE25_64) + log(DESTIN_AGE25_64) + \n    log(dist), family = poisson(link = \"log\"), data = SIM_data, \n    na.action = na.exclude)\n\nCoefficients:\n         (Intercept)  log(ORIGIN_AGE25_64)  log(DESTIN_AGE25_64)  \n           10.407308              0.244859              0.009562  \n           log(dist)  \n           -0.705896  \n\nDegrees of Freedom: 14733 Total (i.e. Null);  14730 Residual\nNull Deviance:      60800000 \nResidual Deviance: 36430000     AIC: 36520000\n\n\n\n\nR-squared function\nWe write a function to compute for the R-squared value in order to measure the variation in the number of trips accounted for by the model\n\nCalcRSquared &lt;- function(observed,estimated){\n  r &lt;- cor(observed,estimated)\n  R2 &lt;- r^2\n  R2\n}\n\nWe compute for the R-squared of the unconstrained SIM using the code chunk below\n\nCalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)\n\n[1] 0.1892576\n\n\n\nr2_mcfadden(uncSIM)\n\n# R2 for Generalized Linear Regression\n       R2: 0.400\n  adj. R2: 0.400\n\n\n\n\nOrigin constrained SIM\nThe code chunk below fits an origin constrained model\n\norcSIM &lt;- glm(formula = TRIPS ~ \n                 ORIGIN_SZ +\n                 log(DESTIN_AGE25_64) +\n                 log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nsummary(orcSIM)\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + log(DESTIN_AGE25_64) + log(dist), \n    family = poisson(link = \"log\"), data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                       Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)           1.211e+01  3.785e-03  3199.012  &lt; 2e-16 ***\nORIGIN_SZAMSZ02       1.008e+00  4.450e-03   226.401  &lt; 2e-16 ***\nORIGIN_SZAMSZ03       5.474e-01  4.563e-03   119.959  &lt; 2e-16 ***\nORIGIN_SZAMSZ04      -7.494e-02  5.187e-03   -14.448  &lt; 2e-16 ***\nORIGIN_SZAMSZ05      -2.006e-01  5.790e-03   -34.650  &lt; 2e-16 ***\nORIGIN_SZAMSZ06       4.193e-01  5.130e-03    81.736  &lt; 2e-16 ***\nORIGIN_SZAMSZ07      -1.372e+00  9.683e-03  -141.686  &lt; 2e-16 ***\nORIGIN_SZAMSZ08      -1.022e+00  8.956e-03  -114.087  &lt; 2e-16 ***\nORIGIN_SZAMSZ09       2.239e-01  5.408e-03    41.396  &lt; 2e-16 ***\nORIGIN_SZAMSZ10       5.061e-01  4.716e-03   107.311  &lt; 2e-16 ***\nORIGIN_SZAMSZ11      -1.856e+00  1.285e-02  -144.414  &lt; 2e-16 ***\nORIGIN_SZAMSZ12      -1.580e+00  1.076e-02  -146.883  &lt; 2e-16 ***\nORIGIN_SZBDSZ01       1.072e+00  4.345e-03   246.734  &lt; 2e-16 ***\nORIGIN_SZBDSZ02       5.198e-01  5.079e-03   102.340  &lt; 2e-16 ***\nORIGIN_SZBDSZ03       9.865e-01  4.490e-03   219.724  &lt; 2e-16 ***\nORIGIN_SZBDSZ04       1.767e+00  3.894e-03   453.646  &lt; 2e-16 ***\nORIGIN_SZBDSZ05       6.395e-01  4.546e-03   140.691  &lt; 2e-16 ***\nORIGIN_SZBDSZ06       9.363e-01  4.543e-03   206.094  &lt; 2e-16 ***\nORIGIN_SZBDSZ07      -1.281e+00  9.558e-03  -133.991  &lt; 2e-16 ***\nORIGIN_SZBDSZ08      -1.167e+00  9.032e-03  -129.194  &lt; 2e-16 ***\nORIGIN_SZBKSZ01      -4.540e-01  6.538e-03   -69.437  &lt; 2e-16 ***\nORIGIN_SZBKSZ02       3.736e-01  5.115e-03    73.050  &lt; 2e-16 ***\nORIGIN_SZBKSZ03       5.841e-01  4.934e-03   118.375  &lt; 2e-16 ***\nORIGIN_SZBKSZ04      -1.177e-01  5.914e-03   -19.895  &lt; 2e-16 ***\nORIGIN_SZBKSZ05      -2.164e-01  5.832e-03   -37.115  &lt; 2e-16 ***\nORIGIN_SZBKSZ06       3.684e-03  5.873e-03     0.627  0.53048    \nORIGIN_SZBKSZ07       7.456e-01  4.426e-03   168.439  &lt; 2e-16 ***\nORIGIN_SZBKSZ08      -2.279e-02  5.348e-03    -4.261 2.04e-05 ***\nORIGIN_SZBKSZ09      -9.572e-02  5.721e-03   -16.733  &lt; 2e-16 ***\nORIGIN_SZBLSZ01      -1.688e+00  1.482e-02  -113.887  &lt; 2e-16 ***\nORIGIN_SZBLSZ02      -2.154e+00  1.924e-02  -111.980  &lt; 2e-16 ***\nORIGIN_SZBLSZ03      -3.249e+00  3.930e-02   -82.662  &lt; 2e-16 ***\nORIGIN_SZBLSZ04      -2.203e+00  2.306e-02   -95.557  &lt; 2e-16 ***\nORIGIN_SZBMSZ01      -1.267e-01  5.222e-03   -24.266  &lt; 2e-16 ***\nORIGIN_SZBMSZ02      -1.075e+00  6.742e-03  -159.386  &lt; 2e-16 ***\nORIGIN_SZBMSZ03      -4.386e-01  5.794e-03   -75.707  &lt; 2e-16 ***\nORIGIN_SZBMSZ04      -6.333e-02  5.157e-03   -12.280  &lt; 2e-16 ***\nORIGIN_SZBMSZ05      -2.256e+00  1.247e-02  -180.957  &lt; 2e-16 ***\nORIGIN_SZBMSZ06      -2.378e+00  1.618e-02  -147.029  &lt; 2e-16 ***\nORIGIN_SZBMSZ07      -4.769e-01  5.653e-03   -84.362  &lt; 2e-16 ***\nORIGIN_SZBMSZ08      -5.652e-01  5.811e-03   -97.259  &lt; 2e-16 ***\nORIGIN_SZBMSZ09      -1.232e+00  8.688e-03  -141.760  &lt; 2e-16 ***\nORIGIN_SZBMSZ10      -1.471e+00  9.130e-03  -161.131  &lt; 2e-16 ***\nORIGIN_SZBMSZ11      -7.866e-01  6.595e-03  -119.263  &lt; 2e-16 ***\nORIGIN_SZBMSZ12      -1.072e+00  9.149e-03  -117.206  &lt; 2e-16 ***\nORIGIN_SZBMSZ13      -1.207e-01  5.691e-03   -21.218  &lt; 2e-16 ***\nORIGIN_SZBMSZ14      -5.376e-01  6.629e-03   -81.098  &lt; 2e-16 ***\nORIGIN_SZBMSZ15      -3.253e-01  6.054e-03   -53.740  &lt; 2e-16 ***\nORIGIN_SZBMSZ16      -1.548e+00  9.144e-03  -169.303  &lt; 2e-16 ***\nORIGIN_SZBMSZ17      -2.169e+00  1.576e-02  -137.622  &lt; 2e-16 ***\nORIGIN_SZBPSZ01       1.369e-01  5.553e-03    24.660  &lt; 2e-16 ***\nORIGIN_SZBPSZ02      -3.292e-02  6.462e-03    -5.094 3.50e-07 ***\nORIGIN_SZBPSZ03       1.491e-01  6.149e-03    24.241  &lt; 2e-16 ***\nORIGIN_SZBPSZ04       3.544e-01  5.084e-03    69.711  &lt; 2e-16 ***\nORIGIN_SZBPSZ05       5.454e-01  4.554e-03   119.764  &lt; 2e-16 ***\nORIGIN_SZBPSZ06      -1.406e+00  9.311e-03  -151.045  &lt; 2e-16 ***\nORIGIN_SZBPSZ07      -1.004e+00  8.575e-03  -117.068  &lt; 2e-16 ***\nORIGIN_SZBSSZ01      -1.625e-02  5.276e-03    -3.080  0.00207 ** \nORIGIN_SZBSSZ02       3.088e-01  4.787e-03    64.495  &lt; 2e-16 ***\nORIGIN_SZBSSZ03       2.555e-01  4.689e-03    54.487  &lt; 2e-16 ***\nORIGIN_SZBTSZ01      -6.646e-02  5.385e-03   -12.340  &lt; 2e-16 ***\nORIGIN_SZBTSZ02      -1.078e+00  7.797e-03  -138.225  &lt; 2e-16 ***\nORIGIN_SZBTSZ03      -2.284e-01  5.727e-03   -39.876  &lt; 2e-16 ***\nORIGIN_SZBTSZ04      -1.053e+00  1.019e-02  -103.339  &lt; 2e-16 ***\nORIGIN_SZBTSZ05      -1.647e+00  1.100e-02  -149.690  &lt; 2e-16 ***\nORIGIN_SZBTSZ06      -7.804e-01  7.181e-03  -108.682  &lt; 2e-16 ***\nORIGIN_SZBTSZ07      -2.298e+00  1.321e-02  -173.921  &lt; 2e-16 ***\nORIGIN_SZBTSZ08      -1.283e+00  9.394e-03  -136.560  &lt; 2e-16 ***\nORIGIN_SZCBSZ01      -1.911e+00  5.483e-02   -34.844  &lt; 2e-16 ***\nORIGIN_SZCCSZ01      -1.758e+00  1.331e-02  -132.099  &lt; 2e-16 ***\nORIGIN_SZCHSZ01      -1.236e+00  1.178e-02  -104.954  &lt; 2e-16 ***\nORIGIN_SZCHSZ02      -5.424e-01  7.940e-03   -68.307  &lt; 2e-16 ***\nORIGIN_SZCHSZ03       4.332e-01  5.841e-03    74.153  &lt; 2e-16 ***\nORIGIN_SZCKSZ01       1.843e-01  5.117e-03    36.007  &lt; 2e-16 ***\nORIGIN_SZCKSZ02       6.800e-01  5.087e-03   133.672  &lt; 2e-16 ***\nORIGIN_SZCKSZ03       8.030e-01  4.522e-03   177.574  &lt; 2e-16 ***\nORIGIN_SZCKSZ04       1.298e+00  4.562e-03   284.446  &lt; 2e-16 ***\nORIGIN_SZCKSZ05       1.011e+00  5.305e-03   190.602  &lt; 2e-16 ***\nORIGIN_SZCKSZ06       1.262e+00  5.042e-03   250.262  &lt; 2e-16 ***\nORIGIN_SZCLSZ01      -6.805e-01  7.661e-03   -88.836  &lt; 2e-16 ***\nORIGIN_SZCLSZ02      -1.837e+00  1.364e-02  -134.665  &lt; 2e-16 ***\nORIGIN_SZCLSZ03      -1.001e+00  7.949e-03  -125.969  &lt; 2e-16 ***\nORIGIN_SZCLSZ04       6.966e-01  4.460e-03   156.204  &lt; 2e-16 ***\nORIGIN_SZCLSZ05      -1.974e+00  1.474e-02  -133.906  &lt; 2e-16 ***\nORIGIN_SZCLSZ06       8.585e-01  4.204e-03   204.230  &lt; 2e-16 ***\nORIGIN_SZCLSZ07      -2.974e-01  5.575e-03   -53.346  &lt; 2e-16 ***\nORIGIN_SZCLSZ08       3.231e-01  5.802e-03    55.688  &lt; 2e-16 ***\nORIGIN_SZCLSZ09      -1.697e+00  1.555e-02  -109.106  &lt; 2e-16 ***\nORIGIN_SZDTSZ02      -4.061e+00  8.341e-02   -48.693  &lt; 2e-16 ***\nORIGIN_SZDTSZ03      -4.031e+00  7.381e-02   -54.618  &lt; 2e-16 ***\nORIGIN_SZDTSZ13      -3.000e+00  3.129e-02   -95.889  &lt; 2e-16 ***\nORIGIN_SZGLSZ01      -1.405e+00  9.192e-03  -152.876  &lt; 2e-16 ***\nORIGIN_SZGLSZ02       2.536e-01  4.889e-03    51.880  &lt; 2e-16 ***\nORIGIN_SZGLSZ03       2.411e-01  4.855e-03    49.649  &lt; 2e-16 ***\nORIGIN_SZGLSZ04       8.350e-01  4.200e-03   198.826  &lt; 2e-16 ***\nORIGIN_SZGLSZ05       6.207e-01  4.375e-03   141.857  &lt; 2e-16 ***\nORIGIN_SZHGSZ01       2.806e-01  4.746e-03    59.121  &lt; 2e-16 ***\nORIGIN_SZHGSZ02       4.917e-01  4.712e-03   104.351  &lt; 2e-16 ***\nORIGIN_SZHGSZ03       2.452e-01  5.113e-03    47.952  &lt; 2e-16 ***\nORIGIN_SZHGSZ04       9.052e-01  4.303e-03   210.358  &lt; 2e-16 ***\nORIGIN_SZHGSZ05       1.170e+00  4.253e-03   275.033  &lt; 2e-16 ***\nORIGIN_SZHGSZ06      -1.016e-01  5.413e-03   -18.773  &lt; 2e-16 ***\nORIGIN_SZHGSZ07       6.984e-01  4.455e-03   156.757  &lt; 2e-16 ***\nORIGIN_SZHGSZ08       1.005e-01  5.354e-03    18.781  &lt; 2e-16 ***\nORIGIN_SZHGSZ09      -5.390e-01  6.962e-03   -77.417  &lt; 2e-16 ***\nORIGIN_SZHGSZ10      -3.512e+00  4.211e-02   -83.388  &lt; 2e-16 ***\nORIGIN_SZJESZ01       4.022e-01  4.869e-03    82.601  &lt; 2e-16 ***\nORIGIN_SZJESZ02       2.273e-01  4.924e-03    46.158  &lt; 2e-16 ***\nORIGIN_SZJESZ03       1.829e-01  5.286e-03    34.598  &lt; 2e-16 ***\nORIGIN_SZJESZ04      -1.177e+00  9.142e-03  -128.767  &lt; 2e-16 ***\nORIGIN_SZJESZ05      -2.065e+00  1.382e-02  -149.494  &lt; 2e-16 ***\nORIGIN_SZJESZ06       2.301e-01  4.853e-03    47.410  &lt; 2e-16 ***\nORIGIN_SZJESZ07      -1.889e+00  1.183e-02  -159.599  &lt; 2e-16 ***\nORIGIN_SZJESZ08      -1.062e+00  1.147e-02   -92.551  &lt; 2e-16 ***\nORIGIN_SZJESZ09       5.237e-01  4.959e-03   105.612  &lt; 2e-16 ***\nORIGIN_SZJESZ10      -1.829e+00  1.800e-02  -101.616  &lt; 2e-16 ***\nORIGIN_SZJESZ11      -2.023e+00  1.931e-02  -104.738  &lt; 2e-16 ***\nORIGIN_SZJWSZ01       2.125e-01  6.405e-03    33.183  &lt; 2e-16 ***\nORIGIN_SZJWSZ02       8.858e-01  4.521e-03   195.929  &lt; 2e-16 ***\nORIGIN_SZJWSZ03       1.269e+00  4.188e-03   302.922  &lt; 2e-16 ***\nORIGIN_SZJWSZ04       1.284e+00  4.280e-03   300.017  &lt; 2e-16 ***\nORIGIN_SZJWSZ05      -1.393e+00  1.252e-02  -111.339  &lt; 2e-16 ***\nORIGIN_SZJWSZ06      -1.015e+00  1.067e-02   -95.109  &lt; 2e-16 ***\nORIGIN_SZJWSZ07      -2.694e+00  2.751e-02   -97.911  &lt; 2e-16 ***\nORIGIN_SZJWSZ08       1.950e+00  4.110e-03   474.430  &lt; 2e-16 ***\nORIGIN_SZJWSZ09       1.831e+00  3.899e-03   469.595  &lt; 2e-16 ***\nORIGIN_SZKLSZ01       1.636e-01  4.902e-03    33.374  &lt; 2e-16 ***\nORIGIN_SZKLSZ02      -5.156e-01  6.321e-03   -81.570  &lt; 2e-16 ***\nORIGIN_SZKLSZ03      -4.145e-01  5.949e-03   -69.666  &lt; 2e-16 ***\nORIGIN_SZKLSZ04      -2.283e+00  1.187e-02  -192.327  &lt; 2e-16 ***\nORIGIN_SZKLSZ05      -8.593e-01  8.272e-03  -103.882  &lt; 2e-16 ***\nORIGIN_SZKLSZ06      -4.709e+00  1.857e-01   -25.352  &lt; 2e-16 ***\nORIGIN_SZKLSZ07      -1.123e+00  8.408e-03  -133.615  &lt; 2e-16 ***\nORIGIN_SZKLSZ08      -1.476e+00  9.152e-03  -161.321  &lt; 2e-16 ***\nORIGIN_SZLKSZ01      -3.273e+00  3.875e-02   -84.465  &lt; 2e-16 ***\nORIGIN_SZMDSZ01      -2.615e+00  2.802e-02   -93.303  &lt; 2e-16 ***\nORIGIN_SZMDSZ02      -8.945e-01  1.035e-02   -86.389  &lt; 2e-16 ***\nORIGIN_SZMDSZ03      -1.998e+00  1.703e-02  -117.297  &lt; 2e-16 ***\nORIGIN_SZMPSZ01      -1.093e+00  8.367e-03  -130.656  &lt; 2e-16 ***\nORIGIN_SZMPSZ02      -5.975e-01  6.898e-03   -86.616  &lt; 2e-16 ***\nORIGIN_SZMPSZ03      -9.706e-03  5.319e-03    -1.825  0.06804 .  \nORIGIN_SZMUSZ02      -3.923e+00  1.038e-01   -37.806  &lt; 2e-16 ***\nORIGIN_SZNTSZ01      -2.829e+00  3.529e-02   -80.157  &lt; 2e-16 ***\nORIGIN_SZNTSZ02      -3.256e+00  2.323e-02  -140.180  &lt; 2e-16 ***\nORIGIN_SZNTSZ03      -9.865e-01  7.777e-03  -126.848  &lt; 2e-16 ***\nORIGIN_SZNTSZ05      -3.353e+00  4.964e-02   -67.546  &lt; 2e-16 ***\nORIGIN_SZNTSZ06      -3.818e+00  5.576e-02   -68.483  &lt; 2e-16 ***\nORIGIN_SZNVSZ01       4.449e-01  4.482e-03    99.269  &lt; 2e-16 ***\nORIGIN_SZNVSZ02      -6.279e-01  6.470e-03   -97.044  &lt; 2e-16 ***\nORIGIN_SZNVSZ03      -1.212e+00  7.788e-03  -155.644  &lt; 2e-16 ***\nORIGIN_SZNVSZ04      -1.469e+00  9.091e-03  -161.543  &lt; 2e-16 ***\nORIGIN_SZNVSZ05      -2.628e+00  1.579e-02  -166.466  &lt; 2e-16 ***\nORIGIN_SZPGSZ01      -9.541e-01  1.223e-02   -78.035  &lt; 2e-16 ***\nORIGIN_SZPGSZ02      -5.353e-01  7.233e-03   -74.009  &lt; 2e-16 ***\nORIGIN_SZPGSZ03       9.574e-01  4.437e-03   215.779  &lt; 2e-16 ***\nORIGIN_SZPGSZ04       1.110e+00  4.417e-03   251.169  &lt; 2e-16 ***\nORIGIN_SZPGSZ05       2.658e-01  5.758e-03    46.156  &lt; 2e-16 ***\nORIGIN_SZPLSZ01      -8.153e-01  1.044e-02   -78.119  &lt; 2e-16 ***\nORIGIN_SZPLSZ02      -1.675e+00  1.478e-02  -113.340  &lt; 2e-16 ***\nORIGIN_SZPLSZ03      -2.963e+00  3.672e-02   -80.686  &lt; 2e-16 ***\nORIGIN_SZPLSZ04      -3.279e+00  3.684e-02   -89.012  &lt; 2e-16 ***\nORIGIN_SZPLSZ05      -2.466e+00  2.245e-02  -109.864  &lt; 2e-16 ***\nORIGIN_SZPNSZ01       1.411e+00  4.584e-03   307.690  &lt; 2e-16 ***\nORIGIN_SZPNSZ02      -5.043e-01  1.108e-02   -45.503  &lt; 2e-16 ***\nORIGIN_SZPNSZ03      -1.878e+00  1.940e-02   -96.796  &lt; 2e-16 ***\nORIGIN_SZPNSZ04      -2.761e+00  3.112e-02   -88.706  &lt; 2e-16 ***\nORIGIN_SZPNSZ05      -2.277e+00  2.628e-02   -86.662  &lt; 2e-16 ***\nORIGIN_SZPRSZ01      -7.934e-01  1.142e-02   -69.499  &lt; 2e-16 ***\nORIGIN_SZPRSZ02       9.414e-01  4.615e-03   203.981  &lt; 2e-16 ***\nORIGIN_SZPRSZ03       7.674e-01  4.626e-03   165.881  &lt; 2e-16 ***\nORIGIN_SZPRSZ04      -3.771e-01  7.516e-03   -50.168  &lt; 2e-16 ***\nORIGIN_SZPRSZ05       1.327e+00  4.325e-03   306.737  &lt; 2e-16 ***\nORIGIN_SZPRSZ06      -4.081e-01  8.651e-03   -47.172  &lt; 2e-16 ***\nORIGIN_SZPRSZ07      -2.151e+00  1.610e-02  -133.558  &lt; 2e-16 ***\nORIGIN_SZPRSZ08       5.293e-04  6.383e-03     0.083  0.93391    \nORIGIN_SZQTSZ01      -4.144e-01  6.846e-03   -60.539  &lt; 2e-16 ***\nORIGIN_SZQTSZ02      -7.967e-01  6.327e-03  -125.933  &lt; 2e-16 ***\nORIGIN_SZQTSZ03      -2.415e-01  5.681e-03   -42.509  &lt; 2e-16 ***\nORIGIN_SZQTSZ04      -1.013e+00  7.129e-03  -142.123  &lt; 2e-16 ***\nORIGIN_SZQTSZ05      -3.923e-01  5.994e-03   -65.446  &lt; 2e-16 ***\nORIGIN_SZQTSZ06      -5.662e-01  6.481e-03   -87.359  &lt; 2e-16 ***\nORIGIN_SZQTSZ07      -1.558e+00  9.635e-03  -161.662  &lt; 2e-16 ***\nORIGIN_SZQTSZ08      -1.577e-01  5.699e-03   -27.665  &lt; 2e-16 ***\nORIGIN_SZQTSZ09      -6.189e-01  6.633e-03   -93.312  &lt; 2e-16 ***\nORIGIN_SZQTSZ10      -4.511e-01  6.512e-03   -69.271  &lt; 2e-16 ***\nORIGIN_SZQTSZ11      -1.455e+00  9.800e-03  -148.421  &lt; 2e-16 ***\nORIGIN_SZQTSZ12      -1.475e+00  1.044e-02  -141.309  &lt; 2e-16 ***\nORIGIN_SZQTSZ13      -3.529e-01  6.413e-03   -55.038  &lt; 2e-16 ***\nORIGIN_SZQTSZ14      -1.591e+00  9.847e-03  -161.565  &lt; 2e-16 ***\nORIGIN_SZQTSZ15      -8.955e-01  1.027e-02   -87.184  &lt; 2e-16 ***\nORIGIN_SZRCSZ01      -1.375e+00  1.265e-02  -108.704  &lt; 2e-16 ***\nORIGIN_SZRCSZ06      -6.196e-01  8.475e-03   -73.116  &lt; 2e-16 ***\nORIGIN_SZRVSZ01      -3.523e+00  3.237e-02  -108.818  &lt; 2e-16 ***\nORIGIN_SZRVSZ02      -2.912e+00  2.776e-02  -104.868  &lt; 2e-16 ***\nORIGIN_SZRVSZ03      -3.145e+00  2.379e-02  -132.232  &lt; 2e-16 ***\nORIGIN_SZRVSZ04      -3.357e+00  5.567e-02   -60.309  &lt; 2e-16 ***\nORIGIN_SZRVSZ05      -2.438e+00  1.644e-02  -148.272  &lt; 2e-16 ***\nORIGIN_SZSBSZ01       5.890e-01  5.529e-03   106.520  &lt; 2e-16 ***\nORIGIN_SZSBSZ02      -7.098e-01  8.213e-03   -86.432  &lt; 2e-16 ***\nORIGIN_SZSBSZ03       9.634e-01  4.611e-03   208.943  &lt; 2e-16 ***\nORIGIN_SZSBSZ04       7.729e-01  5.289e-03   146.136  &lt; 2e-16 ***\nORIGIN_SZSBSZ05      -9.966e-02  6.543e-03   -15.231  &lt; 2e-16 ***\nORIGIN_SZSBSZ06      -1.778e+00  1.719e-02  -103.427  &lt; 2e-16 ***\nORIGIN_SZSBSZ07      -1.161e+00  1.256e-02   -92.436  &lt; 2e-16 ***\nORIGIN_SZSBSZ08      -1.212e+00  1.222e-02   -99.227  &lt; 2e-16 ***\nORIGIN_SZSBSZ09      -5.783e-01  8.579e-03   -67.412  &lt; 2e-16 ***\nORIGIN_SZSESZ02       9.999e-01  4.409e-03   226.798  &lt; 2e-16 ***\nORIGIN_SZSESZ03       1.214e+00  4.164e-03   291.675  &lt; 2e-16 ***\nORIGIN_SZSESZ04       8.141e-01  4.868e-03   167.238  &lt; 2e-16 ***\nORIGIN_SZSESZ05      -2.186e-01  5.915e-03   -36.961  &lt; 2e-16 ***\nORIGIN_SZSESZ06       7.298e-01  4.689e-03   155.641  &lt; 2e-16 ***\nORIGIN_SZSESZ07      -2.543e+00  1.961e-02  -129.689  &lt; 2e-16 ***\nORIGIN_SZSGSZ01      -1.016e+00  8.550e-03  -118.869  &lt; 2e-16 ***\nORIGIN_SZSGSZ02      -1.120e+00  9.589e-03  -116.799  &lt; 2e-16 ***\nORIGIN_SZSGSZ03       2.169e-01  5.167e-03    41.970  &lt; 2e-16 ***\nORIGIN_SZSGSZ04       2.672e-01  4.792e-03    55.757  &lt; 2e-16 ***\nORIGIN_SZSGSZ05      -1.785e+00  1.060e-02  -168.456  &lt; 2e-16 ***\nORIGIN_SZSGSZ06       4.017e-01  4.541e-03    88.470  &lt; 2e-16 ***\nORIGIN_SZSGSZ07      -6.303e-01  6.235e-03  -101.098  &lt; 2e-16 ***\nORIGIN_SZSKSZ01      -1.928e-01  7.765e-03   -24.826  &lt; 2e-16 ***\nORIGIN_SZSKSZ02       3.870e-01  5.689e-03    68.026  &lt; 2e-16 ***\nORIGIN_SZSKSZ03      -6.815e-01  7.983e-03   -85.369  &lt; 2e-16 ***\nORIGIN_SZSKSZ04      -2.528e+00  2.702e-02   -93.548  &lt; 2e-16 ***\nORIGIN_SZSKSZ05      -1.370e+00  1.552e-02   -88.311  &lt; 2e-16 ***\nORIGIN_SZSLSZ01      -3.218e+00  3.058e-02  -105.238  &lt; 2e-16 ***\nORIGIN_SZSLSZ04      -6.800e-01  7.683e-03   -88.497  &lt; 2e-16 ***\nORIGIN_SZSRSZ01      -2.389e+00  1.583e-02  -150.989  &lt; 2e-16 ***\nORIGIN_SZTHSZ01      -2.183e+00  4.887e-02   -44.666  &lt; 2e-16 ***\nORIGIN_SZTHSZ03      -2.243e+00  2.243e-02  -100.025  &lt; 2e-16 ***\nORIGIN_SZTHSZ04      -2.005e+00  2.869e-02   -69.879  &lt; 2e-16 ***\nORIGIN_SZTHSZ06      -2.276e+00  1.784e-02  -127.557  &lt; 2e-16 ***\nORIGIN_SZTMSZ01       4.015e-01  5.814e-03    69.048  &lt; 2e-16 ***\nORIGIN_SZTMSZ02       2.222e+00  3.795e-03   585.568  &lt; 2e-16 ***\nORIGIN_SZTMSZ03       1.412e+00  4.108e-03   343.608  &lt; 2e-16 ***\nORIGIN_SZTMSZ04       9.106e-01  4.742e-03   192.036  &lt; 2e-16 ***\nORIGIN_SZTMSZ05      -3.259e-01  7.534e-03   -43.253  &lt; 2e-16 ***\nORIGIN_SZTNSZ01      -1.806e+00  1.038e-02  -174.076  &lt; 2e-16 ***\nORIGIN_SZTNSZ02      -1.741e+00  9.778e-03  -178.108  &lt; 2e-16 ***\nORIGIN_SZTNSZ03      -2.277e+00  1.338e-02  -170.199  &lt; 2e-16 ***\nORIGIN_SZTNSZ04      -7.703e-01  7.197e-03  -107.032  &lt; 2e-16 ***\nORIGIN_SZTPSZ01      -6.466e-01  6.287e-03  -102.841  &lt; 2e-16 ***\nORIGIN_SZTPSZ02       4.633e-01  4.347e-03   106.578  &lt; 2e-16 ***\nORIGIN_SZTPSZ03      -5.186e-01  6.085e-03   -85.234  &lt; 2e-16 ***\nORIGIN_SZTPSZ04      -2.900e-01  5.779e-03   -50.190  &lt; 2e-16 ***\nORIGIN_SZTPSZ05      -2.169e-01  6.072e-03   -35.720  &lt; 2e-16 ***\nORIGIN_SZTPSZ06       3.357e-01  5.942e-03    56.486  &lt; 2e-16 ***\nORIGIN_SZTPSZ07      -2.517e-01  6.317e-03   -39.846  &lt; 2e-16 ***\nORIGIN_SZTPSZ08      -1.075e+00  9.109e-03  -118.034  &lt; 2e-16 ***\nORIGIN_SZTPSZ09      -3.708e-01  6.189e-03   -59.903  &lt; 2e-16 ***\nORIGIN_SZTPSZ10      -6.889e-01  7.634e-03   -90.229  &lt; 2e-16 ***\nORIGIN_SZTPSZ11       7.661e-02  5.459e-03    14.033  &lt; 2e-16 ***\nORIGIN_SZTPSZ12      -5.971e-01  6.522e-03   -91.552  &lt; 2e-16 ***\nORIGIN_SZTSSZ01      -3.517e+00  4.739e-02   -74.210  &lt; 2e-16 ***\nORIGIN_SZTSSZ02       3.022e-01  7.334e-03    41.203  &lt; 2e-16 ***\nORIGIN_SZTSSZ03       3.730e-01  7.073e-03    52.733  &lt; 2e-16 ***\nORIGIN_SZTSSZ04       3.610e-01  7.463e-03    48.372  &lt; 2e-16 ***\nORIGIN_SZTSSZ05      -1.103e+00  1.404e-02   -78.566  &lt; 2e-16 ***\nORIGIN_SZTSSZ06      -1.310e+00  1.718e-02   -76.286  &lt; 2e-16 ***\nORIGIN_SZWCSZ01      -1.233e-01  7.861e-03   -15.690  &lt; 2e-16 ***\nORIGIN_SZWCSZ02      -2.872e+00  3.159e-02   -90.911  &lt; 2e-16 ***\nORIGIN_SZWCSZ03      -4.138e+00  1.241e-01   -33.349  &lt; 2e-16 ***\nORIGIN_SZWDSZ01       1.370e+00  4.146e-03   330.448  &lt; 2e-16 ***\nORIGIN_SZWDSZ02       1.041e+00  4.747e-03   219.219  &lt; 2e-16 ***\nORIGIN_SZWDSZ03       2.189e+00  4.035e-03   542.344  &lt; 2e-16 ***\nORIGIN_SZWDSZ04       1.142e+00  4.963e-03   230.074  &lt; 2e-16 ***\nORIGIN_SZWDSZ05       5.160e-01  4.998e-03   103.230  &lt; 2e-16 ***\nORIGIN_SZWDSZ06       1.208e+00  4.611e-03   262.019  &lt; 2e-16 ***\nORIGIN_SZWDSZ07      -3.805e-01  8.034e-03   -47.365  &lt; 2e-16 ***\nORIGIN_SZWDSZ08      -4.839e-01  7.878e-03   -61.426  &lt; 2e-16 ***\nORIGIN_SZWDSZ09       1.475e+00  4.401e-03   335.097  &lt; 2e-16 ***\nORIGIN_SZYSSZ01      -1.552e-01  5.643e-03   -27.496  &lt; 2e-16 ***\nORIGIN_SZYSSZ02       8.958e-01  4.973e-03   180.144  &lt; 2e-16 ***\nORIGIN_SZYSSZ03       1.757e+00  4.275e-03   411.050  &lt; 2e-16 ***\nORIGIN_SZYSSZ04       8.439e-01  4.538e-03   185.955  &lt; 2e-16 ***\nORIGIN_SZYSSZ05      -9.995e-02  5.920e-03   -16.884  &lt; 2e-16 ***\nORIGIN_SZYSSZ06      -1.175e+00  1.079e-02  -108.835  &lt; 2e-16 ***\nORIGIN_SZYSSZ07      -1.202e+00  1.127e-02  -106.642  &lt; 2e-16 ***\nORIGIN_SZYSSZ08       1.244e-02  6.104e-03     2.039  0.04148 *  \nORIGIN_SZYSSZ09       1.385e+00  4.239e-03   326.757  &lt; 2e-16 ***\nlog(DESTIN_AGE25_64)  2.298e-02  8.832e-05   260.146  &lt; 2e-16 ***\nlog(dist)            -6.947e-01  1.295e-04 -5363.438  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 60796037  on 14733  degrees of freedom\nResidual deviance: 26726668  on 14453  degrees of freedom\nAIC: 26818857\n\nNumber of Fisher Scoring iterations: 7\n\n\nWe can also examine the R-squared using the function we prepared\n\nCalcRSquared(orcSIM$data$TRIPS, orcSIM$fitted.values)\n\n[1] 0.4165837\n\n\n\n\nDestination Constrained SIM\nThe following code chunk fits a destination constrained model\n\ndecSIM &lt;- glm(formula = TRIPS ~ \n                DESTIN_SZ + \n                log(ORIGIN_AGE25_64) + \n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nsummary(decSIM)\n\n\nCall:\nglm(formula = TRIPS ~ DESTIN_SZ + log(ORIGIN_AGE25_64) + log(dist), \n    family = poisson(link = \"log\"), data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                       Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)          10.8110189  0.0033476  3229.499  &lt; 2e-16 ***\nDESTIN_SZAMSZ02       0.1775885  0.0041530    42.761  &lt; 2e-16 ***\nDESTIN_SZAMSZ03       0.2064091  0.0040888    50.482  &lt; 2e-16 ***\nDESTIN_SZAMSZ04      -0.9406455  0.0060637  -155.127  &lt; 2e-16 ***\nDESTIN_SZAMSZ05      -1.1578100  0.0061804  -187.337  &lt; 2e-16 ***\nDESTIN_SZAMSZ06      -0.8861493  0.0059241  -149.584  &lt; 2e-16 ***\nDESTIN_SZAMSZ07      -1.7712447  0.0096070  -184.370  &lt; 2e-16 ***\nDESTIN_SZAMSZ08      -1.0707197  0.0067763  -158.009  &lt; 2e-16 ***\nDESTIN_SZAMSZ09      -0.9682250  0.0060435  -160.210  &lt; 2e-16 ***\nDESTIN_SZAMSZ10       0.2612773  0.0043738    59.737  &lt; 2e-16 ***\nDESTIN_SZAMSZ11      -0.3714704  0.0086200   -43.094  &lt; 2e-16 ***\nDESTIN_SZAMSZ12       0.0250455  0.0049850     5.024 5.06e-07 ***\nDESTIN_SZBDSZ01       0.5154763  0.0037827   136.271  &lt; 2e-16 ***\nDESTIN_SZBDSZ02      -0.2843120  0.0049517   -57.417  &lt; 2e-16 ***\nDESTIN_SZBDSZ03      -0.0134646  0.0042692    -3.154  0.00161 ** \nDESTIN_SZBDSZ04       1.0014441  0.0034463   290.582  &lt; 2e-16 ***\nDESTIN_SZBDSZ05       0.3721573  0.0038992    95.445  &lt; 2e-16 ***\nDESTIN_SZBDSZ06       0.2013935  0.0042182    47.744  &lt; 2e-16 ***\nDESTIN_SZBDSZ07      -1.0642612  0.0092942  -114.508  &lt; 2e-16 ***\nDESTIN_SZBDSZ08      -1.7769370  0.0105721  -168.077  &lt; 2e-16 ***\nDESTIN_SZBKSZ01      -1.1944766  0.0065580  -182.141  &lt; 2e-16 ***\nDESTIN_SZBKSZ02      -0.2604946  0.0052044   -50.053  &lt; 2e-16 ***\nDESTIN_SZBKSZ03      -0.5905775  0.0055618  -106.184  &lt; 2e-16 ***\nDESTIN_SZBKSZ04      -0.0521573  0.0048274   -10.804  &lt; 2e-16 ***\nDESTIN_SZBKSZ05      -0.8258599  0.0057094  -144.650  &lt; 2e-16 ***\nDESTIN_SZBKSZ06      -0.8696763  0.0060934  -142.725  &lt; 2e-16 ***\nDESTIN_SZBKSZ07       0.2216292  0.0040334    54.949  &lt; 2e-16 ***\nDESTIN_SZBKSZ08      -1.1179375  0.0068749  -162.612  &lt; 2e-16 ***\nDESTIN_SZBKSZ09      -0.2888733  0.0049056   -58.886  &lt; 2e-16 ***\nDESTIN_SZBLSZ01      -0.4487061  0.0070226   -63.894  &lt; 2e-16 ***\nDESTIN_SZBLSZ02       0.6343096  0.0065174    97.326  &lt; 2e-16 ***\nDESTIN_SZBLSZ03       1.3492337  0.0074135   181.997  &lt; 2e-16 ***\nDESTIN_SZBLSZ04      -0.0339193  0.0131568    -2.578  0.00993 ** \nDESTIN_SZBMSZ01      -0.3497912  0.0046910   -74.567  &lt; 2e-16 ***\nDESTIN_SZBMSZ02      -0.5995634  0.0048828  -122.792  &lt; 2e-16 ***\nDESTIN_SZBMSZ03      -0.8726401  0.0056851  -153.495  &lt; 2e-16 ***\nDESTIN_SZBMSZ04      -0.5350402  0.0048888  -109.442  &lt; 2e-16 ***\nDESTIN_SZBMSZ05      -0.4981814  0.0065971   -75.515  &lt; 2e-16 ***\nDESTIN_SZBMSZ06      -2.0640198  0.0123050  -167.739  &lt; 2e-16 ***\nDESTIN_SZBMSZ07      -0.3100988  0.0045283   -68.480  &lt; 2e-16 ***\nDESTIN_SZBMSZ08      -1.2748152  0.0062622  -203.573  &lt; 2e-16 ***\nDESTIN_SZBMSZ09      -2.8056325  0.0143532  -195.471  &lt; 2e-16 ***\nDESTIN_SZBMSZ10      -1.9166407  0.0089273  -214.693  &lt; 2e-16 ***\nDESTIN_SZBMSZ11      -1.7261160  0.0079281  -217.722  &lt; 2e-16 ***\nDESTIN_SZBMSZ12      -1.1495908  0.0077721  -147.912  &lt; 2e-16 ***\nDESTIN_SZBMSZ13      -0.5428008  0.0050824  -106.799  &lt; 2e-16 ***\nDESTIN_SZBMSZ14      -1.1422302  0.0076325  -149.653  &lt; 2e-16 ***\nDESTIN_SZBMSZ15      -1.2217517  0.0068685  -177.878  &lt; 2e-16 ***\nDESTIN_SZBMSZ16      -2.4074288  0.0107900  -223.116  &lt; 2e-16 ***\nDESTIN_SZBMSZ17      -2.6985491  0.0164771  -163.776  &lt; 2e-16 ***\nDESTIN_SZBPSZ01      -0.6183085  0.0054605  -113.233  &lt; 2e-16 ***\nDESTIN_SZBPSZ02      -1.4579175  0.0083271  -175.080  &lt; 2e-16 ***\nDESTIN_SZBPSZ03      -1.0775392  0.0075109  -143.463  &lt; 2e-16 ***\nDESTIN_SZBPSZ04      -0.6645303  0.0058070  -114.436  &lt; 2e-16 ***\nDESTIN_SZBPSZ05       0.3449386  0.0039504    87.318  &lt; 2e-16 ***\nDESTIN_SZBPSZ06      -0.9360064  0.0077394  -120.941  &lt; 2e-16 ***\nDESTIN_SZBPSZ07      -0.6850065  0.0077761   -88.091  &lt; 2e-16 ***\nDESTIN_SZBSSZ01      -0.3144210  0.0045803   -68.647  &lt; 2e-16 ***\nDESTIN_SZBSSZ02      -0.7531935  0.0051075  -147.469  &lt; 2e-16 ***\nDESTIN_SZBSSZ03       0.1964072  0.0038255    51.342  &lt; 2e-16 ***\nDESTIN_SZBTSZ01       0.0749897  0.0041584    18.033  &lt; 2e-16 ***\nDESTIN_SZBTSZ02      -0.8214254  0.0065659  -125.105  &lt; 2e-16 ***\nDESTIN_SZBTSZ03      -0.1672596  0.0047942   -34.888  &lt; 2e-16 ***\nDESTIN_SZBTSZ04      -1.7727273  0.0103706  -170.938  &lt; 2e-16 ***\nDESTIN_SZBTSZ05      -0.8162630  0.0067401  -121.105  &lt; 2e-16 ***\nDESTIN_SZBTSZ06      -0.8159130  0.0059754  -136.546  &lt; 2e-16 ***\nDESTIN_SZBTSZ07      -2.1139258  0.0105602  -200.178  &lt; 2e-16 ***\nDESTIN_SZBTSZ08      -1.3565179  0.0086828  -156.231  &lt; 2e-16 ***\nDESTIN_SZCBSZ01      -4.6643129  0.3162417   -14.749  &lt; 2e-16 ***\nDESTIN_SZCCSZ01      -1.0088833  0.0080155  -125.866  &lt; 2e-16 ***\nDESTIN_SZCHSZ01      -1.1909317  0.0095262  -125.017  &lt; 2e-16 ***\nDESTIN_SZCHSZ02       0.0890035  0.0052277    17.025  &lt; 2e-16 ***\nDESTIN_SZCHSZ03       1.4883985  0.0039094   380.724  &lt; 2e-16 ***\nDESTIN_SZCKSZ01      -0.1684738  0.0047561   -35.422  &lt; 2e-16 ***\nDESTIN_SZCKSZ02      -0.4314614  0.0051537   -83.720  &lt; 2e-16 ***\nDESTIN_SZCKSZ03       0.6413457  0.0038639   165.983  &lt; 2e-16 ***\nDESTIN_SZCKSZ04      -0.6370791  0.0059869  -106.412  &lt; 2e-16 ***\nDESTIN_SZCKSZ05      -0.4185112  0.0065348   -64.044  &lt; 2e-16 ***\nDESTIN_SZCKSZ06       0.7003888  0.0045139   155.163  &lt; 2e-16 ***\nDESTIN_SZCLSZ01       0.3751343  0.0047400    79.143  &lt; 2e-16 ***\nDESTIN_SZCLSZ02      -2.2913668  0.0133371  -171.804  &lt; 2e-16 ***\nDESTIN_SZCLSZ03      -1.0498490  0.0076548  -137.149  &lt; 2e-16 ***\nDESTIN_SZCLSZ04      -0.1118915  0.0044886   -24.928  &lt; 2e-16 ***\nDESTIN_SZCLSZ05      -1.3113032  0.0084067  -155.983  &lt; 2e-16 ***\nDESTIN_SZCLSZ06       0.1661786  0.0040203    41.334  &lt; 2e-16 ***\nDESTIN_SZCLSZ07      -0.6429895  0.0052617  -122.202  &lt; 2e-16 ***\nDESTIN_SZCLSZ08      -0.4271702  0.0057208   -74.670  &lt; 2e-16 ***\nDESTIN_SZCLSZ09       0.3882136  0.0063758    60.888  &lt; 2e-16 ***\nDESTIN_SZDTSZ02      -3.0106480  0.0348374   -86.420  &lt; 2e-16 ***\nDESTIN_SZDTSZ03      -1.4195712  0.0144110   -98.506  &lt; 2e-16 ***\nDESTIN_SZDTSZ13      -2.2368573  0.0161427  -138.567  &lt; 2e-16 ***\nDESTIN_SZGLSZ01       0.0013721  0.0051224     0.268  0.78881    \nDESTIN_SZGLSZ02      -0.3376674  0.0046195   -73.097  &lt; 2e-16 ***\nDESTIN_SZGLSZ03       0.3659900  0.0038384    95.350  &lt; 2e-16 ***\nDESTIN_SZGLSZ04       0.2969928  0.0038026    78.103  &lt; 2e-16 ***\nDESTIN_SZGLSZ05       0.1786445  0.0038853    45.980  &lt; 2e-16 ***\nDESTIN_SZHGSZ01       0.2979206  0.0038825    76.735  &lt; 2e-16 ***\nDESTIN_SZHGSZ02      -0.5701034  0.0051182  -111.388  &lt; 2e-16 ***\nDESTIN_SZHGSZ03      -1.0387610  0.0061020  -170.233  &lt; 2e-16 ***\nDESTIN_SZHGSZ04      -0.2264881  0.0043617   -51.926  &lt; 2e-16 ***\nDESTIN_SZHGSZ05      -0.2287090  0.0044851   -50.993  &lt; 2e-16 ***\nDESTIN_SZHGSZ06      -0.7896437  0.0054081  -146.010  &lt; 2e-16 ***\nDESTIN_SZHGSZ07       0.2268880  0.0040336    56.249  &lt; 2e-16 ***\nDESTIN_SZHGSZ08      -0.4260784  0.0048967   -87.013  &lt; 2e-16 ***\nDESTIN_SZHGSZ09       0.1027784  0.0051341    20.019  &lt; 2e-16 ***\nDESTIN_SZHGSZ10      -2.8571803  0.0262064  -109.026  &lt; 2e-16 ***\nDESTIN_SZJESZ01      -0.0843635  0.0048222   -17.495  &lt; 2e-16 ***\nDESTIN_SZJESZ02      -0.5197682  0.0051511  -100.904  &lt; 2e-16 ***\nDESTIN_SZJESZ03      -0.6250311  0.0056619  -110.392  &lt; 2e-16 ***\nDESTIN_SZJESZ04      -0.3937360  0.0065536   -60.080  &lt; 2e-16 ***\nDESTIN_SZJESZ05      -0.9748291  0.0097665   -99.814  &lt; 2e-16 ***\nDESTIN_SZJESZ06       0.3642736  0.0040600    89.722  &lt; 2e-16 ***\nDESTIN_SZJESZ07      -1.1571882  0.0081557  -141.887  &lt; 2e-16 ***\nDESTIN_SZJESZ08      -0.5955747  0.0078071   -76.286  &lt; 2e-16 ***\nDESTIN_SZJESZ09      -0.3629500  0.0053966   -67.256  &lt; 2e-16 ***\nDESTIN_SZJESZ10       0.7691552  0.0069348   110.912  &lt; 2e-16 ***\nDESTIN_SZJESZ11       0.9365743  0.0065801   142.335  &lt; 2e-16 ***\nDESTIN_SZJWSZ01      -0.4568805  0.0064536   -70.795  &lt; 2e-16 ***\nDESTIN_SZJWSZ02      -0.2880426  0.0051632   -55.788  &lt; 2e-16 ***\nDESTIN_SZJWSZ03       0.6680404  0.0039264   170.142  &lt; 2e-16 ***\nDESTIN_SZJWSZ04       0.9492158  0.0037186   255.262  &lt; 2e-16 ***\nDESTIN_SZJWSZ05      -0.1938053  0.0060810   -31.871  &lt; 2e-16 ***\nDESTIN_SZJWSZ06       0.3813164  0.0054551    69.900  &lt; 2e-16 ***\nDESTIN_SZJWSZ07      -1.2676010  0.0280038   -45.265  &lt; 2e-16 ***\nDESTIN_SZJWSZ08       0.5013149  0.0044573   112.471  &lt; 2e-16 ***\nDESTIN_SZJWSZ09       1.4161404  0.0033937   417.291  &lt; 2e-16 ***\nDESTIN_SZKLSZ01      -0.6909444  0.0051540  -134.059  &lt; 2e-16 ***\nDESTIN_SZKLSZ02      -0.8146023  0.0057129  -142.589  &lt; 2e-16 ***\nDESTIN_SZKLSZ03      -1.3956114  0.0065167  -214.161  &lt; 2e-16 ***\nDESTIN_SZKLSZ04      -1.9070281  0.0087370  -218.270  &lt; 2e-16 ***\nDESTIN_SZKLSZ05      -0.9293576  0.0071070  -130.766  &lt; 2e-16 ***\nDESTIN_SZKLSZ06      -2.5402234  0.0362062   -70.160  &lt; 2e-16 ***\nDESTIN_SZKLSZ07      -1.2017213  0.0065751  -182.769  &lt; 2e-16 ***\nDESTIN_SZKLSZ08      -0.6083433  0.0050916  -119.480  &lt; 2e-16 ***\nDESTIN_SZLKSZ01      -1.5186810  0.0204155   -74.389  &lt; 2e-16 ***\nDESTIN_SZMDSZ01      -1.4601772  0.0198347   -73.617  &lt; 2e-16 ***\nDESTIN_SZMDSZ02      -1.1554609  0.0111345  -103.773  &lt; 2e-16 ***\nDESTIN_SZMDSZ03      -2.9919337  0.0250838  -119.277  &lt; 2e-16 ***\nDESTIN_SZMPSZ01      -1.1705809  0.0077128  -151.771  &lt; 2e-16 ***\nDESTIN_SZMPSZ02      -0.9380957  0.0060321  -155.517  &lt; 2e-16 ***\nDESTIN_SZMPSZ03      -0.1761013  0.0046389   -37.962  &lt; 2e-16 ***\nDESTIN_SZMUSZ02      -2.4525115  0.0199630  -122.853  &lt; 2e-16 ***\nDESTIN_SZNTSZ01      -3.6605524  0.0447752   -81.754  &lt; 2e-16 ***\nDESTIN_SZNTSZ02      -2.0082021  0.0108736  -184.686  &lt; 2e-16 ***\nDESTIN_SZNTSZ03      -1.2387489  0.0076141  -162.691  &lt; 2e-16 ***\nDESTIN_SZNTSZ05      -1.8054361  0.0249540   -72.351  &lt; 2e-16 ***\nDESTIN_SZNTSZ06      -2.9500517  0.0428601   -68.830  &lt; 2e-16 ***\nDESTIN_SZNVSZ01      -0.4089022  0.0044288   -92.327  &lt; 2e-16 ***\nDESTIN_SZNVSZ02      -0.6865452  0.0052770  -130.102  &lt; 2e-16 ***\nDESTIN_SZNVSZ03      -0.7333670  0.0054243  -135.199  &lt; 2e-16 ***\nDESTIN_SZNVSZ04      -2.2095097  0.0106997  -206.503  &lt; 2e-16 ***\nDESTIN_SZNVSZ05      -1.8721104  0.0089058  -210.212  &lt; 2e-16 ***\nDESTIN_SZPGSZ01      -1.8756618  0.0153008  -122.586  &lt; 2e-16 ***\nDESTIN_SZPGSZ02      -0.9435337  0.0067224  -140.356  &lt; 2e-16 ***\nDESTIN_SZPGSZ03       0.3458476  0.0040152    86.134  &lt; 2e-16 ***\nDESTIN_SZPGSZ04      -0.0271485  0.0044805    -6.059 1.37e-09 ***\nDESTIN_SZPGSZ05      -0.8920273  0.0070730  -126.117  &lt; 2e-16 ***\nDESTIN_SZPLSZ01      -0.2153087  0.0068270   -31.538  &lt; 2e-16 ***\nDESTIN_SZPLSZ02      -1.3646116  0.0131155  -104.046  &lt; 2e-16 ***\nDESTIN_SZPLSZ03      -0.0869245  0.0095838    -9.070  &lt; 2e-16 ***\nDESTIN_SZPLSZ04      -0.2574560  0.0093336   -27.584  &lt; 2e-16 ***\nDESTIN_SZPLSZ05      -0.7186364  0.0116835   -61.509  &lt; 2e-16 ***\nDESTIN_SZPNSZ01       1.1326963  0.0049977   226.643  &lt; 2e-16 ***\nDESTIN_SZPNSZ02       1.6516855  0.0064492   256.106  &lt; 2e-16 ***\nDESTIN_SZPNSZ03       0.8504093  0.0077034   110.394  &lt; 2e-16 ***\nDESTIN_SZPNSZ04       1.6891381  0.0075802   222.836  &lt; 2e-16 ***\nDESTIN_SZPNSZ05       0.7402750  0.0115948    63.845  &lt; 2e-16 ***\nDESTIN_SZPRSZ01      -1.0257636  0.0084652  -121.175  &lt; 2e-16 ***\nDESTIN_SZPRSZ02      -0.2028503  0.0049839   -40.701  &lt; 2e-16 ***\nDESTIN_SZPRSZ03       0.5560483  0.0038496   144.442  &lt; 2e-16 ***\nDESTIN_SZPRSZ04      -0.6824142  0.0079047   -86.330  &lt; 2e-16 ***\nDESTIN_SZPRSZ05       0.0316117  0.0044946     7.033 2.02e-12 ***\nDESTIN_SZPRSZ06       0.3706283  0.0052006    71.267  &lt; 2e-16 ***\nDESTIN_SZPRSZ07      -1.4740460  0.0117304  -125.661  &lt; 2e-16 ***\nDESTIN_SZPRSZ08      -0.7869180  0.0064862  -121.321  &lt; 2e-16 ***\nDESTIN_SZQTSZ01      -1.2790095  0.0085392  -149.781  &lt; 2e-16 ***\nDESTIN_SZQTSZ02      -1.4989188  0.0073423  -204.149  &lt; 2e-16 ***\nDESTIN_SZQTSZ03      -0.9334132  0.0064035  -145.765  &lt; 2e-16 ***\nDESTIN_SZQTSZ04      -1.0506142  0.0065335  -160.805  &lt; 2e-16 ***\nDESTIN_SZQTSZ05      -0.9765013  0.0058471  -167.006  &lt; 2e-16 ***\nDESTIN_SZQTSZ06      -1.2206088  0.0063560  -192.042  &lt; 2e-16 ***\nDESTIN_SZQTSZ07      -1.6794007  0.0108727  -154.460  &lt; 2e-16 ***\nDESTIN_SZQTSZ08      -0.1214413  0.0047980   -25.311  &lt; 2e-16 ***\nDESTIN_SZQTSZ09      -0.5252607  0.0057371   -91.555  &lt; 2e-16 ***\nDESTIN_SZQTSZ10      -0.5981644  0.0054192  -110.378  &lt; 2e-16 ***\nDESTIN_SZQTSZ11      -0.0766021  0.0053446   -14.333  &lt; 2e-16 ***\nDESTIN_SZQTSZ12      -0.6153017  0.0070680   -87.054  &lt; 2e-16 ***\nDESTIN_SZQTSZ13      -0.1690535  0.0051315   -32.944  &lt; 2e-16 ***\nDESTIN_SZQTSZ14      -0.5398362  0.0062233   -86.744  &lt; 2e-16 ***\nDESTIN_SZQTSZ15      -0.1873015  0.0073132   -25.611  &lt; 2e-16 ***\nDESTIN_SZRCSZ01      -0.5875494  0.0071798   -81.833  &lt; 2e-16 ***\nDESTIN_SZRCSZ06      -2.0856090  0.0188789  -110.473  &lt; 2e-16 ***\nDESTIN_SZRVSZ01      -2.6183708  0.0162319  -161.310  &lt; 2e-16 ***\nDESTIN_SZRVSZ02      -3.1882190  0.0326141   -97.756  &lt; 2e-16 ***\nDESTIN_SZRVSZ03      -2.5981974  0.0135074  -192.353  &lt; 2e-16 ***\nDESTIN_SZRVSZ04      -1.9741504  0.0154961  -127.396  &lt; 2e-16 ***\nDESTIN_SZRVSZ05      -3.1547734  0.0256310  -123.084  &lt; 2e-16 ***\nDESTIN_SZSBSZ01      -0.3097949  0.0060601   -51.121  &lt; 2e-16 ***\nDESTIN_SZSBSZ02      -1.1229132  0.0076338  -147.097  &lt; 2e-16 ***\nDESTIN_SZSBSZ03       0.6289715  0.0041400   151.926  &lt; 2e-16 ***\nDESTIN_SZSBSZ04       0.1419430  0.0051357    27.638  &lt; 2e-16 ***\nDESTIN_SZSBSZ05      -0.9256413  0.0071963  -128.628  &lt; 2e-16 ***\nDESTIN_SZSBSZ06      -2.3487368  0.0221611  -105.984  &lt; 2e-16 ***\nDESTIN_SZSBSZ07      -0.7864630  0.0181706   -43.282  &lt; 2e-16 ***\nDESTIN_SZSBSZ08       1.3240051  0.0051598   256.599  &lt; 2e-16 ***\nDESTIN_SZSBSZ09       0.8431156  0.0048330   174.449  &lt; 2e-16 ***\nDESTIN_SZSESZ02      -0.2385874  0.0046618   -51.180  &lt; 2e-16 ***\nDESTIN_SZSESZ03       0.5439188  0.0036932   147.276  &lt; 2e-16 ***\nDESTIN_SZSESZ04      -0.6715716  0.0054222  -123.856  &lt; 2e-16 ***\nDESTIN_SZSESZ05      -0.3601932  0.0047508   -75.818  &lt; 2e-16 ***\nDESTIN_SZSESZ06      -0.6088413  0.0057017  -106.782  &lt; 2e-16 ***\nDESTIN_SZSESZ07      -2.9477507  0.0226797  -129.973  &lt; 2e-16 ***\nDESTIN_SZSGSZ01      -0.5100640  0.0058280   -87.519  &lt; 2e-16 ***\nDESTIN_SZSGSZ02      -0.0439941  0.0051633    -8.520  &lt; 2e-16 ***\nDESTIN_SZSGSZ03      -0.3700648  0.0047152   -78.483  &lt; 2e-16 ***\nDESTIN_SZSGSZ04      -0.3021335  0.0046865   -64.468  &lt; 2e-16 ***\nDESTIN_SZSGSZ05      -2.2253287  0.0097908  -227.288  &lt; 2e-16 ***\nDESTIN_SZSGSZ06       0.2963602  0.0037948    78.097  &lt; 2e-16 ***\nDESTIN_SZSGSZ07      -0.5940373  0.0051371  -115.637  &lt; 2e-16 ***\nDESTIN_SZSISZ01      -1.4528976  0.0257790   -56.360  &lt; 2e-16 ***\nDESTIN_SZSKSZ01      -0.0374952  0.0066885    -5.606 2.07e-08 ***\nDESTIN_SZSKSZ02       0.7271418  0.0050281   144.617  &lt; 2e-16 ***\nDESTIN_SZSKSZ03      -0.0640794  0.0059146   -10.834  &lt; 2e-16 ***\nDESTIN_SZSKSZ04      -0.5610767  0.0139676   -40.170  &lt; 2e-16 ***\nDESTIN_SZSKSZ05       0.1510974  0.0104871    14.408  &lt; 2e-16 ***\nDESTIN_SZSLSZ01      -0.5823031  0.0083356   -69.858  &lt; 2e-16 ***\nDESTIN_SZSLSZ04      -0.8166665  0.0070329  -116.122  &lt; 2e-16 ***\nDESTIN_SZSRSZ01      -2.3241796  0.0127215  -182.696  &lt; 2e-16 ***\nDESTIN_SZTHSZ01      -2.8157635  0.0366840   -76.757  &lt; 2e-16 ***\nDESTIN_SZTHSZ03      -2.1005978  0.0250842   -83.742  &lt; 2e-16 ***\nDESTIN_SZTHSZ04      -2.1246250  0.0213690   -99.425  &lt; 2e-16 ***\nDESTIN_SZTHSZ06      -1.4571092  0.0150031   -97.121  &lt; 2e-16 ***\nDESTIN_SZTMSZ01      -0.1234559  0.0055152   -22.385  &lt; 2e-16 ***\nDESTIN_SZTMSZ02       1.5961628  0.0032599   489.635  &lt; 2e-16 ***\nDESTIN_SZTMSZ03       0.6977233  0.0037138   187.875  &lt; 2e-16 ***\nDESTIN_SZTMSZ04       0.8606606  0.0037592   228.947  &lt; 2e-16 ***\nDESTIN_SZTMSZ05       0.3750655  0.0051281    73.140  &lt; 2e-16 ***\nDESTIN_SZTNSZ01      -1.2624562  0.0066979  -188.485  &lt; 2e-16 ***\nDESTIN_SZTNSZ02      -2.0761581  0.0096538  -215.062  &lt; 2e-16 ***\nDESTIN_SZTNSZ03      -2.1128125  0.0115717  -182.584  &lt; 2e-16 ***\nDESTIN_SZTNSZ04      -1.2417494  0.0068502  -181.271  &lt; 2e-16 ***\nDESTIN_SZTPSZ01      -0.7094356  0.0055768  -127.211  &lt; 2e-16 ***\nDESTIN_SZTPSZ02       0.1491604  0.0037260    40.032  &lt; 2e-16 ***\nDESTIN_SZTPSZ03      -0.4973355  0.0054878   -90.626  &lt; 2e-16 ***\nDESTIN_SZTPSZ04      -1.5160395  0.0071592  -211.761  &lt; 2e-16 ***\nDESTIN_SZTPSZ05      -0.9196565  0.0056750  -162.054  &lt; 2e-16 ***\nDESTIN_SZTPSZ06      -0.2710649  0.0062637   -43.276  &lt; 2e-16 ***\nDESTIN_SZTPSZ07      -2.0198681  0.0116556  -173.296  &lt; 2e-16 ***\nDESTIN_SZTPSZ08      -1.4881412  0.0085532  -173.987  &lt; 2e-16 ***\nDESTIN_SZTPSZ09      -0.5901273  0.0059394   -99.358  &lt; 2e-16 ***\nDESTIN_SZTPSZ10      -1.1215711  0.0084488  -132.749  &lt; 2e-16 ***\nDESTIN_SZTPSZ11      -0.4837089  0.0050905   -95.022  &lt; 2e-16 ***\nDESTIN_SZTPSZ12      -0.8653927  0.0061326  -141.113  &lt; 2e-16 ***\nDESTIN_SZTSSZ01      -0.5515103  0.0208541   -26.446  &lt; 2e-16 ***\nDESTIN_SZTSSZ02       0.8373778  0.0093757    89.314  &lt; 2e-16 ***\nDESTIN_SZTSSZ03       1.7021888  0.0064394   264.340  &lt; 2e-16 ***\nDESTIN_SZTSSZ04       1.5355016  0.0067855   226.292  &lt; 2e-16 ***\nDESTIN_SZTSSZ05       1.6932319  0.0073725   229.668  &lt; 2e-16 ***\nDESTIN_SZTSSZ06       0.4567808  0.0137927    33.118  &lt; 2e-16 ***\nDESTIN_SZWCSZ01       1.3967640  0.0045392   307.711  &lt; 2e-16 ***\nDESTIN_SZWCSZ02      -0.4560229  0.0122949   -37.090  &lt; 2e-16 ***\nDESTIN_SZWCSZ03      -2.0710051  0.0325121   -63.699  &lt; 2e-16 ***\nDESTIN_SZWDSZ01       1.5137342  0.0034774   435.310  &lt; 2e-16 ***\nDESTIN_SZWDSZ02      -0.3005475  0.0055149   -54.497  &lt; 2e-16 ***\nDESTIN_SZWDSZ03       1.2514543  0.0036112   346.550  &lt; 2e-16 ***\nDESTIN_SZWDSZ04      -0.1702528  0.0058295   -29.205  &lt; 2e-16 ***\nDESTIN_SZWDSZ05      -0.0005419  0.0053911    -0.101  0.91994    \nDESTIN_SZWDSZ06       0.5203361  0.0040318   129.058  &lt; 2e-16 ***\nDESTIN_SZWDSZ07       0.6006472  0.0061745    97.279  &lt; 2e-16 ***\nDESTIN_SZWDSZ08       0.6650867  0.0060867   109.268  &lt; 2e-16 ***\nDESTIN_SZWDSZ09       0.6237312  0.0044830   139.132  &lt; 2e-16 ***\nDESTIN_SZYSSZ01       1.0471638  0.0038255   273.732  &lt; 2e-16 ***\nDESTIN_SZYSSZ02       0.2341114  0.0048213    48.558  &lt; 2e-16 ***\nDESTIN_SZYSSZ03      -0.0916446  0.0051335   -17.852  &lt; 2e-16 ***\nDESTIN_SZYSSZ04      -0.0085536  0.0048684    -1.757  0.07892 .  \nDESTIN_SZYSSZ05      -1.5775071  0.0100297  -157.283  &lt; 2e-16 ***\nDESTIN_SZYSSZ06      -1.8130307  0.0098617  -183.846  &lt; 2e-16 ***\nDESTIN_SZYSSZ07      -1.1703963  0.0111525  -104.945  &lt; 2e-16 ***\nDESTIN_SZYSSZ08       0.5253514  0.0039556   132.813  &lt; 2e-16 ***\nDESTIN_SZYSSZ09       0.4353435  0.0038890   111.943  &lt; 2e-16 ***\nlog(ORIGIN_AGE25_64)  0.2249135  0.0001404  1602.353  &lt; 2e-16 ***\nlog(dist)            -0.6989356  0.0001287 -5431.279  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 60796037  on 14733  degrees of freedom\nResidual deviance: 26208384  on 14452  degrees of freedom\nAIC: 26300575\n\nNumber of Fisher Scoring iterations: 7\n\n\nWe can compute for the R-squared with the following\n\nCalcRSquared(decSIM$data$TRIPS, decSIM$fitted.values)\n\n[1] 0.4972985\n\n\n\n\nDoubly constrained SIM\nThe code chunk below calibrates a doubly constrained SIM\n\ndbcSIM &lt;- glm(formula = TRIPS ~ \n                ORIGIN_SZ + \n                DESTIN_SZ + \n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nsummary(dbcSIM)\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + DESTIN_SZ + log(dist), family = poisson(link = \"log\"), \n    data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                  Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)     12.4165310  0.0043949  2825.242  &lt; 2e-16 ***\nORIGIN_SZAMSZ02  0.9496891  0.0045740   207.630  &lt; 2e-16 ***\nORIGIN_SZAMSZ03  0.5519174  0.0046672   118.253  &lt; 2e-16 ***\nORIGIN_SZAMSZ04  0.1028140  0.0052468    19.596  &lt; 2e-16 ***\nORIGIN_SZAMSZ05  0.0822549  0.0058663    14.022  &lt; 2e-16 ***\nORIGIN_SZAMSZ06  0.6617809  0.0052580   125.861  &lt; 2e-16 ***\nORIGIN_SZAMSZ07 -0.9508298  0.0097681   -97.340  &lt; 2e-16 ***\nORIGIN_SZAMSZ08 -0.7271779  0.0090946   -79.958  &lt; 2e-16 ***\nORIGIN_SZAMSZ09  0.4896781  0.0055203    88.704  &lt; 2e-16 ***\nORIGIN_SZAMSZ10  0.4819428  0.0048175   100.040  &lt; 2e-16 ***\nORIGIN_SZAMSZ11 -1.7719841  0.0130695  -135.582  &lt; 2e-16 ***\nORIGIN_SZAMSZ12 -1.7679107  0.0108777  -162.526  &lt; 2e-16 ***\nORIGIN_SZBDSZ01  0.8314812  0.0045187   184.010  &lt; 2e-16 ***\nORIGIN_SZBDSZ02  0.4305836  0.0052535    81.961  &lt; 2e-16 ***\nORIGIN_SZBDSZ03  0.8009370  0.0046384   172.676  &lt; 2e-16 ***\nORIGIN_SZBDSZ04  1.4562985  0.0040456   359.971  &lt; 2e-16 ***\nORIGIN_SZBDSZ05  0.4501939  0.0046960    95.867  &lt; 2e-16 ***\nORIGIN_SZBDSZ06  0.7745026  0.0047424   163.314  &lt; 2e-16 ***\nORIGIN_SZBDSZ07 -1.1784123  0.0098105  -120.117  &lt; 2e-16 ***\nORIGIN_SZBDSZ08 -0.9830996  0.0091135  -107.873  &lt; 2e-16 ***\nORIGIN_SZBKSZ01 -0.3042966  0.0067086   -45.359  &lt; 2e-16 ***\nORIGIN_SZBKSZ02  0.4801541  0.0054160    88.655  &lt; 2e-16 ***\nORIGIN_SZBKSZ03  0.7823931  0.0052007   150.440  &lt; 2e-16 ***\nORIGIN_SZBKSZ04 -0.1292545  0.0061735   -20.937  &lt; 2e-16 ***\nORIGIN_SZBKSZ05 -0.0258584  0.0060192    -4.296 1.74e-05 ***\nORIGIN_SZBKSZ06  0.1994719  0.0061206    32.590  &lt; 2e-16 ***\nORIGIN_SZBKSZ07  0.7434860  0.0046598   159.553  &lt; 2e-16 ***\nORIGIN_SZBKSZ08  0.1625007  0.0055219    29.428  &lt; 2e-16 ***\nORIGIN_SZBKSZ09 -0.0864293  0.0059533   -14.518  &lt; 2e-16 ***\nORIGIN_SZBLSZ01 -2.1022485  0.0150316  -139.855  &lt; 2e-16 ***\nORIGIN_SZBLSZ02 -2.9460181  0.0195760  -150.491  &lt; 2e-16 ***\nORIGIN_SZBLSZ03 -4.9412872  0.0398540  -123.985  &lt; 2e-16 ***\nORIGIN_SZBLSZ04 -2.8143593  0.0239209  -117.653  &lt; 2e-16 ***\nORIGIN_SZBMSZ01 -0.0264561  0.0053639    -4.932 8.13e-07 ***\nORIGIN_SZBMSZ02 -0.8656513  0.0068511  -126.353  &lt; 2e-16 ***\nORIGIN_SZBMSZ03 -0.1723467  0.0059613   -28.911  &lt; 2e-16 ***\nORIGIN_SZBMSZ04  0.2169844  0.0053578    40.499  &lt; 2e-16 ***\nORIGIN_SZBMSZ05 -2.0252956  0.0126107  -160.602  &lt; 2e-16 ***\nORIGIN_SZBMSZ06 -1.7642018  0.0163931  -107.619  &lt; 2e-16 ***\nORIGIN_SZBMSZ07 -0.3271629  0.0058137   -56.274  &lt; 2e-16 ***\nORIGIN_SZBMSZ08 -0.2533255  0.0059335   -42.694  &lt; 2e-16 ***\nORIGIN_SZBMSZ09 -0.7712635  0.0087939   -87.704  &lt; 2e-16 ***\nORIGIN_SZBMSZ10 -1.0098048  0.0092519  -109.145  &lt; 2e-16 ***\nORIGIN_SZBMSZ11 -0.3816187  0.0067302   -56.702  &lt; 2e-16 ***\nORIGIN_SZBMSZ12 -0.6666616  0.0095680   -69.676  &lt; 2e-16 ***\nORIGIN_SZBMSZ13 -0.0076108  0.0059040    -1.289  0.19737    \nORIGIN_SZBMSZ14 -0.1682476  0.0069391   -24.246  &lt; 2e-16 ***\nORIGIN_SZBMSZ15  0.0904585  0.0062822    14.399  &lt; 2e-16 ***\nORIGIN_SZBMSZ16 -1.1808741  0.0092258  -127.997  &lt; 2e-16 ***\nORIGIN_SZBMSZ17 -1.7189127  0.0158408  -108.512  &lt; 2e-16 ***\nORIGIN_SZBPSZ01  0.4294645  0.0058051    73.980  &lt; 2e-16 ***\nORIGIN_SZBPSZ02  0.5028906  0.0068169    73.771  &lt; 2e-16 ***\nORIGIN_SZBPSZ03  0.6656178  0.0066126   100.658  &lt; 2e-16 ***\nORIGIN_SZBPSZ04  0.5203612  0.0053224    97.769  &lt; 2e-16 ***\nORIGIN_SZBPSZ05  0.5377769  0.0047907   112.256  &lt; 2e-16 ***\nORIGIN_SZBPSZ06 -1.2327809  0.0094950  -129.835  &lt; 2e-16 ***\nORIGIN_SZBPSZ07 -0.9035255  0.0088739  -101.818  &lt; 2e-16 ***\nORIGIN_SZBSSZ01  0.1210027  0.0053990    22.412  &lt; 2e-16 ***\nORIGIN_SZBSSZ02  0.4618449  0.0048641    94.951  &lt; 2e-16 ***\nORIGIN_SZBSSZ03  0.2160739  0.0047835    45.170  &lt; 2e-16 ***\nORIGIN_SZBTSZ01 -0.1108042  0.0055599   -19.929  &lt; 2e-16 ***\nORIGIN_SZBTSZ02 -0.8911221  0.0079213  -112.498  &lt; 2e-16 ***\nORIGIN_SZBTSZ03 -0.2203980  0.0059325   -37.151  &lt; 2e-16 ***\nORIGIN_SZBTSZ04 -0.6427946  0.0105438   -60.964  &lt; 2e-16 ***\nORIGIN_SZBTSZ05 -1.4662312  0.0111784  -131.166  &lt; 2e-16 ***\nORIGIN_SZBTSZ06 -0.6105884  0.0073456   -83.123  &lt; 2e-16 ***\nORIGIN_SZBTSZ07 -1.9041317  0.0132781  -143.404  &lt; 2e-16 ***\nORIGIN_SZBTSZ08 -1.0627939  0.0095982  -110.728  &lt; 2e-16 ***\nORIGIN_SZCBSZ01 -2.9365941  0.0548632   -53.526  &lt; 2e-16 ***\nORIGIN_SZCCSZ01 -1.5313555  0.0134599  -113.772  &lt; 2e-16 ***\nORIGIN_SZCHSZ01 -1.2034494  0.0119468  -100.734  &lt; 2e-16 ***\nORIGIN_SZCHSZ02 -0.8299415  0.0081984  -101.232  &lt; 2e-16 ***\nORIGIN_SZCHSZ03 -0.5143946  0.0061944   -83.042  &lt; 2e-16 ***\nORIGIN_SZCKSZ01  0.2372583  0.0053612    44.255  &lt; 2e-16 ***\nORIGIN_SZCKSZ02  0.9124836  0.0054472   167.515  &lt; 2e-16 ***\nORIGIN_SZCKSZ03  0.7237808  0.0048401   149.539  &lt; 2e-16 ***\nORIGIN_SZCKSZ04  1.6884022  0.0050169   336.540  &lt; 2e-16 ***\nORIGIN_SZCKSZ05  1.3932005  0.0062346   223.464  &lt; 2e-16 ***\nORIGIN_SZCKSZ06  1.0670053  0.0066112   161.394  &lt; 2e-16 ***\nORIGIN_SZCLSZ01 -0.8602837  0.0079240  -108.567  &lt; 2e-16 ***\nORIGIN_SZCLSZ02 -1.3853421  0.0137444  -100.793  &lt; 2e-16 ***\nORIGIN_SZCLSZ03 -0.8582608  0.0081177  -105.727  &lt; 2e-16 ***\nORIGIN_SZCLSZ04  0.7836027  0.0046427   168.782  &lt; 2e-16 ***\nORIGIN_SZCLSZ05 -1.8121756  0.0148960  -121.655  &lt; 2e-16 ***\nORIGIN_SZCLSZ06  0.8296870  0.0043909   188.955  &lt; 2e-16 ***\nORIGIN_SZCLSZ07 -0.2325219  0.0057432   -40.487  &lt; 2e-16 ***\nORIGIN_SZCLSZ08  0.2714336  0.0062625    43.342  &lt; 2e-16 ***\nORIGIN_SZCLSZ09 -2.2223744  0.0160946  -138.082  &lt; 2e-16 ***\nORIGIN_SZDTSZ02 -4.0704970  0.0834192   -48.796  &lt; 2e-16 ***\nORIGIN_SZDTSZ03 -3.4529031  0.0738295   -46.769  &lt; 2e-16 ***\nORIGIN_SZDTSZ13 -2.8301983  0.0313085   -90.397  &lt; 2e-16 ***\nORIGIN_SZGLSZ01 -1.4674986  0.0093137  -157.563  &lt; 2e-16 ***\nORIGIN_SZGLSZ02  0.2749369  0.0050051    54.931  &lt; 2e-16 ***\nORIGIN_SZGLSZ03  0.0781954  0.0049748    15.718  &lt; 2e-16 ***\nORIGIN_SZGLSZ04  0.8167797  0.0043260   188.808  &lt; 2e-16 ***\nORIGIN_SZGLSZ05  0.5277509  0.0044879   117.595  &lt; 2e-16 ***\nORIGIN_SZHGSZ01  0.2323885  0.0048555    47.861  &lt; 2e-16 ***\nORIGIN_SZHGSZ02  0.5707182  0.0048256   118.268  &lt; 2e-16 ***\nORIGIN_SZHGSZ03  0.4231170  0.0052149    81.136  &lt; 2e-16 ***\nORIGIN_SZHGSZ04  0.9341168  0.0044128   211.681  &lt; 2e-16 ***\nORIGIN_SZHGSZ05  1.2192790  0.0043790   278.437  &lt; 2e-16 ***\nORIGIN_SZHGSZ06  0.0490041  0.0054961     8.916  &lt; 2e-16 ***\nORIGIN_SZHGSZ07  0.6337041  0.0045735   138.559  &lt; 2e-16 ***\nORIGIN_SZHGSZ08  0.0312612  0.0054684     5.717 1.09e-08 ***\nORIGIN_SZHGSZ09 -0.6985397  0.0071800   -97.289  &lt; 2e-16 ***\nORIGIN_SZHGSZ10 -2.9958967  0.0422303   -70.942  &lt; 2e-16 ***\nORIGIN_SZJESZ01  0.4363431  0.0051329    85.010  &lt; 2e-16 ***\nORIGIN_SZJESZ02  0.3460900  0.0051372    67.370  &lt; 2e-16 ***\nORIGIN_SZJESZ03  0.2928005  0.0055108    53.132  &lt; 2e-16 ***\nORIGIN_SZJESZ04 -1.1924298  0.0093540  -127.478  &lt; 2e-16 ***\nORIGIN_SZJESZ05 -2.0178136  0.0139479  -144.668  &lt; 2e-16 ***\nORIGIN_SZJESZ06  0.1637633  0.0050685    32.310  &lt; 2e-16 ***\nORIGIN_SZJESZ07 -1.8227460  0.0119383  -152.680  &lt; 2e-16 ***\nORIGIN_SZJESZ08 -1.1556281  0.0117870   -98.043  &lt; 2e-16 ***\nORIGIN_SZJESZ09  0.4766229  0.0052813    90.248  &lt; 2e-16 ***\nORIGIN_SZJESZ10 -2.6868992  0.0186864  -143.789  &lt; 2e-16 ***\nORIGIN_SZJESZ11 -3.0618150  0.0199755  -153.278  &lt; 2e-16 ***\nORIGIN_SZJWSZ01  0.4417418  0.0068611    64.383  &lt; 2e-16 ***\nORIGIN_SZJWSZ02  0.9738087  0.0047905   203.281  &lt; 2e-16 ***\nORIGIN_SZJWSZ03  1.1548028  0.0045180   255.599  &lt; 2e-16 ***\nORIGIN_SZJWSZ04  0.9078417  0.0046668   194.532  &lt; 2e-16 ***\nORIGIN_SZJWSZ05 -1.7092500  0.0127422  -134.141  &lt; 2e-16 ***\nORIGIN_SZJWSZ06 -1.3284287  0.0109785  -121.002  &lt; 2e-16 ***\nORIGIN_SZJWSZ07 -2.3231549  0.0281427   -82.549  &lt; 2e-16 ***\nORIGIN_SZJWSZ08  1.9386127  0.0046041   421.059  &lt; 2e-16 ***\nORIGIN_SZJWSZ09  1.3987549  0.0042610   328.266  &lt; 2e-16 ***\nORIGIN_SZKLSZ01  0.2617735  0.0050089    52.261  &lt; 2e-16 ***\nORIGIN_SZKLSZ02 -0.4325093  0.0064279   -67.286  &lt; 2e-16 ***\nORIGIN_SZKLSZ03 -0.2787173  0.0060380   -46.161  &lt; 2e-16 ***\nORIGIN_SZKLSZ04 -1.9432693  0.0119163  -163.076  &lt; 2e-16 ***\nORIGIN_SZKLSZ05 -0.5420067  0.0085529   -63.371  &lt; 2e-16 ***\nORIGIN_SZKLSZ06 -4.2949009  0.1857686   -23.120  &lt; 2e-16 ***\nORIGIN_SZKLSZ07 -0.8576946  0.0085178  -100.694  &lt; 2e-16 ***\nORIGIN_SZKLSZ08 -1.3840925  0.0092323  -149.918  &lt; 2e-16 ***\nORIGIN_SZLKSZ01 -2.8108510  0.0392356   -71.640  &lt; 2e-16 ***\nORIGIN_SZMDSZ01 -1.6745388  0.0296543   -56.469  &lt; 2e-16 ***\nORIGIN_SZMDSZ02 -0.8193738  0.0106631   -76.842  &lt; 2e-16 ***\nORIGIN_SZMDSZ03 -1.5088267  0.0172032   -87.706  &lt; 2e-16 ***\nORIGIN_SZMPSZ01 -0.9860154  0.0085053  -115.929  &lt; 2e-16 ***\nORIGIN_SZMPSZ02 -0.5958875  0.0070097   -85.008  &lt; 2e-16 ***\nORIGIN_SZMPSZ03 -0.0490122  0.0054582    -8.980  &lt; 2e-16 ***\nORIGIN_SZMUSZ02 -3.5233367  0.1037749   -33.952  &lt; 2e-16 ***\nORIGIN_SZNTSZ01 -2.6451541  0.0353125   -74.907  &lt; 2e-16 ***\nORIGIN_SZNTSZ02 -2.7710546  0.0232841  -119.011  &lt; 2e-16 ***\nORIGIN_SZNTSZ03 -0.6123404  0.0079083   -77.430  &lt; 2e-16 ***\nORIGIN_SZNTSZ05 -2.9257445  0.0496704   -58.903  &lt; 2e-16 ***\nORIGIN_SZNTSZ06 -3.3260031  0.0557966   -59.609  &lt; 2e-16 ***\nORIGIN_SZNVSZ01  0.6421306  0.0046037   139.482  &lt; 2e-16 ***\nORIGIN_SZNVSZ02 -0.4251550  0.0065890   -64.525  &lt; 2e-16 ***\nORIGIN_SZNVSZ03 -1.0765622  0.0078766  -136.679  &lt; 2e-16 ***\nORIGIN_SZNVSZ04 -1.2289504  0.0091468  -134.358  &lt; 2e-16 ***\nORIGIN_SZNVSZ05 -2.3551389  0.0158219  -148.853  &lt; 2e-16 ***\nORIGIN_SZPGSZ01  0.1518212  0.0154825     9.806  &lt; 2e-16 ***\nORIGIN_SZPGSZ02 -0.4062609  0.0073780   -55.064  &lt; 2e-16 ***\nORIGIN_SZPGSZ03  0.8976913  0.0046122   194.636  &lt; 2e-16 ***\nORIGIN_SZPGSZ04  1.1161685  0.0045850   243.437  &lt; 2e-16 ***\nORIGIN_SZPGSZ05  0.4794249  0.0060213    79.621  &lt; 2e-16 ***\nORIGIN_SZPLSZ01 -0.8322377  0.0107898   -77.132  &lt; 2e-16 ***\nORIGIN_SZPLSZ02 -1.2968937  0.0149841   -86.551  &lt; 2e-16 ***\nORIGIN_SZPLSZ03 -3.2744991  0.0374541   -87.427  &lt; 2e-16 ***\nORIGIN_SZPLSZ04 -3.5423615  0.0372570   -95.079  &lt; 2e-16 ***\nORIGIN_SZPLSZ05 -2.4343705  0.0227807  -106.861  &lt; 2e-16 ***\nORIGIN_SZPNSZ01  0.8052461  0.0056124   143.476  &lt; 2e-16 ***\nORIGIN_SZPNSZ02 -1.8042362  0.0128222  -140.712  &lt; 2e-16 ***\nORIGIN_SZPNSZ03 -2.6363996  0.0200058  -131.782  &lt; 2e-16 ***\nORIGIN_SZPNSZ04 -4.8427070  0.0320126  -151.275  &lt; 2e-16 ***\nORIGIN_SZPNSZ05 -3.6613775  0.0285686  -128.161  &lt; 2e-16 ***\nORIGIN_SZPRSZ01 -0.5645384  0.0117126   -48.199  &lt; 2e-16 ***\nORIGIN_SZPRSZ02  0.9145886  0.0048137   189.998  &lt; 2e-16 ***\nORIGIN_SZPRSZ03  0.4478971  0.0048102    93.113  &lt; 2e-16 ***\nORIGIN_SZPRSZ04 -0.5312444  0.0079019   -67.230  &lt; 2e-16 ***\nORIGIN_SZPRSZ05  1.1462662  0.0045250   253.318  &lt; 2e-16 ***\nORIGIN_SZPRSZ06 -0.7392744  0.0090347   -81.826  &lt; 2e-16 ***\nORIGIN_SZPRSZ07 -2.1667862  0.0162528  -133.318  &lt; 2e-16 ***\nORIGIN_SZPRSZ08 -0.1327079  0.0065712   -20.195  &lt; 2e-16 ***\nORIGIN_SZQTSZ01  0.1062151  0.0071538    14.847  &lt; 2e-16 ***\nORIGIN_SZQTSZ02 -0.4993990  0.0064382   -77.568  &lt; 2e-16 ***\nORIGIN_SZQTSZ03  0.1161844  0.0058822    19.752  &lt; 2e-16 ***\nORIGIN_SZQTSZ04 -0.8102612  0.0072742  -111.389  &lt; 2e-16 ***\nORIGIN_SZQTSZ05 -0.0417272  0.0061917    -6.739 1.59e-11 ***\nORIGIN_SZQTSZ06 -0.2521417  0.0066449   -37.945  &lt; 2e-16 ***\nORIGIN_SZQTSZ07 -1.2395975  0.0097496  -127.143  &lt; 2e-16 ***\nORIGIN_SZQTSZ08 -0.1105467  0.0059364   -18.622  &lt; 2e-16 ***\nORIGIN_SZQTSZ09 -0.5078461  0.0067895   -74.798  &lt; 2e-16 ***\nORIGIN_SZQTSZ10 -0.3866593  0.0066995   -57.714  &lt; 2e-16 ***\nORIGIN_SZQTSZ11 -1.5264609  0.0099770  -152.998  &lt; 2e-16 ***\nORIGIN_SZQTSZ12 -1.3866518  0.0106887  -129.730  &lt; 2e-16 ***\nORIGIN_SZQTSZ13 -0.3764286  0.0066707   -56.430  &lt; 2e-16 ***\nORIGIN_SZQTSZ14 -1.4907399  0.0100120  -148.896  &lt; 2e-16 ***\nORIGIN_SZQTSZ15 -1.0552239  0.0108792   -96.994  &lt; 2e-16 ***\nORIGIN_SZRCSZ01 -1.3136074  0.0126986  -103.445  &lt; 2e-16 ***\nORIGIN_SZRCSZ06 -0.2418276  0.0085509   -28.281  &lt; 2e-16 ***\nORIGIN_SZRVSZ01 -2.9263747  0.0324968   -90.051  &lt; 2e-16 ***\nORIGIN_SZRVSZ02 -2.2980940  0.0278202   -82.605  &lt; 2e-16 ***\nORIGIN_SZRVSZ03 -2.4663765  0.0238674  -103.336  &lt; 2e-16 ***\nORIGIN_SZRVSZ04 -3.1853677  0.0556939   -57.194  &lt; 2e-16 ***\nORIGIN_SZRVSZ05 -1.5695490  0.0166684   -94.163  &lt; 2e-16 ***\nORIGIN_SZSBSZ01  0.7674590  0.0061811   124.163  &lt; 2e-16 ***\nORIGIN_SZSBSZ02 -0.7307279  0.0084105   -86.883  &lt; 2e-16 ***\nORIGIN_SZSBSZ03  0.5920074  0.0050167   118.008  &lt; 2e-16 ***\nORIGIN_SZSBSZ04  0.3684857  0.0058575    62.908  &lt; 2e-16 ***\nORIGIN_SZSBSZ05 -0.0036863  0.0068459    -0.538  0.59026    \nORIGIN_SZSBSZ06 -1.1939284  0.0181541   -65.766  &lt; 2e-16 ***\nORIGIN_SZSBSZ07 -0.4896579  0.0135618   -36.106  &lt; 2e-16 ***\nORIGIN_SZSBSZ08 -2.1221691  0.0127258  -166.762  &lt; 2e-16 ***\nORIGIN_SZSBSZ09 -1.2032410  0.0089611  -134.273  &lt; 2e-16 ***\nORIGIN_SZSESZ02  1.0721820  0.0045336   236.498  &lt; 2e-16 ***\nORIGIN_SZSESZ03  1.0808012  0.0042923   251.801  &lt; 2e-16 ***\nORIGIN_SZSESZ04  1.0137448  0.0050668   200.076  &lt; 2e-16 ***\nORIGIN_SZSESZ05 -0.1678679  0.0060206   -27.882  &lt; 2e-16 ***\nORIGIN_SZSESZ06  0.9165834  0.0048323   189.677  &lt; 2e-16 ***\nORIGIN_SZSESZ07 -2.2499789  0.0196327  -114.603  &lt; 2e-16 ***\nORIGIN_SZSGSZ01 -0.9369800  0.0087282  -107.351  &lt; 2e-16 ***\nORIGIN_SZSGSZ02 -1.1690716  0.0097131  -120.360  &lt; 2e-16 ***\nORIGIN_SZSGSZ03  0.2604352  0.0052709    49.410  &lt; 2e-16 ***\nORIGIN_SZSGSZ04  0.3468823  0.0048897    70.942  &lt; 2e-16 ***\nORIGIN_SZSGSZ05 -1.5927797  0.0106308  -149.827  &lt; 2e-16 ***\nORIGIN_SZSGSZ06  0.3605651  0.0046361    77.774  &lt; 2e-16 ***\nORIGIN_SZSGSZ07 -0.5333873  0.0063119   -84.504  &lt; 2e-16 ***\nORIGIN_SZSKSZ01 -0.2706750  0.0082836   -32.676  &lt; 2e-16 ***\nORIGIN_SZSKSZ02  0.0970953  0.0063378    15.320  &lt; 2e-16 ***\nORIGIN_SZSKSZ03 -0.6954342  0.0082538   -84.256  &lt; 2e-16 ***\nORIGIN_SZSKSZ04 -2.3863580  0.0284607   -83.847  &lt; 2e-16 ***\nORIGIN_SZSKSZ05 -1.5443140  0.0179059   -86.246  &lt; 2e-16 ***\nORIGIN_SZSLSZ01 -2.9450656  0.0307283   -95.842  &lt; 2e-16 ***\nORIGIN_SZSLSZ04 -0.5739349  0.0077851   -73.722  &lt; 2e-16 ***\nORIGIN_SZSRSZ01 -1.6136735  0.0160199  -100.729  &lt; 2e-16 ***\nORIGIN_SZTHSZ01 -2.6034976  0.0489378   -53.200  &lt; 2e-16 ***\nORIGIN_SZTHSZ03 -1.2770601  0.0229815   -55.569  &lt; 2e-16 ***\nORIGIN_SZTHSZ04 -2.0110399  0.0287527   -69.943  &lt; 2e-16 ***\nORIGIN_SZTHSZ06 -1.7720116  0.0180394   -98.230  &lt; 2e-16 ***\nORIGIN_SZTMSZ01  0.1254729  0.0060924    20.595  &lt; 2e-16 ***\nORIGIN_SZTMSZ02  1.6667504  0.0039836   418.403  &lt; 2e-16 ***\nORIGIN_SZTMSZ03  1.0941176  0.0042911   254.976  &lt; 2e-16 ***\nORIGIN_SZTMSZ04  0.3209520  0.0050349    63.746  &lt; 2e-16 ***\nORIGIN_SZTMSZ05 -0.8155124  0.0079342  -102.785  &lt; 2e-16 ***\nORIGIN_SZTNSZ01 -1.4237298  0.0104636  -136.064  &lt; 2e-16 ***\nORIGIN_SZTNSZ02 -1.2718890  0.0098660  -128.916  &lt; 2e-16 ***\nORIGIN_SZTNSZ03 -1.7960517  0.0134675  -133.362  &lt; 2e-16 ***\nORIGIN_SZTNSZ04 -0.3508142  0.0073556   -47.694  &lt; 2e-16 ***\nORIGIN_SZTPSZ01 -0.3841699  0.0064137   -59.898  &lt; 2e-16 ***\nORIGIN_SZTPSZ02  0.5315265  0.0044497   119.451  &lt; 2e-16 ***\nORIGIN_SZTPSZ03 -0.4669723  0.0062160   -75.124  &lt; 2e-16 ***\nORIGIN_SZTPSZ04 -0.0617169  0.0058830   -10.491  &lt; 2e-16 ***\nORIGIN_SZTPSZ05  0.0713309  0.0062133    11.480  &lt; 2e-16 ***\nORIGIN_SZTPSZ06  0.6800356  0.0069456    97.909  &lt; 2e-16 ***\nORIGIN_SZTPSZ07 -0.0432782  0.0064382    -6.722 1.79e-11 ***\nORIGIN_SZTPSZ08 -0.6976429  0.0092416   -75.490  &lt; 2e-16 ***\nORIGIN_SZTPSZ09 -0.3708833  0.0063548   -58.363  &lt; 2e-16 ***\nORIGIN_SZTPSZ10 -0.4063575  0.0077803   -52.229  &lt; 2e-16 ***\nORIGIN_SZTPSZ11  0.1040282  0.0056115    18.538  &lt; 2e-16 ***\nORIGIN_SZTPSZ12 -0.5104672  0.0066261   -77.039  &lt; 2e-16 ***\nORIGIN_SZTSSZ01 -3.5036830  0.0487290   -71.901  &lt; 2e-16 ***\nORIGIN_SZTSSZ02 -0.0386819  0.0094886    -4.077 4.57e-05 ***\nORIGIN_SZTSSZ03 -0.3862387  0.0095139   -40.597  &lt; 2e-16 ***\nORIGIN_SZTSSZ04 -0.6380676  0.0099905   -63.867  &lt; 2e-16 ***\nORIGIN_SZTSSZ05 -2.7354613  0.0162414  -168.425  &lt; 2e-16 ***\nORIGIN_SZTSSZ06 -2.6310865  0.0255772  -102.868  &lt; 2e-16 ***\nORIGIN_SZWCSZ01 -1.1561047  0.0087394  -132.286  &lt; 2e-16 ***\nORIGIN_SZWCSZ02 -2.6956217  0.0319117   -84.471  &lt; 2e-16 ***\nORIGIN_SZWCSZ03 -4.3526889  0.1241082   -35.072  &lt; 2e-16 ***\nORIGIN_SZWDSZ01  0.8712417  0.0043720   199.277  &lt; 2e-16 ***\nORIGIN_SZWDSZ02  0.9119539  0.0050326   181.210  &lt; 2e-16 ***\nORIGIN_SZWDSZ03  1.6205678  0.0045250   358.136  &lt; 2e-16 ***\nORIGIN_SZWDSZ04  1.2081941  0.0054272   222.618  &lt; 2e-16 ***\nORIGIN_SZWDSZ05  0.4284783  0.0052752    81.224  &lt; 2e-16 ***\nORIGIN_SZWDSZ06  0.9018716  0.0049820   181.028  &lt; 2e-16 ***\nORIGIN_SZWDSZ07 -0.6444820  0.0084731   -76.062  &lt; 2e-16 ***\nORIGIN_SZWDSZ08 -0.8764983  0.0082622  -106.085  &lt; 2e-16 ***\nORIGIN_SZWDSZ09  1.3292589  0.0048663   273.158  &lt; 2e-16 ***\nORIGIN_SZYSSZ01 -0.4780462  0.0058489   -81.733  &lt; 2e-16 ***\nORIGIN_SZYSSZ02  0.9323419  0.0054402   171.380  &lt; 2e-16 ***\nORIGIN_SZYSSZ03  2.0577240  0.0046737   440.274  &lt; 2e-16 ***\nORIGIN_SZYSSZ04  0.8697472  0.0047269   184.000  &lt; 2e-16 ***\nORIGIN_SZYSSZ05  0.1662764  0.0060376    27.540  &lt; 2e-16 ***\nORIGIN_SZYSSZ06 -0.8115617  0.0109084   -74.398  &lt; 2e-16 ***\nORIGIN_SZYSSZ07 -0.8971248  0.0119220   -75.250  &lt; 2e-16 ***\nORIGIN_SZYSSZ08 -0.2738680  0.0063553   -43.093  &lt; 2e-16 ***\nORIGIN_SZYSSZ09  1.2274518  0.0044951   273.066  &lt; 2e-16 ***\nDESTIN_SZAMSZ02 -0.0516322  0.0042829   -12.055  &lt; 2e-16 ***\nDESTIN_SZAMSZ03  0.0801823  0.0041904    19.135  &lt; 2e-16 ***\nDESTIN_SZAMSZ04 -0.9282211  0.0061322  -151.368  &lt; 2e-16 ***\nDESTIN_SZAMSZ05 -1.0794168  0.0062543  -172.588  &lt; 2e-16 ***\nDESTIN_SZAMSZ06 -0.8839603  0.0060851  -145.267  &lt; 2e-16 ***\nDESTIN_SZAMSZ07 -1.5835093  0.0096846  -163.508  &lt; 2e-16 ***\nDESTIN_SZAMSZ08 -0.9756903  0.0068829  -141.756  &lt; 2e-16 ***\nDESTIN_SZAMSZ09 -1.0362692  0.0061651  -168.087  &lt; 2e-16 ***\nDESTIN_SZAMSZ10 -0.1227646  0.0044788   -27.410  &lt; 2e-16 ***\nDESTIN_SZAMSZ11 -0.4802374  0.0088108   -54.506  &lt; 2e-16 ***\nDESTIN_SZAMSZ12  0.2142621  0.0050653    42.300  &lt; 2e-16 ***\nDESTIN_SZBDSZ01  0.3582789  0.0039578    90.524  &lt; 2e-16 ***\nDESTIN_SZBDSZ02 -0.4368229  0.0051384   -85.012  &lt; 2e-16 ***\nDESTIN_SZBDSZ03 -0.1568727  0.0044329   -35.388  &lt; 2e-16 ***\nDESTIN_SZBDSZ04  0.6731669  0.0036215   185.882  &lt; 2e-16 ***\nDESTIN_SZBDSZ05  0.3647198  0.0040496    90.062  &lt; 2e-16 ***\nDESTIN_SZBDSZ06  0.0589240  0.0044352    13.286  &lt; 2e-16 ***\nDESTIN_SZBDSZ07 -0.6648168  0.0095742   -69.438  &lt; 2e-16 ***\nDESTIN_SZBDSZ08 -1.7214136  0.0106600  -161.483  &lt; 2e-16 ***\nDESTIN_SZBKSZ01 -1.2688264  0.0067263  -188.637  &lt; 2e-16 ***\nDESTIN_SZBKSZ02 -0.3912129  0.0055446   -70.558  &lt; 2e-16 ***\nDESTIN_SZBKSZ03 -0.8663392  0.0058693  -147.605  &lt; 2e-16 ***\nDESTIN_SZBKSZ04 -0.1247273  0.0051254   -24.335  &lt; 2e-16 ***\nDESTIN_SZBKSZ05 -0.7407774  0.0059120  -125.300  &lt; 2e-16 ***\nDESTIN_SZBKSZ06 -0.9934643  0.0063345  -156.834  &lt; 2e-16 ***\nDESTIN_SZBKSZ07  0.0882230  0.0042928    20.551  &lt; 2e-16 ***\nDESTIN_SZBKSZ08 -1.1134447  0.0070752  -157.372  &lt; 2e-16 ***\nDESTIN_SZBKSZ09 -0.1788171  0.0051327   -34.839  &lt; 2e-16 ***\nDESTIN_SZBLSZ01 -0.7696433  0.0071898  -107.047  &lt; 2e-16 ***\nDESTIN_SZBLSZ02  0.4076650  0.0068001    59.950  &lt; 2e-16 ***\nDESTIN_SZBLSZ03  1.5398488  0.0078230   196.836  &lt; 2e-16 ***\nDESTIN_SZBLSZ04 -0.3499486  0.0136985   -25.546  &lt; 2e-16 ***\nDESTIN_SZBMSZ01 -0.2114705  0.0048311   -43.773  &lt; 2e-16 ***\nDESTIN_SZBMSZ02 -0.3316806  0.0049958   -66.391  &lt; 2e-16 ***\nDESTIN_SZBMSZ03 -0.5134774  0.0058534   -87.723  &lt; 2e-16 ***\nDESTIN_SZBMSZ04 -0.2205274  0.0051028   -43.217  &lt; 2e-16 ***\nDESTIN_SZBMSZ05 -0.2101165  0.0067710   -31.032  &lt; 2e-16 ***\nDESTIN_SZBMSZ06 -1.3832385  0.0124821  -110.818  &lt; 2e-16 ***\nDESTIN_SZBMSZ07 -0.0133462  0.0046787    -2.853  0.00434 ** \nDESTIN_SZBMSZ08 -0.9056756  0.0063868  -141.805  &lt; 2e-16 ***\nDESTIN_SZBMSZ09 -2.3175407  0.0144523  -160.358  &lt; 2e-16 ***\nDESTIN_SZBMSZ10 -1.3973725  0.0090463  -154.470  &lt; 2e-16 ***\nDESTIN_SZBMSZ11 -1.3950206  0.0080459  -173.383  &lt; 2e-16 ***\nDESTIN_SZBMSZ12 -0.6882789  0.0081539   -84.411  &lt; 2e-16 ***\nDESTIN_SZBMSZ13 -0.2729120  0.0052969   -51.523  &lt; 2e-16 ***\nDESTIN_SZBMSZ14 -0.7581980  0.0080215   -94.521  &lt; 2e-16 ***\nDESTIN_SZBMSZ15 -0.9323237  0.0071093  -131.142  &lt; 2e-16 ***\nDESTIN_SZBMSZ16 -2.0655530  0.0108490  -190.391  &lt; 2e-16 ***\nDESTIN_SZBMSZ17 -2.5124893  0.0165366  -151.935  &lt; 2e-16 ***\nDESTIN_SZBPSZ01 -0.8203274  0.0057682  -142.216  &lt; 2e-16 ***\nDESTIN_SZBPSZ02 -1.5284265  0.0087447  -174.783  &lt; 2e-16 ***\nDESTIN_SZBPSZ03 -1.2434382  0.0080852  -153.792  &lt; 2e-16 ***\nDESTIN_SZBPSZ04 -0.7778558  0.0060900  -127.727  &lt; 2e-16 ***\nDESTIN_SZBPSZ05  0.1782204  0.0042331    42.101  &lt; 2e-16 ***\nDESTIN_SZBPSZ06 -0.6758807  0.0079728   -84.773  &lt; 2e-16 ***\nDESTIN_SZBPSZ07 -0.5029450  0.0081151   -61.976  &lt; 2e-16 ***\nDESTIN_SZBSSZ01 -0.1269916  0.0046949   -27.049  &lt; 2e-16 ***\nDESTIN_SZBSSZ02 -0.7536917  0.0051895  -145.233  &lt; 2e-16 ***\nDESTIN_SZBSSZ03  0.2747969  0.0039115    70.254  &lt; 2e-16 ***\nDESTIN_SZBTSZ01  0.1708577  0.0043381    39.385  &lt; 2e-16 ***\nDESTIN_SZBTSZ02 -0.6820190  0.0067243  -101.427  &lt; 2e-16 ***\nDESTIN_SZBTSZ03  0.0610599  0.0049825    12.255  &lt; 2e-16 ***\nDESTIN_SZBTSZ04 -1.3199639  0.0107063  -123.288  &lt; 2e-16 ***\nDESTIN_SZBTSZ05 -0.4174991  0.0069221   -60.314  &lt; 2e-16 ***\nDESTIN_SZBTSZ06 -0.5260242  0.0061145   -86.029  &lt; 2e-16 ***\nDESTIN_SZBTSZ07 -1.6678047  0.0106335  -156.844  &lt; 2e-16 ***\nDESTIN_SZBTSZ08 -0.7999935  0.0089175   -89.711  &lt; 2e-16 ***\nDESTIN_SZCBSZ01 -5.6321332  0.3162476   -17.809  &lt; 2e-16 ***\nDESTIN_SZCCSZ01 -0.9342781  0.0081409  -114.763  &lt; 2e-16 ***\nDESTIN_SZCHSZ01 -1.2808546  0.0096774  -132.355  &lt; 2e-16 ***\nDESTIN_SZCHSZ02  0.0067332  0.0054322     1.239  0.21516    \nDESTIN_SZCHSZ03  1.0988838  0.0041378   265.570  &lt; 2e-16 ***\nDESTIN_SZCKSZ01 -0.3192235  0.0050632   -63.048  &lt; 2e-16 ***\nDESTIN_SZCKSZ02 -0.7776453  0.0055279  -140.676  &lt; 2e-16 ***\nDESTIN_SZCKSZ03  0.2772358  0.0042541    65.170  &lt; 2e-16 ***\nDESTIN_SZCKSZ04 -1.3842048  0.0065159  -212.436  &lt; 2e-16 ***\nDESTIN_SZCKSZ05 -1.2051808  0.0076814  -156.897  &lt; 2e-16 ***\nDESTIN_SZCKSZ06  0.1321955  0.0061568    21.472  &lt; 2e-16 ***\nDESTIN_SZCLSZ01  0.1942449  0.0049977    38.867  &lt; 2e-16 ***\nDESTIN_SZCLSZ02 -2.0828648  0.0134597  -154.749  &lt; 2e-16 ***\nDESTIN_SZCLSZ03 -0.8823728  0.0078307  -112.681  &lt; 2e-16 ***\nDESTIN_SZCLSZ04 -0.2311432  0.0047194   -48.977  &lt; 2e-16 ***\nDESTIN_SZCLSZ05 -1.0113430  0.0085536  -118.237  &lt; 2e-16 ***\nDESTIN_SZCLSZ06  0.0694682  0.0042166    16.475  &lt; 2e-16 ***\nDESTIN_SZCLSZ07 -0.4953961  0.0054184   -91.429  &lt; 2e-16 ***\nDESTIN_SZCLSZ08 -0.3849563  0.0061404   -62.693  &lt; 2e-16 ***\nDESTIN_SZCLSZ09  0.4201808  0.0067112    62.609  &lt; 2e-16 ***\nDESTIN_SZDTSZ02 -2.6513032  0.0348725   -76.029  &lt; 2e-16 ***\nDESTIN_SZDTSZ03 -1.5192228  0.0144477  -105.153  &lt; 2e-16 ***\nDESTIN_SZDTSZ13 -2.2041951  0.0161726  -136.292  &lt; 2e-16 ***\nDESTIN_SZGLSZ01 -0.0139744  0.0052464    -2.664  0.00773 ** \nDESTIN_SZGLSZ02 -0.2850816  0.0047467   -60.059  &lt; 2e-16 ***\nDESTIN_SZGLSZ03  0.3511872  0.0039473    88.969  &lt; 2e-16 ***\nDESTIN_SZGLSZ04  0.2909117  0.0039436    73.769  &lt; 2e-16 ***\nDESTIN_SZGLSZ05  0.1845361  0.0040011    46.121  &lt; 2e-16 ***\nDESTIN_SZHGSZ01  0.1418382  0.0039875    35.571  &lt; 2e-16 ***\nDESTIN_SZHGSZ02 -0.7233151  0.0052374  -138.105  &lt; 2e-16 ***\nDESTIN_SZHGSZ03 -1.1918463  0.0062129  -191.834  &lt; 2e-16 ***\nDESTIN_SZHGSZ04 -0.4380360  0.0044839   -97.691  &lt; 2e-16 ***\nDESTIN_SZHGSZ05 -0.5671024  0.0046427  -122.149  &lt; 2e-16 ***\nDESTIN_SZHGSZ06 -0.8271411  0.0054935  -150.566  &lt; 2e-16 ***\nDESTIN_SZHGSZ07  0.0721800  0.0041589    17.356  &lt; 2e-16 ***\nDESTIN_SZHGSZ08 -0.4297429  0.0050021   -85.913  &lt; 2e-16 ***\nDESTIN_SZHGSZ09 -0.2085461  0.0052544   -39.690  &lt; 2e-16 ***\nDESTIN_SZHGSZ10 -2.9169699  0.0262698  -111.039  &lt; 2e-16 ***\nDESTIN_SZJESZ01 -0.2822473  0.0051166   -55.163  &lt; 2e-16 ***\nDESTIN_SZJESZ02 -0.6761389  0.0053635  -126.063  &lt; 2e-16 ***\nDESTIN_SZJESZ03 -0.7371756  0.0058983  -124.980  &lt; 2e-16 ***\nDESTIN_SZJESZ04 -0.4593491  0.0067970   -67.581  &lt; 2e-16 ***\nDESTIN_SZJESZ05 -1.1418012  0.0099049  -115.277  &lt; 2e-16 ***\nDESTIN_SZJESZ06  0.1759680  0.0042791    41.123  &lt; 2e-16 ***\nDESTIN_SZJESZ07 -1.2260587  0.0082714  -148.229  &lt; 2e-16 ***\nDESTIN_SZJESZ08 -0.8547001  0.0080417  -106.283  &lt; 2e-16 ***\nDESTIN_SZJESZ09 -0.4306353  0.0057006   -75.542  &lt; 2e-16 ***\nDESTIN_SZJESZ10  0.6584971  0.0073664    89.392  &lt; 2e-16 ***\nDESTIN_SZJESZ11  0.9661208  0.0070491   137.056  &lt; 2e-16 ***\nDESTIN_SZJWSZ01 -0.9128436  0.0069529  -131.290  &lt; 2e-16 ***\nDESTIN_SZJWSZ02 -0.7285851  0.0054839  -132.859  &lt; 2e-16 ***\nDESTIN_SZJWSZ03  0.2601455  0.0043215    60.198  &lt; 2e-16 ***\nDESTIN_SZJWSZ04  0.6860274  0.0041135   166.775  &lt; 2e-16 ***\nDESTIN_SZJWSZ05 -0.4684576  0.0062875   -74.506  &lt; 2e-16 ***\nDESTIN_SZJWSZ06 -0.2459774  0.0057575   -42.723  &lt; 2e-16 ***\nDESTIN_SZJWSZ07 -1.8854234  0.0287721   -65.529  &lt; 2e-16 ***\nDESTIN_SZJWSZ08 -0.5523308  0.0051054  -108.186  &lt; 2e-16 ***\nDESTIN_SZJWSZ09  0.8818747  0.0037800   233.301  &lt; 2e-16 ***\nDESTIN_SZKLSZ01 -0.5814386  0.0052711  -110.308  &lt; 2e-16 ***\nDESTIN_SZKLSZ02 -0.7090577  0.0058161  -121.914  &lt; 2e-16 ***\nDESTIN_SZKLSZ03 -1.2191910  0.0065984  -184.772  &lt; 2e-16 ***\nDESTIN_SZKLSZ04 -1.6961428  0.0087866  -193.038  &lt; 2e-16 ***\nDESTIN_SZKLSZ05 -0.6927144  0.0073574   -94.153  &lt; 2e-16 ***\nDESTIN_SZKLSZ06 -2.2967464  0.0362605   -63.340  &lt; 2e-16 ***\nDESTIN_SZKLSZ07 -0.9536980  0.0066777  -142.819  &lt; 2e-16 ***\nDESTIN_SZKLSZ08 -0.4565596  0.0051736   -88.249  &lt; 2e-16 ***\nDESTIN_SZLKSZ01 -1.7277135  0.0207336   -83.329  &lt; 2e-16 ***\nDESTIN_SZMDSZ01 -1.7155417  0.0210080   -81.661  &lt; 2e-16 ***\nDESTIN_SZMDSZ02 -1.3694928  0.0114174  -119.948  &lt; 2e-16 ***\nDESTIN_SZMDSZ03 -2.7183729  0.0252678  -107.582  &lt; 2e-16 ***\nDESTIN_SZMPSZ01 -0.8051991  0.0078564  -102.490  &lt; 2e-16 ***\nDESTIN_SZMPSZ02 -0.7627000  0.0061386  -124.246  &lt; 2e-16 ***\nDESTIN_SZMPSZ03 -0.0649484  0.0047787   -13.591  &lt; 2e-16 ***\nDESTIN_SZMUSZ02 -1.9549128  0.0200160   -97.667  &lt; 2e-16 ***\nDESTIN_SZNTSZ01 -3.3048398  0.0448053   -73.760  &lt; 2e-16 ***\nDESTIN_SZNTSZ02 -1.6454847  0.0109337  -150.497  &lt; 2e-16 ***\nDESTIN_SZNTSZ03 -1.1389723  0.0077396  -147.161  &lt; 2e-16 ***\nDESTIN_SZNTSZ05 -2.0264109  0.0250226   -80.983  &lt; 2e-16 ***\nDESTIN_SZNTSZ06 -3.3496282  0.0428989   -78.082  &lt; 2e-16 ***\nDESTIN_SZNVSZ01 -0.3407614  0.0045493   -74.905  &lt; 2e-16 ***\nDESTIN_SZNVSZ02 -0.4987695  0.0053942   -92.465  &lt; 2e-16 ***\nDESTIN_SZNVSZ03 -0.4936107  0.0055158   -89.491  &lt; 2e-16 ***\nDESTIN_SZNVSZ04 -1.9141281  0.0107557  -177.964  &lt; 2e-16 ***\nDESTIN_SZNVSZ05 -1.5378263  0.0089577  -171.677  &lt; 2e-16 ***\nDESTIN_SZPGSZ01 -1.7744485  0.0194346   -91.304  &lt; 2e-16 ***\nDESTIN_SZPGSZ02 -0.9282918  0.0069006  -134.523  &lt; 2e-16 ***\nDESTIN_SZPGSZ03  0.0885025  0.0042145    21.000  &lt; 2e-16 ***\nDESTIN_SZPGSZ04 -0.3879375  0.0046862   -82.784  &lt; 2e-16 ***\nDESTIN_SZPGSZ05 -0.9649873  0.0074625  -129.311  &lt; 2e-16 ***\nDESTIN_SZPLSZ01 -0.6159175  0.0070845   -86.939  &lt; 2e-16 ***\nDESTIN_SZPLSZ02 -1.7551386  0.0133081  -131.885  &lt; 2e-16 ***\nDESTIN_SZPLSZ03 -0.1378379  0.0098704   -13.965  &lt; 2e-16 ***\nDESTIN_SZPLSZ04 -0.1411200  0.0096446   -14.632  &lt; 2e-16 ***\nDESTIN_SZPLSZ05 -0.8483196  0.0119048   -71.259  &lt; 2e-16 ***\nDESTIN_SZPNSZ01 -0.1579087  0.0057330   -27.544  &lt; 2e-16 ***\nDESTIN_SZPNSZ02  1.0243480  0.0076680   133.587  &lt; 2e-16 ***\nDESTIN_SZPNSZ03  0.0451598  0.0081444     5.545 2.94e-08 ***\nDESTIN_SZPNSZ04  1.8941928  0.0087479   216.530  &lt; 2e-16 ***\nDESTIN_SZPNSZ05  1.0341581  0.0130830    79.046  &lt; 2e-16 ***\nDESTIN_SZPRSZ01 -1.4038513  0.0086911  -161.527  &lt; 2e-16 ***\nDESTIN_SZPRSZ02 -0.4942539  0.0052403   -94.319  &lt; 2e-16 ***\nDESTIN_SZPRSZ03  0.4219510  0.0040281   104.751  &lt; 2e-16 ***\nDESTIN_SZPRSZ04 -0.4841099  0.0083498   -57.979  &lt; 2e-16 ***\nDESTIN_SZPRSZ05 -0.2988481  0.0047512   -62.899  &lt; 2e-16 ***\nDESTIN_SZPRSZ06  0.0012333  0.0054530     0.226  0.82108    \nDESTIN_SZPRSZ07 -1.1417482  0.0118845   -96.070  &lt; 2e-16 ***\nDESTIN_SZPRSZ08 -0.8259249  0.0066757  -123.720  &lt; 2e-16 ***\nDESTIN_SZQTSZ01 -1.2134330  0.0089222  -136.002  &lt; 2e-16 ***\nDESTIN_SZQTSZ02 -1.2397956  0.0074512  -166.388  &lt; 2e-16 ***\nDESTIN_SZQTSZ03 -0.7448659  0.0066511  -111.992  &lt; 2e-16 ***\nDESTIN_SZQTSZ04 -0.6243112  0.0066812   -93.443  &lt; 2e-16 ***\nDESTIN_SZQTSZ05 -0.6102589  0.0060458  -100.940  &lt; 2e-16 ***\nDESTIN_SZQTSZ06 -0.9164592  0.0065095  -140.788  &lt; 2e-16 ***\nDESTIN_SZQTSZ07 -1.4600643  0.0109976  -132.762  &lt; 2e-16 ***\nDESTIN_SZQTSZ08  0.0004582  0.0050178     0.091  0.92724    \nDESTIN_SZQTSZ09 -0.5226213  0.0058901   -88.728  &lt; 2e-16 ***\nDESTIN_SZQTSZ10 -0.3867082  0.0055876   -69.208  &lt; 2e-16 ***\nDESTIN_SZQTSZ11  0.0260589  0.0055065     4.732 2.22e-06 ***\nDESTIN_SZQTSZ12 -0.3387634  0.0072779   -46.547  &lt; 2e-16 ***\nDESTIN_SZQTSZ13 -0.0512118  0.0053664    -9.543  &lt; 2e-16 ***\nDESTIN_SZQTSZ14 -0.2555346  0.0063792   -40.057  &lt; 2e-16 ***\nDESTIN_SZQTSZ15 -0.1820651  0.0077537   -23.481  &lt; 2e-16 ***\nDESTIN_SZRCSZ01 -0.4641196  0.0072515   -64.003  &lt; 2e-16 ***\nDESTIN_SZRCSZ06 -2.0929548  0.0189106  -110.676  &lt; 2e-16 ***\nDESTIN_SZRVSZ01 -1.7885682  0.0163492  -109.398  &lt; 2e-16 ***\nDESTIN_SZRVSZ02 -3.1669721  0.0326320   -97.051  &lt; 2e-16 ***\nDESTIN_SZRVSZ03 -2.0306835  0.0135749  -149.591  &lt; 2e-16 ***\nDESTIN_SZRVSZ04 -1.5113470  0.0155637   -97.107  &lt; 2e-16 ***\nDESTIN_SZRVSZ05 -2.3683855  0.0259334   -91.326  &lt; 2e-16 ***\nDESTIN_SZSBSZ01 -0.5841063  0.0068588   -85.162  &lt; 2e-16 ***\nDESTIN_SZSBSZ02 -1.0777704  0.0078288  -137.667  &lt; 2e-16 ***\nDESTIN_SZSBSZ03  0.4734371  0.0045880   103.190  &lt; 2e-16 ***\nDESTIN_SZSBSZ04  0.0546094  0.0057517     9.494  &lt; 2e-16 ***\nDESTIN_SZSBSZ05 -0.9588198  0.0075242  -127.431  &lt; 2e-16 ***\nDESTIN_SZSBSZ06 -1.8528944  0.0234040   -79.170  &lt; 2e-16 ***\nDESTIN_SZSBSZ07 -1.8403768  0.0195878   -93.955  &lt; 2e-16 ***\nDESTIN_SZSBSZ08  0.9205969  0.0055698   165.285  &lt; 2e-16 ***\nDESTIN_SZSBSZ09  0.5166486  0.0051939    99.472  &lt; 2e-16 ***\nDESTIN_SZSESZ02 -0.5728211  0.0048270  -118.669  &lt; 2e-16 ***\nDESTIN_SZSESZ03  0.2554787  0.0038335    66.645  &lt; 2e-16 ***\nDESTIN_SZSESZ04 -0.8982794  0.0056698  -158.432  &lt; 2e-16 ***\nDESTIN_SZSESZ05 -0.4661655  0.0048578   -95.962  &lt; 2e-16 ***\nDESTIN_SZSESZ06 -0.8392849  0.0059198  -141.777  &lt; 2e-16 ***\nDESTIN_SZSESZ07 -3.2182325  0.0227089  -141.717  &lt; 2e-16 ***\nDESTIN_SZSGSZ01 -0.2751206  0.0059581   -46.176  &lt; 2e-16 ***\nDESTIN_SZSGSZ02 -0.2951806  0.0052515   -56.209  &lt; 2e-16 ***\nDESTIN_SZSGSZ03 -0.4469508  0.0048181   -92.766  &lt; 2e-16 ***\nDESTIN_SZSGSZ04 -0.2842809  0.0047961   -59.274  &lt; 2e-16 ***\nDESTIN_SZSGSZ05 -2.0643753  0.0098252  -210.109  &lt; 2e-16 ***\nDESTIN_SZSGSZ06  0.2501247  0.0038873    64.343  &lt; 2e-16 ***\nDESTIN_SZSGSZ07 -0.5743750  0.0052184  -110.067  &lt; 2e-16 ***\nDESTIN_SZSISZ01 -1.1030669  0.0259113   -42.571  &lt; 2e-16 ***\nDESTIN_SZSKSZ01 -0.5462538  0.0071443   -76.460  &lt; 2e-16 ***\nDESTIN_SZSKSZ02  0.2965180  0.0056707    52.290  &lt; 2e-16 ***\nDESTIN_SZSKSZ03 -0.4521490  0.0062177   -72.719  &lt; 2e-16 ***\nDESTIN_SZSKSZ04 -0.6665145  0.0148252   -44.958  &lt; 2e-16 ***\nDESTIN_SZSKSZ05 -0.1474142  0.0121958   -12.087  &lt; 2e-16 ***\nDESTIN_SZSLSZ01 -0.8855715  0.0084587  -104.693  &lt; 2e-16 ***\nDESTIN_SZSLSZ04 -1.1787840  0.0071355  -165.200  &lt; 2e-16 ***\nDESTIN_SZSRSZ01 -1.6435064  0.0128822  -127.580  &lt; 2e-16 ***\nDESTIN_SZTHSZ01 -3.4388625  0.0367651   -93.536  &lt; 2e-16 ***\nDESTIN_SZTHSZ03 -2.5809435  0.0256853  -100.483  &lt; 2e-16 ***\nDESTIN_SZTHSZ04 -2.4887189  0.0214441  -116.056  &lt; 2e-16 ***\nDESTIN_SZTHSZ06 -1.7965101  0.0152160  -118.067  &lt; 2e-16 ***\nDESTIN_SZTMSZ01 -0.3251891  0.0058067   -56.002  &lt; 2e-16 ***\nDESTIN_SZTMSZ02  1.1558743  0.0034703   333.080  &lt; 2e-16 ***\nDESTIN_SZTMSZ03  0.4525619  0.0039244   115.319  &lt; 2e-16 ***\nDESTIN_SZTMSZ04  0.8223271  0.0040060   205.274  &lt; 2e-16 ***\nDESTIN_SZTMSZ05  0.3880619  0.0054308    71.456  &lt; 2e-16 ***\nDESTIN_SZTNSZ01 -0.9533112  0.0067853  -140.496  &lt; 2e-16 ***\nDESTIN_SZTNSZ02 -1.5909451  0.0097396  -163.348  &lt; 2e-16 ***\nDESTIN_SZTNSZ03 -1.6470771  0.0116598  -141.261  &lt; 2e-16 ***\nDESTIN_SZTNSZ04 -1.0686173  0.0069848  -152.993  &lt; 2e-16 ***\nDESTIN_SZTPSZ01 -0.5180183  0.0056886   -91.063  &lt; 2e-16 ***\nDESTIN_SZTPSZ02  0.2160781  0.0038283    56.443  &lt; 2e-16 ***\nDESTIN_SZTPSZ03 -0.2479956  0.0056651   -43.776  &lt; 2e-16 ***\nDESTIN_SZTPSZ04 -1.5015463  0.0072444  -207.271  &lt; 2e-16 ***\nDESTIN_SZTPSZ05 -0.9551144  0.0057981  -164.729  &lt; 2e-16 ***\nDESTIN_SZTPSZ06 -0.4846634  0.0074621   -64.950  &lt; 2e-16 ***\nDESTIN_SZTPSZ07 -1.9753440  0.0118295  -166.984  &lt; 2e-16 ***\nDESTIN_SZTPSZ08 -1.3455063  0.0086909  -154.817  &lt; 2e-16 ***\nDESTIN_SZTPSZ09 -0.3556620  0.0061296   -58.024  &lt; 2e-16 ***\nDESTIN_SZTPSZ10 -1.3213501  0.0085951  -153.733  &lt; 2e-16 ***\nDESTIN_SZTPSZ11 -0.3877006  0.0052409   -73.977  &lt; 2e-16 ***\nDESTIN_SZTPSZ12 -0.7064020  0.0062472  -113.075  &lt; 2e-16 ***\nDESTIN_SZTSSZ01 -0.8827157  0.0218327   -40.431  &lt; 2e-16 ***\nDESTIN_SZTSSZ02 -0.6067055  0.0115514   -52.522  &lt; 2e-16 ***\nDESTIN_SZTSSZ03  0.4380259  0.0086774    50.479  &lt; 2e-16 ***\nDESTIN_SZTSSZ04  0.4902124  0.0089922    54.515  &lt; 2e-16 ***\nDESTIN_SZTSSZ05  1.4336278  0.0093410   153.477  &lt; 2e-16 ***\nDESTIN_SZTSSZ06  0.9223573  0.0209024    44.127  &lt; 2e-16 ***\nDESTIN_SZWCSZ01  1.1559309  0.0051787   223.208  &lt; 2e-16 ***\nDESTIN_SZWCSZ02 -1.2664455  0.0126131  -100.407  &lt; 2e-16 ***\nDESTIN_SZWCSZ03 -2.7360882  0.0325753   -83.993  &lt; 2e-16 ***\nDESTIN_SZWDSZ01  0.8193492  0.0037301   219.657  &lt; 2e-16 ***\nDESTIN_SZWDSZ02 -0.7852474  0.0058655  -133.875  &lt; 2e-16 ***\nDESTIN_SZWDSZ03  0.5742422  0.0041884   137.104  &lt; 2e-16 ***\nDESTIN_SZWDSZ04 -0.8391525  0.0065075  -128.951  &lt; 2e-16 ***\nDESTIN_SZWDSZ05 -0.3510692  0.0057253   -61.319  &lt; 2e-16 ***\nDESTIN_SZWDSZ06  0.1358804  0.0043968    30.905  &lt; 2e-16 ***\nDESTIN_SZWDSZ07 -0.2207379  0.0066369   -33.259  &lt; 2e-16 ***\nDESTIN_SZWDSZ08 -0.0264655  0.0065065    -4.068 4.75e-05 ***\nDESTIN_SZWDSZ09 -0.2065828  0.0050524   -40.888  &lt; 2e-16 ***\nDESTIN_SZYSSZ01  0.7467996  0.0040979   182.238  &lt; 2e-16 ***\nDESTIN_SZYSSZ02 -0.3002718  0.0053434   -56.195  &lt; 2e-16 ***\nDESTIN_SZYSSZ03 -1.1087686  0.0057219  -193.778  &lt; 2e-16 ***\nDESTIN_SZYSSZ04 -0.3748076  0.0051481   -72.805  &lt; 2e-16 ***\nDESTIN_SZYSSZ05 -1.7909654  0.0102064  -175.475  &lt; 2e-16 ***\nDESTIN_SZYSSZ06 -1.8519179  0.0099601  -185.933  &lt; 2e-16 ***\nDESTIN_SZYSSZ07 -0.9246626  0.0118101   -78.294  &lt; 2e-16 ***\nDESTIN_SZYSSZ08  0.4403129  0.0041268   106.697  &lt; 2e-16 ***\nDESTIN_SZYSSZ09  0.0267012  0.0041393     6.451 1.11e-10 ***\nlog(dist)       -0.6721961  0.0001353 -4969.566  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 60796037  on 14733  degrees of freedom\nResidual deviance: 20988409  on 14175  degrees of freedom\nAIC: 21081154\n\nNumber of Fisher Scoring iterations: 7\n\n\nWe can compute for the R-squared using the following\n\nCalcRSquared(dbcSIM$data$TRIPS, dbcSIM$fitted.values)\n\n[1] 0.5739638\n\n\n\n\nModel comparison\nModel performance can also be measured using the RMSE or root mean square error. We can use the performance package to compare different models’ performance using metrics like RMSE\nFirst, we create an object containing the models to be compared\n\nmodel_list &lt;- list(unconstrained=uncSIM,\n                   originConstrained=orcSIM,\n                   destinationConstrained=decSIM,\n                   doublyConstrained=dbcSIM)\n\nNext, we compute for the RMSE for the models and show the results\n\ncompare_performance(model_list,\n                    metrics = \"RMSE\")\n\n# Comparison of Model Performance Indices\n\nName                   | Model |     RMSE\n-----------------------------------------\nunconstrained          |   glm | 4288.012\noriginConstrained      |   glm | 3659.954\ndestinationConstrained |   glm | 3389.556\ndoublyConstrained      |   glm | 3252.297\n\n\nThe output shows that the doubly constrained model has the best performance using RMSE as it has the lowest value among the four\n\n\nVisualizing fitted values\nIn this last section, we learn to visualize the fitted versus the actual values\nForst, we need to extract the fitted values of the unconstrained model\n\ndf &lt;- as.data.frame(uncSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\nNext, we join this with the object SIM_data\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(uncTRIPS = \"uncSIM$fitted.values\")\n\nWe repeat the same for every model.\n\ndf &lt;- as.data.frame(orcSIM$fitted.values) %&gt;%\n  round(digits = 0)\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(orcTRIPS = \"orcSIM$fitted.values\")\n\n\ndf &lt;- as.data.frame(decSIM$fitted.values) %&gt;%\n  round(digits = 0)\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(decTRIPS = \"decSIM$fitted.values\")\n\n\ndf &lt;- as.data.frame(dbcSIM$fitted.values) %&gt;%\n  round(digits = 0)\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(dbcTRIPS = \"dbcSIM$fitted.values\")\n\nWe then prepare the different plots and store them into separate objects\n\nunc_p &lt;- ggplot(data = SIM_data,\n                aes(x = uncTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\norc_p &lt;- ggplot(data = SIM_data,\n                aes(x = orcTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\ndec_p &lt;- ggplot(data = SIM_data,\n                aes(x = decTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\ndbc_p &lt;- ggplot(data = SIM_data,\n                aes(x = dbcTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\nNext, we display the plots in a 2x2 grid so they can be easily compared against one another\n\nggarrange(unc_p, orc_p, dec_p, dbc_p,\n          ncol = 2,\n          nrow = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "In-class/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class/In-class_Ex01/In-class_Ex01.html",
    "title": "In-Class Exercise 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "The exercise uses the following data sources:\n\nMaster Plan 2014 Subzone Boundary from data.gov.sg in SHP and KML formats\nMaster Plan 2019 Subzone Boundary from data.gov.sg\nPre-school locations from data.gov.sg\nSingapore 2023 population from singstat.gov.sg\n\n\n\n\nThis exercise will make use of four R packages: sf, tidyverse, ggstatsplot and tmap.\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, tidyverse, tmap, ggstatsplot)"
  },
  {
    "objectID": "In-class/In-class_Ex01/In-class_Ex01.html#data-sources",
    "href": "In-class/In-class_Ex01/In-class_Ex01.html#data-sources",
    "title": "In-Class Exercise 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "The exercise uses the following data sources:\n\nMaster Plan 2014 Subzone Boundary from data.gov.sg in SHP and KML formats\nMaster Plan 2019 Subzone Boundary from data.gov.sg\nPre-school locations from data.gov.sg\nSingapore 2023 population from singstat.gov.sg"
  },
  {
    "objectID": "In-class/In-class_Ex01/In-class_Ex01.html#installing-and-launching-r-packages",
    "href": "In-class/In-class_Ex01/In-class_Ex01.html#installing-and-launching-r-packages",
    "title": "In-Class Exercise 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "This exercise will make use of four R packages: sf, tidyverse, ggstatsplot and tmap.\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, tidyverse, tmap, ggstatsplot)"
  },
  {
    "objectID": "In-class/In-class_Ex01/In-class_Ex01.html#importing-the-geospatial-data",
    "href": "In-class/In-class_Ex01/In-class_Ex01.html#importing-the-geospatial-data",
    "title": "In-Class Exercise 1: Introduction to Geospatial Analytics",
    "section": "Importing the Geospatial Data",
    "text": "Importing the Geospatial Data\nThe code chunk below loads the Masterplan subzone boundary shape file as a dataframe mpsz14_shp\n\nmpsz14_shp = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe code chunk below loads the Masterplan subzone boundary KML file as a dataframe mpsz14_kml\n\nmpsz14_kml = st_read(\"data/geospatial/MasterPlan2014SubzoneBoundary.kml\")\n\nRunning the code shows that the data is likely corrupted as it is not being properly loaded into R. To illustrate loading the same data in KML format, we can create a clean KML file using st_write()\n\nst_write(mpsz14_shp,\n         \"data/geospatial/MP14SubzoneBoundary.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/geospatial/MP14SubzoneBoundary.kml' using driver `KML'\nWriting layer `MP14SubzoneBoundary' to data source \n  `data/geospatial/MP14SubzoneBoundary.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\nmpsz14_kml = st_read(\"data/geospatial/MP14SubzoneBoundary.kml\")\n\nReading layer `MP14SubzoneBoundary' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial\\MP14SubzoneBoundary.kml' \n  using driver `KML'\nSimple feature collection with 323 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nThe code chunk below loads the 2019 Masterplan subzone boundary SHP file as a dataframe mpsz19_shp\n\nmpsz19_shp = st_read(dsn = \"data/geospatial\", \n                  layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nThe output shows that the data uses a geographic coordinate system instead of a projected coordinate system that we need for analysis. This needs to be translated before we can analyze this data with our other datasets. To do this, we can revise the code to:\n\nmpsz19_shp = st_read(dsn = \"data/geospatial\", \n                  layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nThe code chunk below loads the KML file and also shows we have it in geographic coordinate system.\n\nmpsz19_kml = st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial\\MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe code chunk below loads the preschool location in KML format into a dataframe\n\npreschool_kml = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe code chunk below loads the preschool location in GeoJSON format into a dataframe\n\npreschool_geojson = st_read(\"data/geospatial/PreSchoolsLocation.geojson\") \n\nReading layer `PreSchoolsLocation' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial\\PreSchoolsLocation.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe last two files are again in GCS (WGS84) rather than projected coordinate system (SVY21) We can reconfirm this with the next code chunk\n\nst_crs(preschool_kml)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nWe can use the following code to import the preschool location and project it into SVY21\n\npreschool &lt;- st_read(\"data/geospatial/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nst_crs(preschool)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nThe code chunks below load the Master Plan 2019 Subzone Boundary Data into R dataframes\n\nmpsz19_shp &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz19_kml &lt;- st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial\\MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe code chunk below checks the CRS information of mpsz19_shp\n\nst_crs(mpsz19_shp)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nIt appears mpsz19_shp does not have the correct EPSG code of 3414. The same is true for preschool. The code chunk below reloads the information and already applies the correct EPSG code\n\nmpsz19_shp &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\npreschool &lt;- st_read(\"data/geospatial/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\In-class\\In-class_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "href": "In-class/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 1: Introduction to Geospatial Analytics",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\nThe code chunk below counts the number of preschools in each planning subzone\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(`PreSch Count` = lengths(\n    st_intersects(mpsz19_shp, preschool)))\n\nThe code below then does the following in one line of code:\n\nDerives the area of each planning zone\nDrops the unit of measurement of the area\nCalculates the density of pre-schools at each planning zone\n\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(Area = units::drop_units(\n    st_area(.)),\n    `PreSch Density` = `PreSch Count` / Area * 1000000\n  )"
  },
  {
    "objectID": "In-class/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "href": "In-class/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "title": "In-Class Exercise 1: Introduction to Geospatial Analytics",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nOnce we have computed the area, we can perform the appropriate analysis to see if there are any relationships.\nThe code below creates a plot of the preschool density and the preschool count using ggscatterstats() of ggstatsplot package\n\nmpsz19_shp$`PreSch Density` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Density`))\nmpsz19_shp$`PreSch Count` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Count`)) \nmpsz19_shp &lt;- as.data.frame(mpsz19_shp)\n\nggscatterstats(data = mpsz19_shp,\n               x = `PreSch Density`,\n               y = `PreSch Count`,\n               type = \"parametric\")\n\nRegistered S3 method overwritten by 'ggside':\n  method from   \n  +.gg   ggplot2\n\n\n`stat_xsidebin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_ysidebin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "In-class/In-class_Ex01/In-class_Ex01.html#importing-and-wrangling-aspatial-data",
    "href": "In-class/In-class_Ex01/In-class_Ex01.html#importing-and-wrangling-aspatial-data",
    "title": "In-Class Exercise 1: Introduction to Geospatial Analytics",
    "section": "Importing and Wrangling Aspatial Data",
    "text": "Importing and Wrangling Aspatial Data\nWe load the 2023 population data into a dataframe called popdata and see that there are 101K rows and 7 columns.\n\npopdata = read_csv(\"data/aspatial/respopagesextod2023.csv\")\n\nRows: 100928 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe code chunk below prepares a dataframe showing the population by planning area and planning subzone\n\npopdata2023 &lt;- popdata %&gt;% \n  group_by(PA, SZ, AG) %&gt;% \n  summarise(`POP`=sum(`Pop`)) %&gt;%  \n  ungroup() %&gt;% \n  pivot_wider(names_from=AG,\n              values_from = POP)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\ncolnames(popdata2023)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_Over\"\n\n\nWe then convert the dataframe where the age ranges are grouped into three groups (ECONOMY ACTIVE, AGED, YOUNG) and also introduce the DEPENDENCY column which is the ratio of YOUNG and AGED compared to ECONOMY ACTIVE\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG=rowSums(.[3:6]) # Aged 0 - 24, 10 - 24\n         +rowSums(.[14])) %&gt;% # Aged 5 - 9\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+ # Aged 25 - 59\n  rowSums(.[15])) %&gt;%  # Aged 60 -64\n  mutate(`AGED`=rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL`=rowSums(.[3:21])) %&gt;%\n  mutate(`DEPENDENCY`=(`YOUNG` + `AGED`)\n  / `ECONOMY ACTIVE`) %&gt;% \n  select(`PA`, `SZ`, `YOUNG`, \n         `ECONOMY ACTIVE`, `AGED`,\n         `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "In-class/In-class_Ex01/In-class_Ex01.html#joining-aspatial-data-with-geospatial-data",
    "href": "In-class/In-class_Ex01/In-class_Ex01.html#joining-aspatial-data-with-geospatial-data",
    "title": "In-Class Exercise 1: Introduction to Geospatial Analytics",
    "section": "Joining Aspatial Data with Geospatial Data",
    "text": "Joining Aspatial Data with Geospatial Data\nthe code chunk below combines the population information with the geospatial data for the planning zone boundaries\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) \n\nmpsz_pop2023 &lt;- left_join(mpsz19_shp, popdata2023,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\npop2023_mpsz &lt;- left_join(popdata2023, mpsz19_shp, \n                          by = c(\"SZ\" = \"SUBZONE_N\"))"
  },
  {
    "objectID": "In-class/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class/In-class_Ex03/In-class_Ex03.html",
    "title": "Advanced SPPA Methods - NCKDE",
    "section": "",
    "text": "Data for this exercise are from public sources and will be used to analyse the distribution of childcare centers in the Punggol planning area. Two datasets in ESRI shapefile format will be used:\n\nA line feature geospatial dataset which includes the road network of Punggol planning area\nA point feature geospatial dataset which includes the location of childcare centers in the Punggol planning area\n\n\n\n\nThis exercise will make use of five R packages: sf, spNetwork, tidyverse, and tmap.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nsPNetwork - provides functions for performing SPPA methods like KDE and K-function on a network. The package can also be used to build spatial matrices to conduct traditional spatial analyses with spatial weights based on reticular distances\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spNetwork, tmap, tidyverse)\n\nWe use the random seed 1234 to ensure reproducibility of results\n\nset.seed(1234)"
  },
  {
    "objectID": "In-class/In-class_Ex03/In-class_Ex03.html#data-sources",
    "href": "In-class/In-class_Ex03/In-class_Ex03.html#data-sources",
    "title": "Advanced SPPA Methods - NCKDE",
    "section": "",
    "text": "Data for this exercise are from public sources and will be used to analyse the distribution of childcare centers in the Punggol planning area. Two datasets in ESRI shapefile format will be used:\n\nA line feature geospatial dataset which includes the road network of Punggol planning area\nA point feature geospatial dataset which includes the location of childcare centers in the Punggol planning area"
  },
  {
    "objectID": "In-class/In-class_Ex03/In-class_Ex03.html#installing-and-launching-r-packages",
    "href": "In-class/In-class_Ex03/In-class_Ex03.html#installing-and-launching-r-packages",
    "title": "Advanced SPPA Methods - NCKDE",
    "section": "",
    "text": "This exercise will make use of five R packages: sf, spNetwork, tidyverse, and tmap.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nsPNetwork - provides functions for performing SPPA methods like KDE and K-function on a network. The package can also be used to build spatial matrices to conduct traditional spatial analyses with spatial weights based on reticular distances\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spNetwork, tmap, tidyverse)\n\nWe use the random seed 1234 to ensure reproducibility of results\n\nset.seed(1234)"
  },
  {
    "objectID": "In-class/In-class_Ex03/In-class_Ex03.html#visualization-of-the-sf-objects",
    "href": "In-class/In-class_Ex03/In-class_Ex03.html#visualization-of-the-sf-objects",
    "title": "Advanced SPPA Methods - NCKDE",
    "section": "Visualization of the sf objects",
    "text": "Visualization of the sf objects\nThe code below uses plot() in one map. The add=T argument in the second line allows the two plots to be added one over the other.\n\nplot(st_geometry(network))\nplot(childcare, add=T, col=\"red\", pch=10)\n\n\n\n\n\n\n\n\nThe following code chunk produces a similar map using tmap package.\n\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\ntm_shape(network) +\n  tm_lines() +\n  tm_shape(childcare) +\n  tm_dots(col = \"red\")\n\n\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting"
  },
  {
    "objectID": "In-class/In-class_Ex03/In-class_Ex03.html#preparing-the-lixels-objects",
    "href": "In-class/In-class_Ex03/In-class_Ex03.html#preparing-the-lixels-objects",
    "title": "Advanced SPPA Methods - NCKDE",
    "section": "Preparing the lixels objects",
    "text": "Preparing the lixels objects\nA requirement for NKDE is that the lines objects needs to be cut into lixels. The code below uses lixelize_lines() on network, using a length of 700 and a minimum distance (mindist) of 350.\n\nlixels &lt;- lixelize_lines(network,\n                         700,\n                         mindist = 350)"
  },
  {
    "objectID": "In-class/In-class_Ex03/In-class_Ex03.html#generating-the-line-center-points",
    "href": "In-class/In-class_Ex03/In-class_Ex03.html#generating-the-line-center-points",
    "title": "Advanced SPPA Methods - NCKDE",
    "section": "Generating the line center points",
    "text": "Generating the line center points\nThe code below generates the center points for the lixels\n\nsamples &lt;- lines_center(lixels)\n\nxxx\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(lixels) +\n  tm_lines() +\n  tm_shape(samples) +\n  tm_dots(size = 0.01)\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "In-class/In-class_Ex03/In-class_Ex03.html#computing-nkde",
    "href": "In-class/In-class_Ex03/In-class_Ex03.html#computing-nkde",
    "title": "Advanced SPPA Methods - NCKDE",
    "section": "Computing NKDE",
    "text": "Computing NKDE\nThe code below computes for the NKDE of childcare centres around network\n\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1, nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nThe output of the code is a list of density or intensity values. We copy these values into the original lixel and lixel midpoint dataframes.\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\nNote that the previous values are based on metres and resulted in very low density values. We can change the density values to event per square km by using the code below\n\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000"
  },
  {
    "objectID": "In-class/In-class_Ex03/In-class_Ex03.html#computing-the-k-and-g-functions",
    "href": "In-class/In-class_Ex03/In-class_Ex03.html#computing-the-k-and-g-functions",
    "title": "Advanced SPPA Methods - NCKDE",
    "section": "Computing the K and G Functions",
    "text": "Computing the K and G Functions\nThe code block below computes for the K- and G-functions based on the data using kfunctions()\n\nkfun_childcare &lt;- kfunctions(network, \n                             childcare,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 49, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\nThe output can be visualized by calling plotk for the K-function and plotg for the G-function\n\nkfun_childcare$plotk"
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html",
    "title": "Emerging Hot Spot Analysis",
    "section": "",
    "text": "In this exercise, we perform emerging hotspot analysis on the Hunan GDPPC data."
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#deriving-the-spatial-weights",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#deriving-the-spatial-weights",
    "title": "Emerging Hot Spot Analysis",
    "section": "Deriving the spatial weights",
    "text": "Deriving the spatial weights\nWe use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data"
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#computing-gi",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#computing-gi",
    "title": "Emerging Hot Spot Analysis",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nWe use the following chunk to calculate the local Gi* for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- GDPPC_nb %&gt;%\n  group_by(Year) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#performing-mann-kendall-test-on-gi-of-one-location",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#performing-mann-kendall-test-on-gi-of-one-location",
    "title": "Emerging Hot Spot Analysis",
    "section": "Performing Mann-Kendall Test on Gi of one location",
    "text": "Performing Mann-Kendall Test on Gi of one location\nThe code chunk below evaluates Changsha county for trends\n\ncbg &lt;- gi_stars %&gt;%\n  ungroup() %&gt;%\n  filter(County == \"Changsha\") %&gt;%\n  select(County, Year, gi_star)\n\nWe then plot the result using ggplot2 package\n\nggplot(data = cbg,\n       aes(x = Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\n\n\n\n\n\n\nWe can convert this into an interactive plot by passing the chart into ggplotly()\n\np &lt;- ggplot(data = cbg,\n       aes(x = Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\nWe can conduct the Mann-Kendall test on Changsha by using MannKendall() from the Kendall package in the code chunk below\n\n\\(H_0\\) - No monotonic trend on the GDPPC value of Changsha\n\\(H_1\\) - Monotonic trend is present on the GDPPC value of Changsha\n\n\ncbg %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.485 0.00742    66  136.  589.\n\n\nThe output gives the tau value, the significance level (sl) or p-value."
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#mann-kendall-test-dataframe",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#mann-kendall-test-dataframe",
    "title": "Emerging Hot Spot Analysis",
    "section": "Mann-Kendall Test Dataframe",
    "text": "Mann-Kendall Test Dataframe\nThe code chunk below runs the Mann-Kendall test on all counties in Hunan.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(County) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\nhead(ehsa)\n\n# A tibble: 6 × 6\n  County        tau        sl     S     D  varS\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Anhua      0.191  0.303        26  136.  589.\n2 Anren     -0.294  0.108       -40  136.  589.\n3 Anxiang    0      1             0  136.  589.\n4 Baojing   -0.691  0.000128    -94  136.  589.\n5 Chaling   -0.0882 0.650       -12  136.  589.\n6 Changning -0.750  0.0000318  -102  136.  589.\n\n\nNot all the counties are showing statistically significant result (i.e., sl &lt; 0.05)\nWe can examine some of the counties to observe how their Gi* values change overtime.\nThe code chunk below\n\ncountycheck &lt;- gi_stars %&gt;%\n  ungroup() %&gt;%\n  filter(County == \"Changsha\") %&gt;%\n  select(County, Year, gi_star)\n\np &lt;- ggplot(data = countycheck,\n       aes(x = Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)"
  },
  {
    "objectID": "In-class/In-class_Ex05/In-class_Ex05.html#visualising-ehsa",
    "href": "In-class/In-class_Ex05/In-class_Ex05.html#visualising-ehsa",
    "title": "Emerging Hot Spot Analysis",
    "section": "Visualising EHSA",
    "text": "Visualising EHSA\nTo visualize, we first hoing the ehsa results with the sf dataframe\n\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\nWe can then use tmap package to create a filled map based on the classification filtered for statistically significant results.\n\nehsa_sig &lt;- hunan_ehsa %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(ehsa_sig) +\n  tm_fill(\"classification\") +\n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "In-class/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class/In-class_Ex07/In-class_Ex07.html",
    "title": "Geospatially Weighted Forecasting Model",
    "section": "",
    "text": "In this in-class exercise, we go through some of the steps to work on HDB data and then revise the hands-on exercise."
  },
  {
    "objectID": "In-class/In-class_Ex07/In-class_Ex07.html#loading-packages",
    "href": "In-class/In-class_Ex07/In-class_Ex07.html#loading-packages",
    "title": "Geospatially Weighted Forecasting Model",
    "section": "Loading Packages",
    "text": "Loading Packages\nThe following code chunk loads five packages for this exercise. We use the three of them for the first time:\n\nhttr - allows us to let R “talk to http”\nrvest - is used for crawling websites (https://rvest.tidyverse.org)\njsonlite - to be able to work with the crawled data which is returned in JSON format\n\n\npacman::p_load(tidyverse, sf, httr, jsonlite, rvest)"
  },
  {
    "objectID": "In-class/In-class_Ex07/In-class_Ex07.html#loading-and-preparing-aspatial-data",
    "href": "In-class/In-class_Ex07/In-class_Ex07.html#loading-and-preparing-aspatial-data",
    "title": "Geospatially Weighted Forecasting Model",
    "section": "Loading and Preparing Aspatial Data",
    "text": "Loading and Preparing Aspatial Data\nWe use the following code chunk to load the latest HDB resale prices from data.gov.sg. For the problem, we are only concerned with 2023 information and onwards, so we apply a filter in the pipeline.\n\nresale &lt;- read_csv(\"data/aspatial/resale.csv\") %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\nRows: 192234 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): month, town, flat_type, block, street_name, storey_range, flat_mode...\ndbl (3): floor_area_sqm, lease_commence_date, resale_price\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can take a look at the data using head()\n\nhead(resale)\n\n# A tibble: 6 × 11\n  month town  flat_type block street_name storey_range floor_area_sqm flat_model\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;     \n1 2023… ANG … 2 ROOM    406   ANG MO KIO… 01 TO 03                 44 Improved  \n2 2023… ANG … 2 ROOM    323   ANG MO KIO… 04 TO 06                 49 Improved  \n3 2023… ANG … 2 ROOM    314   ANG MO KIO… 04 TO 06                 44 Improved  \n4 2023… ANG … 2 ROOM    314   ANG MO KIO… 07 TO 09                 44 Improved  \n5 2023… ANG … 2 ROOM    170   ANG MO KIO… 01 TO 03                 45 Improved  \n6 2023… ANG … 3 ROOM    225   ANG MO KIO… 04 TO 06                 67 New Gener…\n# ℹ 3 more variables: lease_commence_date &lt;dbl&gt;, remaining_lease &lt;chr&gt;,\n#   resale_price &lt;dbl&gt;\n\n\nIt is worth noting that there are no exact locations readily present in the data set. There are towns and block numbers, but there are no postcodes.\nThe code chunk below produces an address column, and remaining lease columns. We produce the address column to try to use it for reverse geocoding later.\n\nresale_tidy &lt;- resale %&gt;%\n  mutate(address = paste(block,street_name)) %&gt;%\n  mutate(remaining_lease_yr = as.integer(\n    str_sub(remaining_lease, 0, 2)))%&gt;%\n  mutate(remaining_lease_mth = as.integer(\n    str_sub(remaining_lease, 9, 11)))\n\nWe will only demonstrate geocoding via API and web crawling, we limit the data to just a single month. (Sept 2024)\n\nresale_selected &lt;- resale_tidy %&gt;%\n  filter(month == \"2024-09\")\n\nSince we only need to pass an address once for reverse geocoding, we reduce the data to unique addresses and then sort them.\n\nadd_list &lt;- sort(unique(resale_selected$address))\n\nWith this list, we can crawl onemap.gov.sg to perform the reverse geocoding for each address on at a time. It passes each address into the search field one at a time and then appends the first nonNIL result into postal_coords The following code chunk creates the function to perform this.\n\nget_coords &lt;- function(add_list){\n  \n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (i in add_list){\n    #print(i)\n\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each address\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(address= i, \n                            postal = postal, \n                            latitude = lat, \n                            longitude = lng)\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found &gt; 1){\n      # Remove those with NIL as postal\n      res_sub &lt;- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row &lt;- data.frame(address= i, \n                                postal = NA, \n                                latitude = NA, \n                                longitude = NA)\n      }\n      \n      else{\n        top1 &lt;- head(res_sub, n = 1)\n        postal &lt;- top1$POSTAL \n        lat &lt;- top1$LATITUDE\n        lng &lt;- top1$LONGITUDE\n        new_row &lt;- data.frame(address= i, \n                              postal = postal, \n                              latitude = lat, \n                              longitude = lng)\n      }\n    }\n\n    else {\n      new_row &lt;- data.frame(address= i, \n                            postal = NA, \n                            latitude = NA, \n                            longitude = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  return(postal_coords)\n}\n\nWe then use the following code chunk to run the function.\n\ncoords &lt;- get_coords(add_list)\n\n\nwrite_rds(coords, \"data/rds/coords.rds\")"
  },
  {
    "objectID": "In-class/In-class_Ex07/In-class_Ex07.html#prediction-using-mlr",
    "href": "In-class/In-class_Ex07/In-class_Ex07.html#prediction-using-mlr",
    "title": "Geospatially Weighted Forecasting Model",
    "section": "Prediction using MLR",
    "text": "Prediction using MLR\nWe can use lm() to produce the non-spatial multiple linear regression model. We then use olsrr package to produce a publication level report\n\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\n\nolsrr::ols_regress(price_mlr)\n\n                              Model Summary                                \n--------------------------------------------------------------------------\nR                           0.857       RMSE                    61241.154 \nR-Squared                   0.735       MSE                3755930224.018 \nAdj. R-Squared              0.734       Coef. Var                  14.137 \nPred R-Squared              0.734       AIC                    257198.078 \nMAE                     47287.918       SBC                    257313.971 \n--------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares           DF       Mean Square       F          Sig. \n--------------------------------------------------------------------------------\nRegression    1.073265e+14           14      7.666177e+12    2041.086    0.0000 \nResidual       3.87612e+13        10320    3755930224.018                       \nTotal         1.460877e+14        10334                                         \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                 \n------------------------------------------------------------------------------------------------------------------\n                   model          Beta    Std. Error    Std. Beta       t        Sig          lower         upper \n------------------------------------------------------------------------------------------------------------------\n             (Intercept)    106428.137     10719.139                   9.929    0.000     85416.547    127439.728 \n          floor_area_sqm      2781.241        90.565        0.166     30.710    0.000      2603.716      2958.767 \n            storey_order     13662.787       339.522        0.224     40.241    0.000     12997.258     14328.316 \n    remaining_lease_mths       345.606         4.589        0.449     75.306    0.000       336.610       354.602 \n                PROX_CBD    -17042.547       200.926       -0.593    -84.820    0.000    -17436.401    -16648.693 \n        PROX_ELDERLYCARE    -14388.896       994.879       -0.079    -14.463    0.000    -16339.052    -12438.740 \n             PROX_HAWKER    -20000.092      1267.512       -0.087    -15.779    0.000    -22484.662    -17515.522 \n                PROX_MRT    -31665.727      1732.322       -0.103    -18.279    0.000    -35061.414    -28270.039 \n               PROX_PARK     -5460.628      1475.918       -0.021     -3.700    0.000     -8353.713     -2567.543 \n               PROX_MALL    -13370.809      2014.963       -0.040     -6.636    0.000    -17320.526     -9421.091 \n        PROX_SUPERMARKET    -26867.912      4117.382       -0.036     -6.525    0.000    -34938.779    -18797.045 \nWITHIN_350M_KINDERGARTEN      8444.132       636.522        0.071     13.266    0.000      7196.425      9691.838 \n   WITHIN_350M_CHILDCARE     -4758.093       351.020       -0.080    -13.555    0.000     -5446.161     -4070.026 \n         WITHIN_350M_BUS      1091.322       220.861        0.027      4.941    0.000       658.392      1524.251 \n       WITHIN_1KM_PRISCH     -7757.364       484.342       -0.100    -16.016    0.000     -8706.767     -6807.960 \n------------------------------------------------------------------------------------------------------------------\n\n\nAside from olsrr, we can also use the performance package to diagnose the model. The code chunk below checks for multicollinearity using the VIF. This is also in the easystats package.\n\nvif &lt;- performance::check_collinearity(price_mlr)\nkable(vif,\n      caption = \"Variable Inflation Factor Results\") %&gt;%\n  kable_styling(font_size = 18)\n\n\nplot(vif) +\n  theme(axis.text.x = element_text(angle =45, hjust = 1))\n\nCalculating adaptive bandwidth is done using bw.gwr() Note that the function can already work with sf format.\n\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\nWe see that the recommended bandwidth is 41. We write the results into an RDS file so we do not need to rerun the code block.\n\nwrite_rds(bw_adaptive, \"data/rds/bw_adaptive.rds\")\n\n\n\n[1] 41\n\n\nCalibrating the model is then done with the computed adaptive bandwidth through the gwr.basic() function\n\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\nPredicting test data is done with the gwr.predict() function\n\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data, \n                        predictdata = test_data, \n                        bw=bw_adaptive, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE,\n                        dMat1 = dmat_gwr)"
  },
  {
    "objectID": "In-class/In-class_Ex07/In-class_Ex07.html#prediction-models-with-random-forests",
    "href": "In-class/In-class_Ex07/In-class_Ex07.html#prediction-models-with-random-forests",
    "title": "Geospatially Weighted Forecasting Model",
    "section": "Prediction models with random forests",
    "text": "Prediction models with random forests\nSpatialML needs the coordinate information in order to perform training and predictions.\n\ncoords &lt;- st_coordinates(mdata)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\nWe also need to remove the geometry from the datasets to ensure that they are simple dataframes and not sf dataframes.\n\ntrain_data_nogeom &lt;- train_data %&gt;% \n  st_drop_geometry()\n\ntest_data_nogeom &lt;- test_data %&gt;% \n  st_drop_geometry()\n\nWe can calibrate the non-spatial model using ranger()\n\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_order + \n               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n               WITHIN_1KM_PRISCH,\n             data=train_data_nogeom)\nrf\n\nWe can calibrate the spatial model using grf()\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data_nogeom, \n                     bw=bw_adaptive,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\nwrite_rds(gwRF_adaptive, \"data/rds/gwRF_adaptive.rds\")\n\nPredicting test data values first needs including the coordinates into the test data set before passing it into predict.grf()\n\ntest_data_nogeom &lt;- cbind(test_data_nogeom, coords_test)\n\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data_nogeom, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\nThe returned object is a list. We convert it into a dataframe to be able to work with it for visualizations and calculations. We then combine it with the original dataset\n\nGRF_pred_df &lt;- as.data.frame(gwRF_pred)\n\ntest_data_p &lt;- cbind(test_data_nogeom, GRF_pred_df)\n\nWe can then compute the RMSE and plot the actual and predicted values."
  },
  {
    "objectID": "In-class/In-class_Ex08/data/geospatial/hexagons.html",
    "href": "In-class/In-class_Ex08/data/geospatial/hexagons.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 0 0     false"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "",
    "text": "Welcome to my website for my work in the course ISSS 626 - Geospatial Analytics and Applications during the August term of 2024 under Dr Kam Tin Seong.\nThe course covers theory and methods of geospatial analysis and the tools in R to implement such analyses."
  },
  {
    "objectID": "index.html#hands-on-exercises",
    "href": "index.html#hands-on-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nHands-on exercises are assigned before each lesson to give us hands-on experience in using R on the geospatial analysis methods and functions we learned. We are given step-by-step instructions and explanations (through Dr Kam’s online book) which complement our pre-class reading, and prepares us for the in-class exercises.\n\nGeospatial Data Wrangling with R (for Session 1, 26 Aug ’24)\nChoropleth Mapping with R (for Session 1, 26 Aug ’24)\n1st Order Spatial Point Patterns Analysis Methods (for Session 2, 2 Sept ’24)\n2nd Order Spatial Point Patterns Analysis Methods (for Session 2, 2 Sept ’24)\nNetwork Constrained Spatial Point Pattern Analysis (for Session 3, 9 Sept 2024)\nSpatial Weights and Applications (for Session 4, 16 Sept 2024)\nGlobal Measures of Spatial Autocorrelation (for Session 5, 23 Sept 2024)\nLocal Measures of Spatial Autocorrelation (for Session 5, 23 Sept 2024)\nGeographical Segmentation with Spatially Constrained Clustering Techniques (for Session 6, 30 Sept 2024)\nGeographically Weighted Regression Models (for Session 7, 14 Oct 2024)\nGeographically Weighted Predictive Models (for Session 8, 21 Oct 2024)\nModelling Geographic Accessibility (for Session 9, 28 Oct 2024)\nSpatial Interaction Models (for Session 10, 4 Nov 2024)"
  },
  {
    "objectID": "index.html#in-class-exercises",
    "href": "index.html#in-class-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nIn each session, we go through a hands-on exercise to revise the readings for the lesson. The objective of these is to further reinforce the concepts and tools learned in the readings and in the take-home exercise. These will also go further into the analysis and interpretation of results.\n\nIntroduction to Geospatial Analytics (26 Aug ’24)\nSpatial Point Pattern Analysis - Data Load for Take-home Exercise 1 (2 Sept ’24)\nAdvanced Spatial Point Patterns Analysis (9 Sept ’24)\nSpatial Autocorrelation (23 Sept’24)\nEmerging Hot Spot Analysis (30 Sept ’24)\nGeographically Weighted Regression (14 Oct 2024)\nGeographically Weighted Forecasting (21 Oct 2024)\nGeographic Accessibility (28 Oct 2024)"
  },
  {
    "objectID": "index.html#take-home-exercises",
    "href": "index.html#take-home-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "Take-Home Exercises",
    "text": "Take-Home Exercises\nTake-home exercises are where we students are able to apply the methods and techniques learned in class in real-world cases– using real-world data and aim to generate real insights.\nFor each of these, we are given a specific problem, objectives and a base data set. Each student will work on the problem independently, guided only by the previous exercises and lessons.\n\nGeospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region\nDiscovering the Impact of COVID-19 on Thai tourism economy\nGeographically Weighted Modeling of Financial Inclusion in Tanzania"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "",
    "text": "We look into the performance of the provinces across Thailand across various tourism indicators like revenue, number of tourists and occupancy rate and see if there is any spatial and spatiotemporal relationship present. We use various techniques to verify spatial randomness, and monotonicity, and where these are violated, identify clusters, outliers and hot or coldspots that can lead to targetted policies to address the problems or learn from the success of such provinces."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.1-background",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.1-background",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.1 Background",
    "text": "A.1 Background\nTourism is a major industry in Thailand as it made up to 20% of their gross domestic product pre-pandemic. However, like the rest of the world, the industry has taken a hit with COVID-19 in 2020, and has slowly been recovering since 2021. Recent reports are stating that Thailand is already, but still, at 80% of its peak level in 2019.\nWhile we speak about the industry in general, the state of tourism within Thailand, and their recovery status are not the same. For example, tourism revenues have been focused on Bangkok, Phuket and Chonburi pre-pandemic.\nWe are interested in understanding the state of tourism across Thailand with regards to its spatial distribution and time and space distribution– both in absolutes and in terms of the trend with respect to the pandemic."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.2-objectives",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.2-objectives",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nFor this study, we want to understand the state of tourism in Thailand at a provincial level, and answer the following questions:\n\nAre the key tourism indicators in Thailand (at a province level) independent from space and from space and time?\nIf tourism or any tourism indicators are not independent, what are the clusters, outliers and emerging hotspots and coldspots?\n\nWe will use the appropriate packages in R in order to perform the different analysis (spatial and otherwise) to support our answers to the above questions."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.3-data-sources",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.3-data-sources",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe following data sources are used for this analysis:\n\nThailand Domestic Tourism Statistics from Kaggle covering the years 2019-2023 and are at province and month level across 8 indicators:\n\nno_tourist_all - total number of domestic tourists\nno_tourist_foreign - number of foreign tourists\nno_tourist_occupied - number of hotel rooms occupied\nno_tourist_thai - number of Thai tourists\noccupancy_rate - the percentage of occupied travel accommodations (hotel rooms)\nrevenue_all - total tourism revenue, in M-THB (appears as net profit in the raw data)\nrevenue_foreign - revenue generated by foreign tourists, in M-THB (appears as net profit in the raw data)\nrevenue_thai - revenue generated by Thai tourists, in M-THB (appears as net profit in the raw data)\n\nThailand-Subnational Administrative Boundaries from Human Data Exchange in shapefile format"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.4-importing-and-launching-r-packages",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.4-importing-and-launching-r-packages",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, the following R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nsf - package for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - package with functions for plotting cartographic quality maps\nsfdep - for handling spatial data\ncoorplot, ggpubr, heatmaply, factoextra - packages for multivariate data visualization and analysis\ncluster, ClustGeo, NbClust - packages for performing cluster analysis\n\n\n\n\npacman::p_load(sf, tmap, spdep, sfdep, tidyverse,\n               ggpubr, heatmaply, factoextra,\n               NbClust, cluster, ClustGeo)\n\n\n\n\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#b.1.-thailand-subnational-boundary-provincial-level",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#b.1.-thailand-subnational-boundary-provincial-level",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "B.1. Thailand Subnational Boundary, Provincial Level",
    "text": "B.1. Thailand Subnational Boundary, Provincial Level\nWe load the Thailand subnational administrative boundary shapefile into an R dataframe using st_read() from the sf package. We need to analyze at the provincial level so we will be using the files suffixed by “1”.\n\nthai_sf &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThe output states that the object is of multipolygon geometry type containing 77 features (provinces, records) across 16 fields. (columns) We can check the contents of the object using a number of methods. For the code chunk below, we use glimpse() which lists the columns, gives the data type and the first elements.\n\nglimpse(thai_sf)\n\nRows: 77\nColumns: 17\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,…\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P…\n$ ADM1_TH    &lt;chr&gt; \"กรุงเทพมหานคร\", \"สมุทรปราการ\", \"นนทบุรี\", \"ปทุมธานี\", \"พระนครศรีอ…\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH…\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",…\n$ ADM0_TH    &lt;chr&gt; \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศ…\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",…\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18…\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22…\n$ validTo    &lt;date&gt; -001-11-30, -001-11-30, -001-11-30, -001-11-30, -001-11-30…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (…\n\n\nFor clarity, we can clean up this dataframe by:\n\nKeeping only relevant columns: The province name and code, geometry\nRenaming the columns: change ADM1 to Province\n\nThe following code chunk executes these steps by using select() for the first step and rename() for the second step. We again use glimpse() to give a preview of the dataset’s columns.\n\nthai_sf &lt;- thai_sf %&gt;%\n  select(ADM1_EN, ADM1_PCODE, geometry) %&gt;%\n  rename(Province = ADM1_EN, ProvCode = ADM1_PCODE)\n\nglimpse(thai_sf)\n\nRows: 77\nColumns: 3\n$ Province &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"Phr…\n$ ProvCode &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH17…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((100.6139 13..., MULTIPOLYGON (((…\n\n\nWe can check if there are any missing values by using is.na() and then check across each column using colSums() from Base R.\n\ncolSums(is.na(thai_sf))\n\nProvince ProvCode geometry \n       0        0        0 \n\n\nThe output shows that there are no missing values for any of the retained columns.\nFinally, we can quickly check if the object depicts Thailand properly by producing a quick map using qtm() from tmap package.\n\nqtm(thai_sf)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#b.2.-thailand-tourism-data-by-province-jan-2019---feb-2023",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#b.2.-thailand-tourism-data-by-province-jan-2019---feb-2023",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "B.2. Thailand Tourism Data by Province, Jan 2019 - Feb 2023",
    "text": "B.2. Thailand Tourism Data by Province, Jan 2019 - Feb 2023\nThe code chunk below loads the tourism statistics data into a dataframe tourism. We use read_csv() to import the data from the file.\n\ntourism &lt;- read_csv(\"data/aspatial/thailand_domestic_tourism_2019_2023.csv\")\n\nRows: 30800 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): province_thai, province_eng, region_thai, region_eng, variable\ndbl  (1): value\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can check the contents by using the code chunk below.\n\ntourism\n\n# A tibble: 30,800 × 7\n   date       province_thai province_eng   region_thai region_eng variable value\n   &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n 1 2019-01-01 กรุงเทพมหานคร  Bangkok        ภาคกลาง     central    occupan…  93.4\n 2 2019-01-01 ลพบุรี          Lopburi        ภาคกลาง     central    occupan…  61.3\n 3 2019-01-01 พระนครศรีอยุธยา Phra Nakhon S… ภาคกลาง     central    occupan…  73.4\n 4 2019-01-01 สระบุรี         Saraburi       ภาคกลาง     central    occupan…  67.3\n 5 2019-01-01 ชัยนาท         Chainat        ภาคกลาง     central    occupan…  79.3\n 6 2019-01-01 นครปฐม        Nakhon Pathom  ภาคกลาง     central    occupan…  71.7\n 7 2019-01-01 สิงห์บุรี         Sing Buri      ภาคกลาง     central    occupan…  64.6\n 8 2019-01-01 อ่างทอง        Ang Thong      ภาคกลาง     central    occupan…  71.2\n 9 2019-01-01 นนทบุรี         Nonthaburi     ภาคกลาง     central    occupan…  75.1\n10 2019-01-01 ปทุมธานี        Pathum Thani   ภาคกลาง     central    occupan…  60.8\n# ℹ 30,790 more rows\n\n\nThe imported data contains 7 fields and 30,800 records at a province and month level.\nBefore we analyze the dataset, let use remove unnecessary columns and rename the column names, similar to the previous dataset, using the code chunk below. (by using select() and rename())\n\ntourism &lt;- tourism %&gt;%\n  select(date, province_eng, region_eng, variable, value) %&gt;%\n  rename(Date = date, Province = province_eng, Region = region_eng, Indicator = variable, Value = value)\n\nhead(tourism)\n\n# A tibble: 6 × 5\n  Date       Province                 Region  Indicator      Value\n  &lt;date&gt;     &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2019-01-01 Bangkok                  central occupancy_rate  93.4\n2 2019-01-01 Lopburi                  central occupancy_rate  61.3\n3 2019-01-01 Phra Nakhon Si Ayutthaya central occupancy_rate  73.4\n4 2019-01-01 Saraburi                 central occupancy_rate  67.3\n5 2019-01-01 Chainat                  central occupancy_rate  79.3\n6 2019-01-01 Nakhon Pathom            central occupancy_rate  71.7\n\n\nWe have kept only five of the columns which provides the date, the English descriptions for the location (province and region) as well as the (potential) tourism indicator and its value.\nWe can also check for any missing values across these five columns using the code below. (using is.na() and colSums())\n\ncolSums(is.na(tourism))\n\n     Date  Province    Region Indicator     Value \n        0         0         0         0         0 \n\n\nEach province will be repeated across multiple dates and across multiple indicators. Let us first doublecheck the different values in Indicator. We use unique() in the code chunk below to achieve this.\n\nunique(tourism$Indicator)\n\n[1] \"occupancy_rate\"      \"no_tourist_occupied\" \"no_tourist_all\"     \n[4] \"no_tourist_thai\"     \"no_tourist_foreign\"  \"net_profit_all\"     \n[7] \"net_profit_thai\"     \"net_profit_foreign\" \n\n\nWe are aware that the ‘net_profit’ indicators are actually revenue so it is better to update them now to avoid misunderstanding later. We use recode() from dplyr to replace instances with alternative values.\n\ntourism &lt;- tourism %&gt;%\n  mutate(Indicator = recode(Indicator,\n                            \"net_profit_all\" = \"revenue_all\",\n                            \"net_profit_thai\" = \"revenue_thai\",\n                            \"net_profit_foreign\" = \"revenue_foreign\"))\n\nunique(tourism$Indicator)\n\n[1] \"occupancy_rate\"      \"no_tourist_occupied\" \"no_tourist_all\"     \n[4] \"no_tourist_thai\"     \"no_tourist_foreign\"  \"revenue_all\"        \n[7] \"revenue_thai\"        \"revenue_foreign\"    \n\n\nWe will not define which indicators to use until we perform some EDA (Exploratory Data Analysis) in the next section.\nBefore we move to the next section, we will also introduce some columns into the dataset to make filtering and other analysis easier. For now, we will do this by adding columns for the months and years based on the Date column.\n\ntourism &lt;- tourism %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  mutate(MonthNum = month(Date)) %&gt;%\n  mutate(Month = month(Date, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(MonthYear = format(ymd(Date), \"%Y-%m\"))\n\nhead(tourism)\n\n# A tibble: 6 × 9\n  Date       Province      Region Indicator Value  Year MonthNum Month MonthYear\n  &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt;    \n1 2019-01-01 Bangkok       centr… occupanc…  93.4  2019        1 Jan   2019-01  \n2 2019-01-01 Lopburi       centr… occupanc…  61.3  2019        1 Jan   2019-01  \n3 2019-01-01 Phra Nakhon … centr… occupanc…  73.4  2019        1 Jan   2019-01  \n4 2019-01-01 Saraburi      centr… occupanc…  67.3  2019        1 Jan   2019-01  \n5 2019-01-01 Chainat       centr… occupanc…  79.3  2019        1 Jan   2019-01  \n6 2019-01-01 Nakhon Pathom centr… occupanc…  71.7  2019        1 Jan   2019-01"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.1-tourism-revenue",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.1-tourism-revenue",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.1 Tourism Revenue",
    "text": "C.1 Tourism Revenue\nWe first look at tourism revenue which is currently reported in million Thai baht. We will use a constant rate of 34.784 THB per USD based on 2023 exchange rates to scale down the numbers and transform it into something more recognizable for most of the readers.\nWe first create a plot for the monthly tourism revenue in total and by foreign and local tourists. The code below selects the relevant data and prepares the line plot using ggplot(). Finally, we use ggplotly() to render it as an interactive chart so we can easily examine the resulting chart.\n\n# Subset the data to just the required indicators\naggregated_data &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"revenue_all\", \"revenue_thai\", \"revenue_foreign\")) %&gt;%\n  group_by(MonthYear, Indicator) %&gt;%\n  summarise(TotalValue = sum(Value, na.rm = TRUE) / 34.784) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'MonthYear'. You can override using the\n`.groups` argument.\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thai Tourism Revenue by Month\",\n       y = \"Million USD\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"revenue_all\" = \"blue\", \"revenue_thai\" = \"green\", \"revenue_foreign\" = \"red\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe resulting chart is consistent with the expectation on the impact and recovery from the pandemic. We can view the above chart as a timeline:\n\nUp to (mid)January 2020: Pre-covid. No travel restrictions have been set yet. (Jan 2019 - Jan 2020, 13 months)\nFeb 2020 to November 2021: Covid. Various lockdown measures in place. All foreign non-essential travel is banned. There is some local tourist activity, but another set of measures in May 2021 again prevents non-essential movement (Feb 2020 - Oct 2021, 22 months)\nNovember 2021 onwards: Post-Covid. Travel restrictions have been eased or lifted and tourism revenues have been recovering (Nov 2021 - Feb 2023, 16 months)\n\nPre- and post-covid, we see that foreign tourists contribute more to the overall revenue, and their contribution has a large amount of variance. Local tourists during the same period have contributed a more stable amount month-on-month.\nWe can code the three periods mentioned above into the tourism dataset for convenience. We use the ifelse() function to do this based on the cutoff dates mentioned above.\n\ntourism$Period &lt;- ifelse(tourism$Date &lt; as.Date(\"2020-02-01\"), \"Pre-Covid\",\n                         ifelse(tourism$Date &gt; as.Date(\"2021-10-01\"), \"Post-Covid\", \"Covid\"))\n\nWe can next check the tourism revenue at the province level. As plotting all 77 provinces across all the periods will not produce readable charts, we will focus on top 20 provinces for the different periods. We will also take the average monthly revenue rather than the total since each period has a different number of months.\nWe prepare a new dataframe that summarizes the indicators for each province in tourism. Aside from the average revenue per period, we will also compute for the average number of visitors as well as the average spend per visitor.\n\ntourism_period &lt;- tourism %&gt;%\n  group_by(Province) %&gt;%\n  summarise(\n    PreCovid_Revenue_total = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_total = sum(Value[Period == \"Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_total = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_all\"], na.rm = TRUE)/16/34.784,\n    PreCovid_Revenue_foreign = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_foreign = sum(Value[Period == \"Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_foreign = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_foreign\"], na.rm = TRUE)/16/34.784,\n    PreCovid_Revenue_thai = sum(Value[Period == \"Pre-Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/13/34.784,\n    Covid_Revenue_thai = sum(Value[Period == \"Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/22/34.784,\n    PostCovid_Revenue_thai = sum(Value[Period == \"Post-Covid\" & Indicator == \"revenue_thai\"], na.rm = TRUE)/16/34.784,\n    PreCovid_tourists_total = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/13,\n    Covid_tourists_total = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/22,\n    PostCovid_tourists_total = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_all\"], na.rm = TRUE)/16,\n    PreCovid_tourists_foreign = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/13,\n    Covid_tourists_foreign = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/22,\n    PostCovid_tourists_foreign = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_foreign\"], na.rm = TRUE)/16,\n    PreCovid_tourists_thai = sum(Value[Period == \"Pre-Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/13,\n    Covid_tourists_thai = sum(Value[Period == \"Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/22,\n    PostCovid_tourists_thai = sum(Value[Period == \"Post-Covid\" & Indicator == \"no_tourist_thai\"], na.rm = TRUE)/16\n  ) %&gt;%\n  mutate(PreCovidSpend_total = PreCovid_Revenue_total / PreCovid_tourists_total * 1000000) %&gt;%\n  mutate(CovidSpend_total = Covid_Revenue_total / Covid_tourists_total * 1000000) %&gt;%\n  mutate(PostCovidSpend_total = PostCovid_Revenue_total / PostCovid_tourists_total * 1000000) %&gt;%\n  mutate(PreCovidSpend_foreign = PreCovid_Revenue_foreign / PreCovid_tourists_foreign * 1000000) %&gt;%\n  mutate(CovidSpend_foreign = Covid_Revenue_foreign / Covid_tourists_foreign * 1000000) %&gt;%\n  mutate(PostCovidSpend_foreign = PostCovid_Revenue_foreign / PostCovid_tourists_foreign * 1000000) %&gt;%\n  mutate(PreCovidSpend_thai = PreCovid_Revenue_thai / PreCovid_tourists_thai * 1000000) %&gt;%\n  mutate(CovidSpend_thai = Covid_Revenue_thai / Covid_tourists_thai * 1000000) %&gt;%\n  mutate(PostCovidSpend_thai = PostCovid_Revenue_thai / PostCovid_tourists_thai * 1000000)\n\nWith the summarized dataframe prepared, we can now prepare a few visualizations to look at the provinces with regards to the average monthly tourism revenue.\nFirst, let us try using a scatterplot to see both the average revenue pre-Covid (x-axis) and post-Covid. (y-axis) Provinces with the highest pre-Covid revenue will appear the rightmost, while those that have the highest post-Covid revenue will appear the rightmost.\nThe code below uses the plotly package to produce an interactive scatterplot of the pre- and post-covid average monthly revenue for all tourists. With the interactive chart, the province names will be visible by hovering over and the user can zoom in by creating a selection in the chart.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_Revenue_total,\n  y = ~PostCovid_Revenue_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue - M-USD',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBaed on the plot, we see that Bangkok, Phuket and Chonburi have consistently been the top 3 highest revenue generating before and after the pandemic. When we look further down the list, we see some shifts for some of the provinces.\nTo aid the reader, we recreate the chart with those top 3 provinces excluded using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = filter(tourism_period, !(Province %in% c(\"Bangkok\", \"Phuket\", \"Chonburi\"))),\n  x = ~PreCovid_Revenue_total,\n  y = ~PostCovid_Revenue_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue - M-USD, exc Top 3 Provinces',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nSome key observations from the above charts are:\n\nChiang Mai has moved from top 5 to top 4. A large reason for this is a drop from Krabi. Pre-covid, Krabi was top 4, but has dropped to at least top 10.\nChiang Rai and Prachuap Khiri Khan have risen to top 5 and 6. These provinces were top 9 or lower before.\nSongkhla and Phang Nga were in the top 10 pre-Covid but are also showing a drop in ranking post-Covid\n\nWe can do the same chart for just the revenue from foreign tourists using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_Revenue_foreign,\n  y = ~PostCovid_Revenue_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Revenue from Foreign Tourists - M-USD',\n    xaxis = list(title = 'Average PreCovid Revenue'),\n    yaxis = list(title = 'Average PostCovid Revenue')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nThe top 3 provinces are the same for both, but there are differences further down the list.\nWe can also show these numbers graphically in a map. Before we do this, let us add one set of measures in tourism_period to indicate the recovery rate. This will be the ratio of the post-covid and pre-covid measures and will indicate how much of the pre-covid level has been achieved. (on average)\nWe perform this using the code chunk below. We do this for all sets of measures across revenue, number of tourists and the average spending.\n\ntourism_period &lt;- tourism_period %&gt;%\n  mutate(Revenue_total_recovery = PostCovid_Revenue_total / PreCovid_Revenue_total) %&gt;%\n  mutate(Revenue_foreign_recovery = PostCovid_Revenue_foreign / PreCovid_Revenue_foreign) %&gt;%\n  mutate(Revenue_thai_recovery = PostCovid_Revenue_thai / PreCovid_Revenue_thai) %&gt;%\n  mutate(Tourists_total_recovery = PostCovid_tourists_total / PreCovid_tourists_total) %&gt;%\n  mutate(Tourists_foreign_recovery = PostCovid_tourists_foreign / PreCovid_tourists_foreign) %&gt;%\n  mutate(Tourists_thai_recovery = PostCovid_tourists_thai / PreCovid_tourists_thai) %&gt;%\n  mutate(Spend_total_recovery = PostCovidSpend_total / PreCovidSpend_total) %&gt;%\n  mutate(Spend_foreign_recovery = PostCovidSpend_foreign / PreCovidSpend_foreign) %&gt;%\n  mutate(Spend_thai_recovery = PostCovidSpend_thai / PreCovidSpend_thai)\n\nWe need to include all the indicators into the sf dataframe. This means merging the tourism_period and the thai_sf dataframes. Let us first check that the naming is the same for both dataframes by checking which values do not have a match. We use the code below which uses left_join() to match and then filter() to check those that do not have matches.\n\n# Identify mismatched Province names in tourism_period\nmismatched_values &lt;- tourism_period %&gt;%\n  left_join(thai_sf, by = \"Province\") %&gt;%\n  filter(is.na(ProvCode)) %&gt;%\n  select(Province)\n\nmismatched_tourism &lt;- mismatched_values$Province\n\n# Identify mismatched Province names in thai_sf\nmismatched_values &lt;- thai_sf %&gt;%\n  left_join(tourism_period, by = \"Province\") %&gt;%\n  filter(is.na(Covid_Revenue_total)) %&gt;%\n  select(Province)\n\nmismatched_thai &lt;- mismatched_values$Province\n\n# Print the mismatched values\nlist(\n  mismatched_in_tourism_period = mismatched_tourism,\n  mismatched_in_thai_sf = mismatched_thai\n)\n\n$mismatched_in_tourism_period\n[1] \"Buriram\"         \"Chainat\"         \"Chonburi\"        \"Lopburi\"        \n[5] \"Nong Bua Lamphu\" \"Phang Nga\"       \"Prachinburi\"     \"Sisaket\"        \n\n$mismatched_in_thai_sf\n[1] \"Lop Buri\"         \"Chai Nat\"         \"Chon Buri\"        \"Prachin Buri\"    \n[5] \"Buri Ram\"         \"Si Sa Ket\"        \"Nong Bua Lam Phu\" \"Phangnga\"        \n\n\nWe see that there are 8 mismatched province names for each of the dataframes. We need to standardize these namings to ensure that the indicators are mapped to the correct province. We will opt to keep the descriptions from tourism_period which gives more compact naming. We use recode() in the code chunk below to accomplish this in a new dataframe.\n\nthaitourism_sf &lt;- thai_sf %&gt;%\n  mutate(Province = recode(Province,\n                            \"Lop Buri\" = \"Lopburi\",\n                            \"Chai Nat\" = \"Chainat\",\n                            \"Chon Buri\" = \"Chonburi\",\n                            \"Prachin Buri\" = \"Prachinburi\",\n                            \"Buri Ram\" = \"Buriram\",\n                            \"Si Sa Ket\" = \"Sisaket\",\n                            \"Nong Bua Lam Phu\" = \"Nong Bua Lamphu\",\n                            \"Phangnga\" = \"Phang Nga\"))\n\nWe can now use leftjoin() in the codechunk below to merge the two datasets.\n\nthaitourism_sf &lt;- left_join(thaitourism_sf, tourism_period,\n                     by=c(\"Province\"=\"Province\"))\n\nThe code chunk below confirms that the new object is still an sf dataframe.\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can use the tmap package to produce side-by side maps of Thailand with the average monthly tourism revenue before and after covid using the code chunk below. Given the wide range of values, we will use quantiles for the data classes. We also include the recovery rate of each of the provinces as a third map.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_Revenue_total\", \"PostCovid_Revenue_total\", \"Revenue_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Monthly Revenue - M-USD\", \"Monthly Revenue - M-USD\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nAs also seen in the scatterplot, we also see some change in rankings with the maps. For example, some provinces previously in the top 20% have moved down to the next 20%. (e.g., Khon Kaen and Phang Nga)\nIf we focus on the third map, we also see what seems like a cluster of provinces in the south which are lagging with regards to their recovery on the average tourism revenue. We will watch out for these once we conduct our cluster analyses."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.2-number-of-tourists",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.2-number-of-tourists",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.2 Number of Tourists",
    "text": "C.2 Number of Tourists\nThe next measure we can look at is the number of tourists. We produce a similar line chart as we did for tourism revenue with the code chunk below. We display the number of tourists in thousands.\n\n# Subset the data to just the required indicators\naggregated_data &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"no_tourist_all\", \"no_tourist_thai\", \"no_tourist_foreign\")) %&gt;%\n  group_by(MonthYear, Indicator) %&gt;%\n  summarise(TotalValue = sum(Value, na.rm = TRUE) / 1000) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'MonthYear'. You can override using the\n`.groups` argument.\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thailand Number of Tourists by Month\",\n       y = \"Tourists, Thousands\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"no_tourist_all\" = \"blue\", \"no_tourist_thai\" = \"green\", \"no_tourist_foreign\" = \"red\"))\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe trend for the total follows the same general movement as the chart for revenue, however, it looks like tourist numbers are primarily driven by locals than foreigners.\nSimilar to the previous section, we can produce an interactive scatterplot to see the number of tourists each province gets on average before and after the pandemic. We do this first for the total number of tourists.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_tourists_total,\n  y = ~PostCovid_tourists_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid No of Tourists',\n    xaxis = list(title = 'PreCovid Average'),\n    yaxis = list(title = 'PostCovid Average')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBangkok, Chonburi and Phuket had the highest number of visitors pre-Covid, but Phuket appears to have dropped off in favor of Kanchanaburi post-covid.\nWe can also look at the foreign tourists using the code chunk below. We skip local tourists and focus only on foreign ones as they deviate from the overall number.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovid_tourists_foreign,\n  y = ~PostCovid_tourists_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid No of Foreign Tourists',\n    xaxis = list(title = 'PreCovid Average'),\n    yaxis = list(title = 'PostCovid Average')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBangkok, Phuket and Chonburi appear as the top 3 destinations for foreign tourists (in terms of number) before and after Covid. We also see that Krabi dropped from the top 3 post covid.\nWe can also produce side-by-side maps for the number of tourists and their recovery rates using the code chunk below.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_tourists_total\", \"PostCovid_tourists_total\", \"Tourists_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Monthly Tourists\", \"Monthly Tourists\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWe again see the southern region mostly lagging with regards to their recovery post covid also in terms of the number of visitors."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.3-average-spend-per-visitor",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.3-average-spend-per-visitor",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.3 Average Spend Per Visitor",
    "text": "C.3 Average Spend Per Visitor\nWe next look at the average per spend per visitor which is the quotient of the tourism revenue and the total number of tourists. This will tell us whether tourists are spending more or less around the pandemic.\nFirst we produce a similar line graph as before to look at the trend at an overall picture using the code chunk below.\n\n# Subset the data to just the required indicators\naggregated_data &lt;&lt;- tourism %&gt;%\n  group_by(MonthYear) %&gt;%\n  summarise(\n    Spend_total = sum(Value[Indicator == \"revenue_all\"]) / sum(Value[Indicator == \"no_tourist_all\"]) * 1000000 / 34.784,\n    Spend_thai = sum(Value[Indicator == \"revenue_thai\"]) / sum(Value[Indicator == \"no_tourist_thai\"]) * 1000000 / 34.784,\n    Spend_foreign = sum(Value[Indicator == \"revenue_foreign\"]) / sum(Value[Indicator == \"no_tourist_foreign\"]) * 1000000 / 34.784\n  ) %&gt;%\n  pivot_longer(cols = starts_with(\"Spend\"), names_to = \"Indicator\", values_to = \"TotalValue\") %&gt;%\n  mutate(Indicator = case_when(\n    Indicator == \"Spend_total\" ~ \"Spend_total\",\n    Indicator == \"Spend_thai\" ~ \"Spend_thai\",\n    Indicator == \"Spend_foreign\" ~ \"Spend_foreign\"\n  ))\n\n# Create the line chart\np &lt;- ggplot(aggregated_data, aes(x = MonthYear, y = TotalValue, color = Indicator, group = Indicator)) +\n  geom_line(size = 1) +\n  labs(title = \"Thailand Average Spend Per Tourist by Month\",\n       y = \"Average Spend, USD\",\n       x = \"Month-Year\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"Spend_total\" = \"blue\", \"Spend_thai\" = \"green\", \"Spend_foreign\" = \"red\"))\n\n# Convert the ggplot chart to an interactive plotly chart\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThe chart shows that the average spend of foreign tourists are much higher than local ones and appears to be the same before and after Covid. There appears to be a shift in the average spend for all tourists overall which is probably driven by an increase in the contribution for the number of local tourists versus foreign tourists. Local tourists appear to show a step decrease after covid as well in terms of their average spending.\nNext, we check the average spending of tourists across provinces using a similar scatterplot as before.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovidSpend_total,\n  y = ~PostCovidSpend_total,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Average Spend Per Tourist',\n    xaxis = list(title = 'PreCovid Average $'),\n    yaxis = list(title = 'PostCovid Average $')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nPhuket saw the highest average spend pre- and post-covid. Krabi was second highest pre-Covid but drop to third, as Bangkok rose from fourth to second over that period.\nWe can do the same for foreign tourists as the earlier chart showed that it was very different from the total. We use the code chunk below tor produce a similar scatterplot but taking the foreign tourist figures.\n\nplot &lt;- plot_ly(\n  data = tourism_period,\n  x = ~PreCovidSpend_foreign,\n  y = ~PostCovidSpend_foreign,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~Province,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Average Spend Per Foreign Tourist',\n    xaxis = list(title = 'PreCovid Average $'),\n    yaxis = list(title = 'PostCovid Average $')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nPhuket, Bangkok and Chonburi have been consistent as top 4 highest spend for foreign tourists. From the chart above, we see that Chiang Rai has risen to the number five spot for highest foreign tourist spending post-Covid.\nWe can also produce a map visualization of the average spend using tmap package. We focus on the total tourist population in the visualization below.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovidSpend_total\", \"PostCovidSpend_total\", \"Spend_total_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Average Tourist Spending\", \"Average Tourist Spending\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWe again see slow recovery in the southern region, but at the same time, this region has the highest average spending pre- and post-covid. (i.e., top 20%)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.4-occupancy-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#c.4-occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "C.4 Occupancy Rate",
    "text": "C.4 Occupancy Rate\nThe final single indicator we will look at is the occupancy rate. We have not prepared and included the data for occupancy rate before as this is a ratio measure which be just summed or averaged. In order to be able to aggregate occupancy rate, we not only need the actual occupancy rate from the data, but we also need the number of rooms occupied which is given by the no_tourist_occupied indicator in the data, and also the number of rooms in total– which is not included in the data.\nThe following code chunk prepares a new dataframe from tourist with the following transformation steps:\n\nRetain MonthYear, Province, and records for occupany rate and number of rooms occupied\nKeep values for the two indicators as separate columns. We use average just in case a province appears multiple times on the same date (to resolve conflicts)\nWe compute the total number of rooms as the number of rooms occupied divided by the occupancy rate\nWe add the tags for the period for pre- and post- covid\n\n\noccupancy_df &lt;- tourism %&gt;%\n  filter(Indicator %in% c(\"occupancy_rate\", \"no_tourist_occupied\")) %&gt;%\n  group_by(Date, MonthYear, Province) %&gt;%\n  summarise(\n    occupancy = mean(Value[Indicator == \"occupancy_rate\"], na.rm = TRUE),\n    occupied_rooms = mean(Value[Indicator == \"no_tourist_occupied\"], na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(total_rooms = ifelse(occupancy == 0, 0, occupied_rooms / occupancy * 100))\n\n`summarise()` has grouped output by 'Date', 'MonthYear'. You can override using\nthe `.groups` argument.\n\noccupancy_df$Period &lt;- ifelse(occupancy_df$Date &lt; as.Date(\"2020-02-01\"), \"Pre-Covid\",\n                         ifelse(occupancy_df$Date &gt; as.Date(\"2021-10-01\"), \"Post-Covid\", \"Covid\"))\n\nhead(occupancy_df)\n\n# A tibble: 6 × 7\n  Date       MonthYear Province      occupancy occupied_rooms total_rooms Period\n  &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n1 2019-01-01 2019-01   Amnat Charoen      65.2           8551      13125. Pre-C…\n2 2019-01-01 2019-01   Ang Thong          71.2          19140      26878. Pre-C…\n3 2019-01-01 2019-01   Bangkok            93.4        3334971    3571780. Pre-C…\n4 2019-01-01 2019-01   Bueng Kan          73.0          37974      52055. Pre-C…\n5 2019-01-01 2019-01   Buriram            71.3         113655     159493. Pre-C…\n6 2019-01-01 2019-01   Chachoengsao       59.4          38687      65130. Pre-C…\n\n\nBefore we produce the charts, let us update thaitourism_sf with the aggregated occupancy rates by:\n\nComputing Pre- and Post-covid total number of rooms and occupied rooms per province\nComputing Pre- and Post-covid occupancy rate per province based on step 1\nAdd the new columns into thaitourism_sf using left_join()\n\nThe first two steps are accomplished in the first code block while the third step is accomplished in the second.\n\noccupancy_summary &lt;- occupancy_df %&gt;%\n  group_by(Province) %&gt;%\n  summarise(\n    PreCovid_occupied_rooms = sum(occupied_rooms[Period == \"Pre-Covid\"], na.rm = TRUE),\n    PostCovid_occupied_rooms = sum(occupied_rooms[Period == \"Post-Covid\"], na.rm = TRUE),\n    PreCovid_total_rooms = sum(total_rooms[Period == \"Pre-Covid\"], na.rm = TRUE),\n    PostCovid_total_rooms = sum(total_rooms[Period == \"Post-Covid\"], na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    PreCovid_occupancy = ifelse(PreCovid_total_rooms == 0, 0, (PreCovid_occupied_rooms / PreCovid_total_rooms) * 100),\n    PostCovid_occupancy = ifelse(PostCovid_total_rooms == 0, 0, (PostCovid_occupied_rooms / PostCovid_total_rooms) * 100)\n  )\n\nhead(occupancy_summary)\n\n# A tibble: 6 × 7\n  Province    PreCovid_occupied_ro…¹ PostCovid_occupied_r…² PreCovid_total_rooms\n  &lt;chr&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n1 Amnat Char…                 106667                  91166              189198.\n2 Ang Thong                   208960                 116396              324038.\n3 Bangkok                   39621389               23666935            47956613.\n4 Bueng Kan                   362415                 465507              608851.\n5 Buriram                    1319062                1827050             2137721.\n6 Chachoengs…                 518580                 372576              917461.\n# ℹ abbreviated names: ¹​PreCovid_occupied_rooms, ²​PostCovid_occupied_rooms\n# ℹ 3 more variables: PostCovid_total_rooms &lt;dbl&gt;, PreCovid_occupancy &lt;dbl&gt;,\n#   PostCovid_occupancy &lt;dbl&gt;\n\n\n\nthaitourism_sf &lt;- left_join(thaitourism_sf, occupancy_summary, by = c(\"Province\"=\"Province\"))\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nFinally, we add a column for the recovery of the occupancy rate using the code chunk below.\n\nthaitourism_sf &lt;- mutate(thaitourism_sf, Occupancy_recovery = ifelse(PreCovid_occupancy == 0, 0, (PostCovid_occupancy / PreCovid_occupancy)), .before = -1)\n\n\nclass(thaitourism_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nFor the first visualization, let us plot the occupancy rate at a total level. The code below summarizes based on occupancy_df and computes a national occupancy rate to plot in a line chart.\n\naggregated_data &lt;&lt;- occupancy_df %&gt;%\n  group_by(MonthYear) %&gt;%\n  summarise(\n    occupancy_rate = sum(occupied_rooms, na.rm = TRUE) / sum(total_rooms, na.rm = TRUE) * 100\n  )\n\nggplot(aggregated_data, aes(x = MonthYear, y = occupancy_rate, group = 1)) +\n  geom_line() +\n  labs(\n    title = \"Occupancy Rate Over Time\",\n    x = \"MonthYear\",\n    y = \"Occupancy Rate (%)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThe chart shows that occupancy rate has picked up after October 2021 and appears to have more or less reached pre-covid levels in the most recent months.\nWe can also produce a scatterplot to show pre- and post-covid occupancy rates at a province level. We do this using the code chunk below which produces an interactive plot.\n\nplot &lt;- plot_ly(\n  data = occupancy_summary,\n  x = ~PreCovid_occupancy,\n  y = ~PostCovid_occupancy,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste('Province:', Province, '&lt;br&gt;PreCovid Occupancy:', round(PreCovid_occupancy), '&lt;br&gt;PostCovid Occupancy:', round(PostCovid_occupancy)),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Monthly Pre- and Post-Covid Occupancy Rate (%)',\n    xaxis = list(title = 'PreCovid Occupancy'),\n    yaxis = list(title = 'PostCovid Occupancy')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nThe occupancy rates are showing a dispersed pattern as the overall rankings on occupancy rates have significantly changed pre- and post-covid. Bangkok, Chonburi and Suphan Buri reported the highest occupancy rates before covid, but Nan, Chang Rai and Nakhon Phanom have the highest rates post covid.\nThe next visualization for this measure is a side-by-side map for the pre- and post-covid occupancy rates as well as the recovery rate for occupancy. We use the code chunk below which uses tmap package to produce the maps.\n\ntm_shape(thaitourism_sf) +\n  tm_fill(col = c(\"PreCovid_occupancy\", \"PostCovid_occupancy\", \"Occupancy_recovery\"),\n          style = \"quantile\",\n          palette = list(\"viridis\", \"viridis\", \"RdYlGn\"),\n          title = c(\"Average Occupancy Rate\", \"Average Occupancy Rate\", \"Post vs Pre\")) +\n  tm_borders(col = \"black\") +\n  tm_layout(title = c(\"Pre Covid\", \"Post Covid\", \"Recovery Rate\"),\n            title.position = c(\"right\", \"top\"),\n            legend.position = c(\"right\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nBefore we leave this section, let us try to understand occupancy rate a bit more. Going back to earlier, we want to understand if high occupancy rate post-Covid is being driven by the number of available rooms. To help us answer this, we create a scatterplot of the available rooms against the occupancy rate post-Covid using the code chunk below.\n\nplot &lt;- plot_ly(\n  data = occupancy_summary,\n  x = ~PostCovid_total_rooms,\n  y = ~PostCovid_occupancy,\n  type = 'scatter',\n  mode = 'markers',\n  text = ~paste('Province:', Province, '&lt;br&gt;Number of Rooms:', round(PostCovid_total_rooms), '&lt;br&gt;Occupancy Rate:', round(PostCovid_occupancy)),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = 'Post-Covid Number of Rooms and Occupancy Rate (%)',\n    xaxis = list(title = 'Number of Rooms'),\n    yaxis = list(title = 'Occupancy Rate')\n  )\n\n# Display the plot\nplot\n\n\n\n\n\nBelow 10M rooms, there appears to be an upward trend in occupancy rate to the number of rooms. Provinces with more than 10M rooms go against the trend and appear to be capped to 60% occupancy."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.1-variable-selection",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.1-variable-selection",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.1 Variable Selection",
    "text": "D.1 Variable Selection\nWe will turn our attention to the first three measures discussed discussed in the previous section: tourism revenue, number of tourists and average tourist spending. We will not analyse the occupancy rate further as it is highly dependent on the number of rooms.\nWe will focus on checking signs of spatial autocorrelation or association before and after covid, as well as the recovery rate at the overall level for these three indicators– so we will be looking at 9 variables for our analysis."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.2-deriving-the-contiguity-and-weight-matrix",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.2-deriving-the-contiguity-and-weight-matrix",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.2 Deriving the Contiguity and Weight Matrix",
    "text": "D.2 Deriving the Contiguity and Weight Matrix\nFor the tests for this section, we need to derive a neighbor list as well as a weight matrix for each province to its neighbors. Given the presence of islands, we need to use distance rather than contiguity to define neighbors.\nThe first step is to understand the distribution of distances between nearest neighbors to find a proper cut-off distance. The code chunk below\n\nlongitude &lt;- map_dbl(thaitourism_sf$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(thaitourism_sf$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.55   51.92   64.33   63.24   76.77  110.94 \n\n\nThe maximum distance is 110.94 so setting a distance threshold of 111 should ensure that each province should have at least one neighbor. We then produce a nearest neighbor list for each province using dnearneigh()\n\nwm_d111 &lt;- dnearneigh(coords, 0, 111, longlat = TRUE)\nwm_d111\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 350 \nPercentage nonzero weights: 5.903188 \nAverage number of links: 4.545455 \n2 disjoint connected subgraphs\n\n\nWe import this as a new column in our sf object and compute for weights using st_weights() based on this.\n\nwm_thai &lt;- thaitourism_sf %&gt;%\n  mutate(nb = I(wm_d111),\n         wt = st_weights(nb,\n                         style=\"W\"),\n         .before=1)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.3-global-morans-i-test",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#d.3-global-morans-i-test",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "D.3 Global Moran’s I Test",
    "text": "D.3 Global Moran’s I Test\nGlobal tests of spatial autocorrelation compares the value of each point/province to the overall value in order to conclude on spatial dependence. For this, we will focus on using Global Moran’s I which will work on the following general hypotheses:\n\n\\(H_0\\) - The value of (variable) is randomly distributed across provinces in Thailand\n\\(H_1\\) - The value of (variable) is not randomly distributed across provinces in Thailand\n\nFurther, the value of the test statistic \\(I\\) will also give indication on the underlying pattern:\n\n\\(I &gt; 0\\) - Clustering; observations tend to be similar\n\\(I &lt; 0\\) - Dispersed / regular; observations tend to be dissimilar\nWhere \\(I\\) close to zero - observations are arranged randomly\n\nTo perform Global Moran’s I test, with permutations, we use global_moran_perm() from sfdep package. We will use a 5% significance level for all the testing to be performed, and we will run 100 permutations / simulations for each test.\n\nD.3.1 Global Moran’s Test on Tourism Revenue\nThe code chunks in the tabs below run the Global Moran’s I permutation test on total tourism revenue (per month) pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid RevenuePost-Covid RevenueRevenue Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovid_Revenue_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.049413, observed rank = 93, p-value = 0.14\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovid_Revenue_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.020361, observed rank = 84, p-value = 0.32\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Revenue_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.43763, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nThe results show no evidence to reject spatial independence for the total revenue pre- and post-Covid. However, it shows signs of clustering for the revenue recovery rate as the p-value is below 0.05 and the statistic is above 1.\n\n\nD.3.2 Global Moran’s Test on Number of Tourists\nThe code chunks in the tabs below run the Global Moran’s I permutation test on total number of tourists (per month) pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid Number of TouristsPost-Covid Number of TouristsNumber of Tourists Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovid_tourists_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.062493, observed rank = 92, p-value = 0.16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovid_tourists_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.10696, observed rank = 95, p-value = 0.1\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Tourists_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.27768, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nWe see similar results here. The results show no evidence to reject spatial independence for the total number of tourists pre- and post-Covid. However, it shows signs of clustering for the number of tourists recovery rate as the p-value is below 0.05 and the statistic is above 1.\n\n\nD.3.3 Global Moran’s Test on Average Tourist Spend\nThe code chunks in the tabs below run the Global Moran’s I permutation test on average tourist spend pre-Covid, post-Covid and the recovery rate.\n\nPre-Covid Average SpendPost-Covid Average SpendAverage Spend Recovery Rate\n\n\n\nglobal_moran_perm(wm_thai$PreCovidSpend_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.423, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$PostCovidSpend_total,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.20651, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nglobal_moran_perm(wm_thai$Spend_total_recovery,\n                  wm_thai$nb,\n                  wm_thai$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.31497, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\nAverage tourist spending is showing signs of clustering on all dimensions (pre-Covid, post-Covid and the recovery rate) as the p-value is below 0.05 and the I statistic is above 0 in all cases.\n\n\nD.3.3 Global Moran’s Test Summary\nBased on the results of the testing on the total revenue, number of tourists and spend, we see that the following variables are not exhibiting a random distribution, and show signs of clustering:\n\nTotal tourism revenue recovery rate\nTotal number of tourists recovery rate\nPre-Covid Average spend per Tourist\nPost-Covid Average spend per Tourist\nAverage spend per tourist recovery rate\n\nWe will conduct tests for local association to identify the clusters and outliers among provinces for each of these variables."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.1-analysing-lisa-total-tourism-recovery-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.1-analysing-lisa-total-tourism-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.1 Analysing LISA: Total tourism recovery rate",
    "text": "E.1 Analysing LISA: Total tourism recovery rate\nWe first compute for the LISA for the recovery rate of the total number of tourists using local_moran() function in the code chunk below. The code below uses 100 simulations to produce the test results.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Revenue_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe can then visualize the test statistic and p-values for each province in a map using tmap package in the code chunk below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(c(\"ii\", \"p_ii_sim\"), title = c(\"Local Moran's I\",\"P Value\")) +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"LISA for Total Revenue Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"grey90\")\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nOutliers are generally provinces where the test statistic is negative, and clusters where it is positive– if they are significant. We see some potential outliers. We can produce a different set of plots to allow us to identify these types of provinces.\nUsing a LISA map, we can show graphically the location of clusters and outliers based on this (tourism revenue recovery rate)\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Revenue Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can also observe the contents of the object lisa_sig to see the statistics for the identified significant provinces. The code chunk below shows each class separately for easier reference.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 6.422716 xmax: 100.3366 ymax: 10.12626\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Krabi               Low-Low (((99.11329 7.489274, 99.11337 7.489274, 99.11343…\n3 Phang Nga           Low-Low (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7…\n4 Phuket              Low-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417…\n5 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n6 Satun               Low-Low (((100.0903 6.425736, 100.09 6.425543, 100.0896 6…\n7 Trang               Low-Low (((99.47579 6.97262, 99.47565 6.972616, 99.47537 …\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.984 ymin: 14.94191 xmax: 101.3582 ymax: 19.63808\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province    mean                                                      geometry\n  &lt;chr&gt;       &lt;fct&gt;                                           &lt;MULTIPOLYGON [°]&gt;\n1 Nan         High-High (((100.8948 19.63432, 100.8952 19.63431, 100.8957 19.63…\n2 Uthai Thani High-High (((99.13905 15.79655, 99.13918 15.79652, 99.13965 15.79…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nWe can summarize the findings from this analysis as:\n\nThere is a cluster of 7 provinces at the south of Thailand that have slower recovery in terms of their average tourism revenue. This includes popular destinations like Phuket and Krabi and their neighboring provinces\nThere is a cluster of 3 provinces in the north that have faster recovery on the same metric. This includes Chiang Rai, Nan and Phayao\nThe analysis revealed two outliers. Chachoengsao has high recovery while neighboring provinces are low. Nakhon Ratchasima has low recovery while neighboring provinces are high\n\nreveal the following"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.2-analysing-lisa-number-of-tourists-recovery-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.2-analysing-lisa-number-of-tourists-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.2 Analysing LISA: Number of tourists recovery rate",
    "text": "E.2 Analysing LISA: Number of tourists recovery rate\nWe compute for the LISA using local_moran() function for the number of tourist recovery rate in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Tourists_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will go straight to producing the LISA map based on the lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"No of Tourist Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nTo aid interpretation, we display results tabularly as before using the code chunk below.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 100.3366 ymax: 10.12626\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Krabi               Low-Low (((99.11329 7.489274, 99.11337 7.489274, 99.11343…\n3 Phang Nga           Low-Low (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7…\n4 Phuket              Low-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417…\n5 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.15364 ymin: 6.422716 xmax: 101.9901 ymax: 13.9767\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province     mean                                                     geometry\n  &lt;chr&gt;        &lt;fct&gt;                                          &lt;MULTIPOLYGON [°]&gt;\n1 Chachoengsao High-Low (((101.0612 13.97613, 101.0625 13.976, 101.0629 13.9760…\n2 Satun        High-Low (((100.0903 6.425736, 100.09 6.425543, 100.0896 6.42572…\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe findings can be summarized as:\n\nThere is a cluster at the south of five provinces with slower recovery with regards to the number of tourists– including Phuket and Krabi (similar to previous metric)\nSatun at the southern part has high recovery rate while its neighboring provinces are lower"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.3-analysing-lisa-pre-covid-average-spend",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.3-analysing-lisa-pre-covid-average-spend",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.3 Analysing LISA: Pre-Covid Average Spend",
    "text": "E.3 Analysing LISA: Pre-Covid Average Spend\nWe compute for the LISA using local_moran() function for the average tourist spending post-Covid in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    PreCovidSpend_total, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Pre-Covid Average Tourist Spend\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 10 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 14.06424 xmax: 105.637 ymax: 18.22037\nGeodetic CRS:  WGS 84\n# A tibble: 10 × 3\n   Province         mean                                                geometry\n   &lt;chr&gt;            &lt;fct&gt;                                     &lt;MULTIPOLYGON [°]&gt;\n 1 Lopburi          Low-Low (((101.3453 15.75254, 101.3457 15.75224, 101.3466 1…\n 2 Sing Buri        Low-Low (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.…\n 3 Chainat          Low-Low (((100.1199 15.41243, 100.121 15.41234, 100.1229 15…\n 4 Ubon Ratchathani Low-Low (((105.0633 16.09675, 105.0634 16.09671, 105.0638 1…\n 5 Yasothon         Low-Low (((104.3952 16.34843, 104.3983 16.34707, 104.4 16.3…\n 6 Khon Kaen        Low-Low (((102.7072 17.08713, 102.708 17.087, 102.7096 17.0…\n 7 Loei             Low-Low (((102.095 18.21708, 102.0962 18.21675, 102.0971 18…\n 8 Roi Et           Low-Low (((104.314 16.43758, 104.3135 16.43452, 104.3137 16…\n 9 Nakhon Sawan     Low-Low (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.…\n10 Suphan Buri      Low-Low (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 99.41499 ymax: 9.478956\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  Province  mean                                                        geometry\n  &lt;chr&gt;     &lt;fct&gt;                                             &lt;MULTIPOLYGON [°]&gt;\n1 Krabi     High-High (((99.11329 7.489274, 99.11337 7.489274, 99.11343 7.48929…\n2 Phang Nga High-High (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7.744467,…\n3 Phuket    High-High (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.47851…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe findings can be summarized as:\n\nThere are multiple clusters, totaling 10 provinces, in the center of Thailand that have low average spending pre-Covid. These are composed of lesser known tourist destinations\nPhuket and Phang Nga make up a two-province cluster with high average tourist spending\nThere are no outliers identified in the analysis"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.4-analysing-lisa-post-covid-average-spend",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.4-analysing-lisa-post-covid-average-spend",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.4 Analysing LISA: Post-Covid Average Spend",
    "text": "E.4 Analysing LISA: Post-Covid Average Spend\nWe compute for the LISA using local_moran() function for the average tourist spending post-Covid in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    PostCovidSpend_total, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Post-Covid Average Tourist Spend\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe again display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 14.06424 xmax: 105.637 ymax: 18.22037\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  Province         mean                                                 geometry\n  &lt;chr&gt;            &lt;fct&gt;                                      &lt;MULTIPOLYGON [°]&gt;\n1 Sing Buri        Low-Low (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.0…\n2 Ubon Ratchathani Low-Low (((105.0633 16.09675, 105.0634 16.09671, 105.0638 16…\n3 Loei             Low-Low (((102.095 18.21708, 102.0962 18.21675, 102.0971 18.…\n4 Roi Et           Low-Low (((104.314 16.43758, 104.3135 16.43452, 104.3137 16.…\n5 Mukdahan         Low-Low (((104.2527 16.89302, 104.2527 16.89274, 104.2527 16…\n6 Nakhon Sawan     Low-Low (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.1…\n7 Suphan Buri      Low-Low (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.0…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.63083 ymin: 7.467277 xmax: 99.41499 ymax: 9.478956\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  Province  mean                                                        geometry\n  &lt;chr&gt;     &lt;fct&gt;                                             &lt;MULTIPOLYGON [°]&gt;\n1 Krabi     High-High (((99.11329 7.489274, 99.11337 7.489274, 99.11343 7.48929…\n2 Phang Nga High-High (((98.61471 7.74431, 98.61461 7.74431, 98.61437 7.744467,…\n3 Phuket    High-High (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.47851…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 × 3\n# ℹ 3 variables: Province &lt;chr&gt;, mean &lt;fct&gt;, geometry &lt;GEOMETRY [°]&gt;\n\n\nThe results very closely resember the ones for Pre-Covid average spending. One significant change is that the high spend cluster at the south now includes Krabi. (so it now consists of three provinces)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.5-analysing-lisa-average-spend-recovery-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#e.5-analysing-lisa-average-spend-recovery-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "E.5 Analysing LISA: Average Spend Recovery Rate",
    "text": "E.5 Analysing LISA: Average Spend Recovery Rate\nWe compute for the LISA using local_moran() function for the average tourist spending recovery rate in the code chunk below.\n\nlisa &lt;- wm_thai %&gt;%\n  mutate(local_moran = local_moran(\n    Spend_total_recovery, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe will now produce the LISA map based on the generated lisa object. Again, we use 0.05 to determine statistical significance.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\", title = \"LISA Class α=5% \") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"Province\", size = 0.5) +\ntm_layout(\n    main.title = \"Average Tourist Spend Recovery Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe again display the results tabularly using the code chunk below to make it easier to interpret.\n\nlisa_sig[lisa_sig$mean == \"Low-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.32594 ymin: 5.613038 xmax: 101.7248 ymax: 10.78906\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 3\n  Province            mean                                              geometry\n  &lt;chr&gt;               &lt;fct&gt;                                   &lt;MULTIPOLYGON [°]&gt;\n1 Nakhon Si Thammarat Low-Low (((99.77467 9.313729, 99.77478 9.313647, 99.77488…\n2 Surat Thani         Low-Low (((99.96396 9.309907, 99.96376 9.30955, 99.96353 …\n3 Ranong              Low-Low (((98.35294 9.440758, 98.35316 9.440558, 98.3533 …\n4 Trang               Low-Low (((99.47579 6.97262, 99.47565 6.972616, 99.47537 …\n5 Pattani             Low-Low (((101.2827 6.952051, 101.2839 6.95182, 101.2848 …\n6 Yala                Low-Low (((101.2927 6.681118, 101.2937 6.679529, 101.2939…\n\nlisa_sig[lisa_sig$mean == \"High-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.01629 ymin: 15.3183 xmax: 101.7972 ymax: 17.178\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  Province       mean                                                   geometry\n  &lt;chr&gt;          &lt;fct&gt;                                        &lt;MULTIPOLYGON [°]&gt;\n1 Kamphaeng Phet High-High (((99.48875 16.91044, 99.48883 16.91016, 99.48884 16…\n2 Phetchabun     High-High (((101.3987 17.17792, 101.399 17.17781, 101.3993 17.…\n\nlisa_sig[lisa_sig$mean == \"High-Low\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.25791 ymin: 7.478502 xmax: 98.48333 ymax: 8.200333\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  Province mean                                                         geometry\n  &lt;chr&gt;    &lt;fct&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n1 Phuket   High-Low (((98.31437 7.478515, 98.31425 7.478502, 98.31417 7.478513,…\n\nlisa_sig[lisa_sig$mean == \"Low-High\", c(\"Province\", \"mean\")]\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.42 ymin: 14.64684 xmax: 101.4044 ymax: 15.75613\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  Province mean                                                         geometry\n  &lt;chr&gt;    &lt;fct&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n1 Lopburi  Low-High (((101.3453 15.75254, 101.3457 15.75224, 101.3466 15.75236,…\n\n\nThe findings can be summarized as:\n\nThere are two clusters at the south, totaling 5 provinces, that have slow recovery. This includes the provinces Surat Thani, Nakhon Si Thammarat, Trang, Pattani and Yala\nThere is a cluster of three provinces at the center that have high recovery rate. This includes Kanpaeng Phet, Phichit and Phetchabun\nPhuket is appearing as an outlier with high tourist spend recovery rate relative to its neighbors."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.1-ehsa---post-covid-tourism-revenue",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.1-ehsa---post-covid-tourism-revenue",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.1 EHSA - Post-covid tourism revenue",
    "text": "F.1 EHSA - Post-covid tourism revenue\nWe perform EHSA on the post-covid (2022 onwards) values for the total tourism revenue.\nWe create post-Covid versions of our datasets using the code chunk below so it is easier to refer to them later. We also include a column for the year and month as integers as EHSA requires discrete numeric values as a time variable.\n\ntourism_postCov &lt;- subset(tourism, Date &gt; as.Date(\"2021-12-31\"))\ntourism_postCov$YYYYMM &lt;- as.integer(format(tourism_postCov$Date, \"%Y%m\"))\n\noccupancy_postCov &lt;- subset(occupancy_df, Date &gt; as.Date(\"2021-12-31\"))\noccupancy_postCov$YYYYMM &lt;- as.integer(format(occupancy_postCov$Date, \"%Y%m\"))\n\n\nF.1.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the post-covid tourism revenue. We use filter() to only select the rows for the measure of interest, and then select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(filter(tourism_postCov, tourism_postCov$Indicator == \"revenue_all\"),\n                           YYYYMM, Province, Value),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.1.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    Value, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.1.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe Mann Kendall test checks for signs of monotonicity for a the local \\(G_i^*\\) statistic. Where the results are significant, the test infers that there are signs of monotonicity for that province / observation.\nThe code chunk below runs the Mann Kendall test (without permutations) on each province for the selected variable using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 33 × 6\n   Province         tau         sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen  0.956 0.00000250    87  91.0  334.\n 2 Bangkok        0.473 0.0215        43  91.0  334.\n 3 Bueng Kan      0.626 0.00217       57  91.0  334.\n 4 Buriram        0.758 0.000197      69  91.0  334.\n 5 Chaiyaphum     0.780 0.000127      71  91.0  334.\n 6 Chanthaburi    0.626 0.00217       57  91.0  334.\n 7 Kalasin        0.802 0.0000809     73  91.0  334.\n 8 Kamphaeng Phet 0.890 0.0000119     81  91.0  334.\n 9 Lampang        0.626 0.00217       57  91.0  334.\n10 Lamphun        0.956 0.00000250    87  91.0  334.\n# ℹ 23 more rows\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 21 × 6\n   Province        tau        sl     S     D  varS\n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ang Thong    -0.582 0.00442     -53  91.0  334.\n 2 Chachoengsao -0.802 0.0000809   -73  91.0  334.\n 3 Chainat      -0.626 0.00217     -57  91.0  334.\n 4 Chiang Rai   -0.429 0.0375      -39  91.0  334.\n 5 Chumphon     -0.890 0.0000119   -81  91.0  334.\n 6 Kanchanaburi -0.890 0.0000119   -81  91.0  334.\n 7 Mae Hong Son -0.429 0.0375      -39  91.0  334.\n 8 Mukdahan     -0.473 0.0215      -43  91.0  334.\n 9 Phetchabun   -0.824 0.0000510   -75  91.0  334.\n10 Phetchaburi  -0.802 0.0000809   -73  91.0  334.\n# ℹ 11 more rows\n\n\nThe results show that 33 of the 77 provinces are showing significant positive trend– which might be expected as provinces are recovering post-Covid. However, there are 21 provinces which are showing a significant negative trend which might be a concern if any of these are expected to be major tourist destinations.\n\n\nF.1.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"Value\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourism Revenue\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n  Province  classification         geometry                      \n1 \"Bangkok\" \"intensifying hotspot\" MULTIPOLYGON (((100.6139 13...\n   Province        classification        geometry                      \n2  \"Nonthaburi\"    \"consecutive hotspot\" MULTIPOLYGON (((100.3415 14...\n8  \"Khon Kaen\"     \"consecutive hotspot\" MULTIPOLYGON (((102.7072 17...\n10 \"Sakon Nakhon\"  \"consecutive hotspot\" MULTIPOLYGON (((103.5404 18...\n11 \"Nakhon Phanom\" \"consecutive hotspot\" MULTIPOLYGON (((104.192 18....\n15 \"Kanchanaburi\"  \"consecutive hotspot\" MULTIPOLYGON (((98.58631 15...\n   Province                   classification     \n3  \"Phra Nakhon Si Ayutthaya\" \"sporadic coldspot\"\n4  \"Sing Buri\"                \"sporadic coldspot\"\n5  \"Trat\"                     \"sporadic coldspot\"\n6  \"Chachoengsao\"             \"sporadic coldspot\"\n7  \"Buri Ram\"                 \"sporadic coldspot\"\n9  \"Nong Khai\"                \"sporadic coldspot\"\n12 \"Mukdahan\"                 \"sporadic coldspot\"\n13 \"Lamphun\"                  \"sporadic coldspot\"\n14 \"Lampang\"                  \"sporadic coldspot\"\n16 \"Prachuap Khiri Khan\"      \"sporadic coldspot\"\n17 \"Ranong\"                   \"sporadic coldspot\"\n18 \"Phatthalung\"              \"sporadic coldspot\"\n19 \"Pattani\"                  \"sporadic coldspot\"\n   geometry                      \n3  MULTIPOLYGON (((100.5131 14...\n4  MULTIPOLYGON (((100.3691 15...\n5  MULTIPOLYGON (((102.5216 11...\n6  MULTIPOLYGON (((101.0612 13...\n7  MULTIPOLYGON (((102.9303 15...\n9  MULTIPOLYGON (((103.2985 18...\n12 MULTIPOLYGON (((104.2527 16...\n13 MULTIPOLYGON (((99.18821 18...\n14 MULTIPOLYGON (((99.58445 19...\n16 MULTIPOLYGON (((99.56326 11...\n17 MULTIPOLYGON (((98.35294 9....\n18 MULTIPOLYGON (((99.96416 7....\n19 MULTIPOLYGON (((101.2827 6....\n\n\nThe results identifies Bangkok as an intensifying hotspot, and six others as consecutive hotspots.\nThere are 13 provinces identified as sporadic coldspots which are locations that are cold spots for less than 90% of the time, but never identified as significant hotspots."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.2-ehsa---post-covid-number-of-tourists",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.2-ehsa---post-covid-number-of-tourists",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.2 EHSA - Post-covid number of tourists",
    "text": "F.2 EHSA - Post-covid number of tourists\nWe perform EHSA on the post-covid (2022 onwards) values for the total number of tourists.\n\nF.2.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the post-covid number of tourists. We use filter() to only select the rows for the measure of interest, and then select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(filter(tourism_postCov, tourism_postCov$Indicator == \"no_tourist_all\"),\n                           YYYYMM, Province, Value),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.2.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    Value, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.2.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 14 × 6\n   Province                   tau        sl     S     D  varS\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok                  0.648 0.00150      59  91.0  334.\n 2 Krabi                    0.714 0.000459     65  91.0  334.\n 3 Nakhon Si Thammarat      0.890 0.0000119    81  91.0  334.\n 4 Nonthaburi               0.626 0.00217      57  91.0  334.\n 5 Pattani                  0.648 0.00150      59  91.0  334.\n 6 Phatthalung              0.780 0.000127     71  91.0  334.\n 7 Phra Nakhon Si Ayutthaya 0.604 0.00311      55  91.0  334.\n 8 Phuket                   0.670 0.00102      61  91.0  334.\n 9 Prachuap Khiri Khan      0.648 0.00150      59  91.0  334.\n10 Ratchaburi               0.429 0.0375       39  91.0  334.\n11 Samut Prakan             0.473 0.0215       43  91.0  334.\n12 Surat Thani              0.648 0.00150      59  91.0  334.\n13 Tak                      0.692 0.000688     63  91.0  334.\n14 Trang                    0.451 0.0285       41  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 18 × 6\n   Province            tau        sl     S     D  varS\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Chachoengsao     -0.758 0.000197    -69  91.0  334.\n 2 Kanchanaburi     -0.692 0.000688    -63  91.0  334.\n 3 Khon Kaen        -0.604 0.00311     -55  91.0  334.\n 4 Loei             -0.516 0.0118      -47  91.0  334.\n 5 Mukdahan         -0.648 0.00150     -59  91.0  334.\n 6 Nakhon Nayok     -0.824 0.0000510   -75  91.0  334.\n 7 Nakhon Pathom    -0.495 0.0160      -45  91.0  334.\n 8 Nong Khai        -0.648 0.00150     -59  91.0  334.\n 9 Phetchabun       -0.560 0.00620     -51  91.0  334.\n10 Phetchaburi      -0.780 0.000127    -71  91.0  334.\n11 Samut Sakhon     -0.802 0.0000809   -73  91.0  334.\n12 Samut Songkhram  -0.780 0.000127    -71  91.0  334.\n13 Sing Buri        -0.626 0.00217     -57  91.0  334.\n14 Surin            -0.560 0.00620     -51  91.0  334.\n15 Trat             -0.780 0.000127    -71  91.0  334.\n16 Ubon Ratchathani -0.495 0.0160      -45  91.0  334.\n17 Udon Thani       -0.407 0.0487      -37  91.0  334.\n18 Yasothon         -0.758 0.000197    -69  91.0  334.\n\n\nThe results show that 14 of the 77 provinces are showing significant positive trend. However, there are 18 provinces (more) which are showing a significant negative trend.\n\n\nF.2.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"Value\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Number of Tourists\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n  Province       classification     geometry                      \n1 \"Nonthaburi\"   \"sporadic hotspot\" MULTIPOLYGON (((100.3415 14...\n2 \"Sakon Nakhon\" \"sporadic hotspot\" MULTIPOLYGON (((103.5404 18...\n  Province  classification      geometry                      \n3 \"Lamphun\" \"sporadic coldspot\" MULTIPOLYGON (((99.18821 18...\n6 \"Ranong\"  \"sporadic coldspot\" MULTIPOLYGON (((98.35294 9....\n7 \"Yala\"    \"sporadic coldspot\" MULTIPOLYGON (((101.2927 6....\n  Province       classification        geometry                      \n4 \"Nakhon Sawan\" \"consecutive hotspot\" MULTIPOLYGON (((100.0266 16...\n5 \"Phuket\"       \"consecutive hotspot\" MULTIPOLYGON (((98.31437 7....\n\n\nThe results identifies Phuket and Nakhon Sawan as consecutive hotspots which means they had a single uninterrupted run of being significant hotspots, but have been significant hotspot for less than 90% of the time.\nNonthaburi and Sakon Nakhon are sporadic hotspots, while Lamphun, Ranong and Yala are sporadic coldspots which mean that they have been on-and-off as hot and coldspots for the number of tourists."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.3-ehsa---occupancy-rate",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#f.3-ehsa---occupancy-rate",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "F.3 EHSA - Occupancy Rate",
    "text": "F.3 EHSA - Occupancy Rate\nWe perform EHSA on the post-covid (2022 onwards) values for the tourist occupancy rates.\n\nF.3.1 Creating spacetime cube\nWe then create the space-time cube using spacetime() for just the occupancy rates. We use select() to limit the original dataframe to only the date, province and variable.\n\nstcube &lt;- spacetime(select(occupancy_postCov,\n                           YYYYMM, Province, occupancy),\n                    thai_sf,\n                    .loc_col = \"Province\",\n                    .time_col = \"YYYYMM\")\n\nTo check that the object is in the correct format, we can use is_spacetime_cube() and confirm that it returns TRUE\n\nis_spacetime_cube(stcube)\n\n[1] TRUE\n\n\n\n\nF.3.2 Calculating \\(G_i^*\\) statistics\nBefore computing for the \\(G_i^*\\) statistics, we need to derive the spatial weights. We use the code chunk below to identify the neighbors and to derive inverse distance weights. The alpha argument of st_inverse_distance() determines the level of distance decay.\n\nstcube_nb &lt;- stcube %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt=st_inverse_distance(nb,\n                  geometry,\n                  scale = 1,\n                  alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe then use the following chunk to calculate the local \\(G_i^*\\) for each location. We do this using local_gstar_perm() of sfdep package. We then use unnest() to unnest the gi_star column of the newly created data frame.\n\ngi_stars &lt;- stcube_nb %&gt;%\n  group_by(YYYYMM) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    occupancy, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\nF.3.3 Identifying Hot- and Cold-spots using Mann Kendall test\nThe code chunk below runs the Mann Kendall test (without permutations) on each province using MannKendall(). Provinces where the tau value is positive are showing an increasing trend, while negative tau values show decreasing trend.\nWe display the records with significant test results, and show negative and positive tau values as separate tables.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(Province) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\nfilter(ehsa, sl &lt; 0.05, tau &gt; 0)\n\n# A tibble: 17 × 6\n   Province              tau         sl     S     D  varS\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Bangkok             0.934 0.00000429    85  91.0  334.\n 2 Buriram             0.604 0.00311       55  91.0  334.\n 3 Chumphon            0.670 0.00102       61  91.0  334.\n 4 Krabi               0.714 0.000459      65  91.0  334.\n 5 Nakhon Si Thammarat 0.824 0.0000510     75  91.0  334.\n 6 Narathiwat          0.626 0.00217       57  91.0  334.\n 7 Nonthaburi          0.714 0.000459      65  91.0  334.\n 8 Phatthalung         0.692 0.000688      63  91.0  334.\n 9 Phuket              0.890 0.0000119     81  91.0  334.\n10 Prachuap Khiri Khan 0.934 0.00000429    85  91.0  334.\n11 Ranong              0.560 0.00620       51  91.0  334.\n12 Ratchaburi          0.604 0.00311       55  91.0  334.\n13 Rayong              0.626 0.00217       57  91.0  334.\n14 Samut Prakan        0.736 0.000302      67  91.0  334.\n15 Songkhla            0.648 0.00150       59  91.0  334.\n16 Surat Thani         0.912 0.00000715    83  91.0  334.\n17 Yala                0.495 0.0160        45  91.0  334.\n\nfilter(ehsa, sl &lt; 0.05, tau &lt; 0)\n\n# A tibble: 23 × 6\n   Province         tau       sl     S     D  varS\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amnat Charoen -0.780 0.000127   -71  91.0  334.\n 2 Chachoengsao  -0.582 0.00442    -53  91.0  334.\n 3 Chainat       -0.473 0.0215     -43  91.0  334.\n 4 Chaiyaphum    -0.736 0.000302   -67  91.0  334.\n 5 Chanthaburi   -0.429 0.0375     -39  91.0  334.\n 6 Kanchanaburi  -0.582 0.00442    -53  91.0  334.\n 7 Lopburi       -0.451 0.0285     -41  91.0  334.\n 8 Nakhon Nayok  -0.604 0.00311    -55  91.0  334.\n 9 Nakhon Pathom -0.648 0.00150    -59  91.0  334.\n10 Nakhon Sawan  -0.451 0.0285     -41  91.0  334.\n# ℹ 13 more rows\n\n\nThe results show that 17 of the 77 provinces are showing significant positive trend. However, there are 23 provinces (more) which are showing a significant negative trend.\n\n\nF.3.4 Emerging Hot Spot Analysis\nWe can use emerging_hot_spot_analysis() to perform EHSA, including the previous hypothesis testing, and classify provinces based on the test results.\nThe code chunk below uses emerging_hot_spot_analysis() directly on the spacetime cube to run the test with 100 permutations.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = stcube,\n  .var = \"occupancy\",\n  k = 1,\n  nsim = 99\n)\n\nWe can visualize the number of provinces per category using the code chunk below which utilizes ggplot() to build a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"EHSA Classification - PostCovid Tourism Revenue\",\n       y = \"Number of Provinces\", x=\"\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = 0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nWe can also visualize the hot- and coldspot classifications visually. First, we need to combine the ehsa object with the sf dataframe with the code chunk below.\n\nthai_ehsa &lt;- thai_sf %&gt;%\n  left_join(ehsa,\n            by = join_by(Province == location))\n\nNext, we use tmap package to create a choropleth map based on the EHSA classification. We only include provinces that have significant test results and a pattern detected.\n\nehsa_sig &lt;- filter(thai_ehsa, !(classification == \"no pattern detected\"), p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(thai_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\", title = \"EHSA Classification\") +\n  tm_text(\"Province\", size = 0.5) +\n  tm_borders(alpha = 0.4) +\ntm_layout(\n    main.title = \"Post Covid Tourist Occupancy Rate\",\n    legend.position = c(\"right\", \"bottom\"),\n    bg.color = \"beige\",\n    asp = 0)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\nWe can display the same provinces tabularly using the code chunk below\n\nfor (i in unique(ehsa_sig$classification)){\nprint(as.matrix(ehsa_sig[ehsa_sig$classification == i, c(\"Province\", \"classification\")]))\n}\n\n   Province                   classification      \n1  \"Nonthaburi\"               \"oscilating hotspot\"\n2  \"Phra Nakhon Si Ayutthaya\" \"oscilating hotspot\"\n5  \"Chanthaburi\"              \"oscilating hotspot\"\n7  \"Sa Kaeo\"                  \"oscilating hotspot\"\n8  \"Loei\"                     \"oscilating hotspot\"\n9  \"Maha Sarakham\"            \"oscilating hotspot\"\n11 \"Sakon Nakhon\"             \"oscilating hotspot\"\n13 \"Lamphun\"                  \"oscilating hotspot\"\n16 \"Nakhon Sawan\"             \"oscilating hotspot\"\n18 \"Kamphaeng Phet\"           \"oscilating hotspot\"\n19 \"Tak\"                      \"oscilating hotspot\"\n20 \"Sukhothai\"                \"oscilating hotspot\"\n21 \"Kanchanaburi\"             \"oscilating hotspot\"\n25 \"Nakhon Si Thammarat\"      \"oscilating hotspot\"\n26 \"Phuket\"                   \"oscilating hotspot\"\n   geometry                      \n1  MULTIPOLYGON (((100.3415 14...\n2  MULTIPOLYGON (((100.5131 14...\n5  MULTIPOLYGON (((102.2517 12...\n7  MULTIPOLYGON (((102.1877 14...\n8  MULTIPOLYGON (((102.095 18....\n9  MULTIPOLYGON (((103.1562 16...\n11 MULTIPOLYGON (((103.5404 18...\n13 MULTIPOLYGON (((99.18821 18...\n16 MULTIPOLYGON (((100.0266 16...\n18 MULTIPOLYGON (((99.48875 16...\n19 MULTIPOLYGON (((97.97318 17...\n20 MULTIPOLYGON (((99.60051 17...\n21 MULTIPOLYGON (((98.58631 15...\n25 MULTIPOLYGON (((99.77467 9....\n26 MULTIPOLYGON (((98.31437 7....\n   Province          classification      geometry                      \n3  \"Sing Buri\"       \"sporadic coldspot\" MULTIPOLYGON (((100.3691 15...\n4  \"Chai Nat\"        \"sporadic coldspot\" MULTIPOLYGON (((100.1199 15...\n10 \"Roi Et\"          \"sporadic coldspot\" MULTIPOLYGON (((104.314 16....\n12 \"Chiang Mai\"      \"sporadic coldspot\" MULTIPOLYGON (((99.52512 20...\n15 \"Phrae\"           \"sporadic coldspot\" MULTIPOLYGON (((100.1597 18...\n17 \"Uthai Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.13905 15...\n23 \"Samut Songkhram\" \"sporadic coldspot\" MULTIPOLYGON (((100.0116 13...\n27 \"Surat Thani\"     \"sporadic coldspot\" MULTIPOLYGON (((99.96396 9....\n28 \"Songkhla\"        \"sporadic coldspot\" MULTIPOLYGON (((100.5973 7....\n29 \"Satun\"           \"sporadic coldspot\" MULTIPOLYGON (((100.0903 6....\n31 \"Narathiwat\"      \"sporadic coldspot\" MULTIPOLYGON (((101.6323 6....\n   Province       classification         geometry                      \n6  \"Prachin Buri\" \"consecutive coldspot\" MULTIPOLYGON (((101.4881 14...\n24 \"Phetchaburi\"  \"consecutive coldspot\" MULTIPOLYGON (((99.75869 13...\n   Province        classification        geometry                      \n14 \"Uttaradit\"     \"consecutive hotspot\" MULTIPOLYGON (((101.0924 18...\n22 \"Nakhon Pathom\" \"consecutive hotspot\" MULTIPOLYGON (((100.2231 14...\n30 \"Yala\"          \"consecutive hotspot\" MULTIPOLYGON (((101.2927 6....\n\n\nThe results identify a number of cold and hotspots. If we focus on the coldspots:\n\nwe see that Prachin Buri and Phetchaburi are consecutive coldspots\nA list of 11 provinces, including Chiang Mai, are sporadic coldspots"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.1-loading-tanzania-district-boundaries",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.1-loading-tanzania-district-boundaries",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.1 Loading Tanzania District boundaries",
    "text": "B.1 Loading Tanzania District boundaries\nWe load the district level boundaries in the following code chunk using st_read() and indicating the appropriate layer name. (i.e., the level 2 map) We also use rename() to already change the shapeName field to district to make it more understandable. We also project the map onto EPSG 32737 using st_transform() in order to be able to reference distances in terms of metres.\n\ntz_dist &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"geoBoundaries-TZA-ADM2\") %&gt;%\n  rename(district = shapeName) %&gt;%\n  st_transform(32737)\n\nReading layer `geoBoundaries-TZA-ADM2' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 170 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.58953 ymin: -11.76235 xmax: 40.44473 ymax: -0.983143\nGeodetic CRS:  WGS 84\n\n\n\ntz_dist\n\nSimple feature collection with 170 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -548018.9 ymin: 8698528 xmax: 658181.6 ymax: 9890194\nProjected CRS: WGS 84 / UTM zone 37S\nFirst 10 features:\n       district shapeISO                 shapeID shapeGroup shapeType\n1        Arusha     &lt;NA&gt; 72390352B32479700182608        TZA      ADM2\n2  Arusha Urban     &lt;NA&gt; 72390352B90906351205470        TZA      ADM2\n3        Karatu     &lt;NA&gt; 72390352B22674606658861        TZA      ADM2\n4       Longido     &lt;NA&gt; 72390352B95731720096997        TZA      ADM2\n5          Meru     &lt;NA&gt; 72390352B99598192663387        TZA      ADM2\n6       Monduli     &lt;NA&gt; 72390352B11439822404473        TZA      ADM2\n7    Ngorongoro     &lt;NA&gt; 72390352B42279830137418        TZA      ADM2\n8         Ilala     &lt;NA&gt; 72390352B40584164885098        TZA      ADM2\n9     Kinondoni     &lt;NA&gt; 72390352B66429416458525        TZA      ADM2\n10       Temeke     &lt;NA&gt; 72390352B94835751472469        TZA      ADM2\n                         geometry\n1  MULTIPOLYGON (((262372 9603...\n2  MULTIPOLYGON (((251788.2 96...\n3  MULTIPOLYGON (((148006.1 96...\n4  MULTIPOLYGON (((206258.1 96...\n5  MULTIPOLYGON (((262372 9603...\n6  MULTIPOLYGON (((226729.3 96...\n7  MULTIPOLYGON (((160641.8 96...\n8  MULTIPOLYGON (((530993 9249...\n9  MULTIPOLYGON (((529848.2 92...\n10 MULTIPOLYGON (((531400.6 92...\n\n\nThe output shows that there are 170 objects loaded which corresponds to individual districts. The object is also of multipolygon class which could indicate that there are districts with discontinuous land areas, like islands.\nWe can create a simple map to visualize the boundaries using qtm() from tmap.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nqtm(tz_dist, text = \"district\", text.size = 0.4)\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.2-deriving-district-centroids",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.2-deriving-district-centroids",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.2 Deriving District centroids",
    "text": "B.2 Deriving District centroids\nBefore we load the aspatial data, we will process the district boundary map to be able to use it for future operations with the other dataset. One step that needs to be done is to define representative points, which can be the centroids for the boundary map. The primary purpose of this is to be able to map the aspatial data for a district into a single location. In order to do this, the first step is to convert the multipolygon layer object into a polygon object which will allow for proper centroid calculations for each district.\nWe use the code chunk below to convert the sf object into polygons using st_cast() and then create a column for each individual polygon’s area using mutate() with st_area(). We then use groupby() to reduce back the object to one row per district and then filter() to keep only the largest polygon for each district.\n\ntz_dist_poly &lt;- tz_dist %&gt;%\n  st_cast(\"POLYGON\") %&gt;%\n  mutate(area = st_area(.)) %&gt;%\n  group_by(district) %&gt;%\n  filter(area == max(area)) %&gt;%\n  ungroup() %&gt;%\n  select(-area) %&gt;%\n  select(district)\n\nWarning in st_cast.sf(., \"POLYGON\"): repeating attributes for all\nsub-geometries for which they may not be constant\n\n\nWe can produce a map with tmap package to see if the operation had any irregular effects on the geography. In order to see the difference between the original map and the polygon map, we add the original map as the first layer in red, and then overlay the polygon map. The areas which now appear red are the polygons or areas that have been excluded from the original map to the polygon map.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"red\") +\ntm_shape(tz_dist_poly) +\n  tm_polygons(\"grey\") +\n  tm_layout(title = \"Full vs Poly Map\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nFrom the output, we see that in addition to a few small islands, there are three inland areas that have been excluded from the original map. While this produces holes in the new map, this might not be a big concern right now as long as the centroids we get from the remaining geometries is meaningful.\nIn order to generate the centroids, we can now use st_centroid() to compute them across each district’s largest polygon.\n\ntz_dist_centroids &lt;- st_centroid(tz_dist_poly)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nWe can check the location of the centroids by plotting them as a layer on top of the original district boundary layer using tmap package in the code below.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_centroids) +\n  tm_dots(\"green\", size = 0.2) +\n  tm_layout(title = \"District Centroids\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nThe centroid locations look mostly acceptable, with a few exceptions where they might be lying somewhere away from the district boundaries given some of the districts have odd, nonconvex shapes."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.2-loading-finscope-tanzania-2023-respondent-data",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.2-loading-finscope-tanzania-2023-respondent-data",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.2 Loading Finscope Tanzania 2023 Respondent Data",
    "text": "B.2 Loading Finscope Tanzania 2023 Respondent Data\nWe can use read_csv() in the code chunk below to load the raw respondent data into an R object.\n\nfstz23 &lt;- read_csv(\"data/aspatial/FinScope Tanzania 2023_Individual Main Data_FINAL.csv\", show_col_types = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nThere are 9915 rows or records, and 721 columns or fields. Most of these columns should not be relevant in meeting our objective, so it is advised to limit the data we work with to those meaningful variable. These variables should be our variable(s) of concern, or the dependent variable(s), and the variables that may contribute to it, or the independent variables."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.4-preparing-finscope-tanzania-2023-respondent-data",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.4-preparing-finscope-tanzania-2023-respondent-data",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.4 Preparing Finscope Tanzania 2023 Respondent Data",
    "text": "B.4 Preparing Finscope Tanzania 2023 Respondent Data\n\nB.4.1 Selecting potential variables\nThe first step in preparing the dataset is to reduce the data by keeping only the potentially relevant fields. This means identifying fields that can be used as is or to derive both the response variable(s) and the explanatory variables.\nThis is performed by scanning the datamap file (i.e., data dictionary) that accompanies the dataset. From there, we decide to keep only the following variables. We also change the variable names to a shorter and more recognizable one.\n\nFinancial Inclusion (3)Geographic (4)Demographic (11)Economic (8)Technographic (3)\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nBANKED\nClassified as: Banked; Not Banked\nfi_banked\n\n\nOVERALL_FORMAL\nClassified as using formal instruments: Yes, No\nfi_formal\n\n\nINFORMAL\nClassified as using informal instruments: Yes, No\nfi_informal\n\n\n\n\n\n\n\n\nVariable\nDescription\nNew Variable Name\n\n\n\n\nreg_name\nRegion name\nregion\n\n\ndist_name\nDistrict name\ndistrict\n\n\nward_name\nWard name\nward\n\n\nclustertype\nIndicates if in rural or urban\nurban\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc8c\nAge\nage\n\n\nc9\nGender: male. or female\nfemale\n\n\nc10\nMarital status: married, divorced, widowed, single (4 levels)\nmaritalstatus\n\n\nc11\nHighest level education (10 levels of values)\neducation\n\n\nc2\nHead of household: respondent, not the respondent\nhead_hh\n\n\nc8n_a1\nVisually impaired: Yes, No\nvisual_impaired\n\n\nc8n_b1\nHearing impaired: Yes, No\nhearing_impaired\n\n\nc8n_c1\nCommunication impaired: Yes, No\ncomm_impaired\n\n\nc8n_d1\nMovement impaired: Yes, No\nmove_impaired\n\n\nc8n_e1\nDifficulty with daily activities: Yes, No\ndaily_impaired\n\n\nc8n_f1\nDifficulty remembering and concentrating: Yes, No\ncogn_impaired\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc12_1\nLand ownership (6 levels)\nland_own\n\n\nc14\nFamily involved in agriculture, fishing or aquaculture: Yes, No\nagricultural\n\n\nc18_2\nPrimary source of funds (12 levels)\nsource_of_funds\n\n\nc27__17\nHas some form of ID: Yes, No\nhas_id\n\n\nD2_1__1\nReceives salary from regular job: Yes, No\nreg_job\n\n\nD2_1__2\nReceives money from selling goods produced: Yes, No\nproduction\n\n\nD2_1__11\nDoes not receive income: Yes, No\nno_income\n\n\nIncomeMain\nDerived variable for main source of income (14 levels)\nincome_source\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nNew Variable Name\n\n\n\n\nc23__1\nAccess to mobile phone: No, Yes\nmobile\n\n\nc23__2\nAccess to internet: No, Yes\ninternet\n\n\nc24_1\nPersonally own mobile phone: No, Yes\nown_mobile\n\n\n\n\n\n\nThe code block below compiles the 29 variables and their new names in two separate objects. These will be used in the succeeding steps for data preparation.\n\ncolstokeep &lt;- c(\"reg_name\", \"dist_name\", \"ward_name\", \"clustertype\",\n                \"c8c\", \"c9\", \"c10\", \"c11\", \"c2\",\n                \"c8n_a1\", \"c8n_b1\", \"c8n_c1\", \"c8n_d1\", \"c8n_e1\", \"c8n_f1\",\n                \"c12_1\", \"c14\", \"c18_2\",\n                \"c23__1\", \"c23__2\", \"c24_1\", \"c27__17\",\n                \"D2_1__1\", \"D2_1__2\", \"D2_1__11\",\n                \"BANKED\", \"OVERALL_FORMAL\", \"INFORMAL\", \"IncomeMain\")\nnewnames &lt;- c(\"region\", \"district\", \"ward\", \"urban\",\n              \"age\", \"female\", \"maritalstatus\", \"education\", \"head_hh\",\n              \"visual_impaired\", \"hearing_impaired\", \"comm_impaired\",\n              \"move_impaired\", \"daily_impaired\", \"cogn_impaired\",\n              \"land_own\", \"agricultural\", \"source_of_funds\",\n              \"mobile\", \"internet\", \"own_mobile\", \"has_id\",\n              \"reg_job\", \"production\", \"no_income\",\n              \"fi_banked\", \"fi_formal\", \"fi_informal\", \"income_source\")\nlength(colstokeep)\n\n[1] 29\n\nlength(newnames)\n\n[1] 29\n\n\nWe then use the code chunk below to keep the selected variables using select() and then to rename the variable or column names using colnames()\n\nfstz23_sf &lt;- fstz23 %&gt;%\n  select(all_of(colstokeep))\n\ncolnames(fstz23_sf) &lt;- newnames\n\n\n\nB.4.2 Recoding of variables\nRecoding of variables is a data preparation step where variable values are replaced by another. This may be done for reasons like cleaning the data or standardising the data. This is generally performed in R using the recode() function.\n\nB.4.2.1 Recoding of district names\nWe have district names in the map and in the survey data. As these are two different data sources, there is a chance that they mismatch. We need to ensure that they use the same names as we will use these to add the geographic information to the survey data.\nWe first check which names in each set do not have a corresponding match in the other. We can do this by performing a left join using left_join() and checking which elements do not have matches. We can the use filter() to find the records that did not return a valid value from the other dataset. The code chunk performs this left join approach twice as it needs to be checked for direction for each data source, and then we display the district names that are unmatched for each dataset.\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatched_tzmap &lt;- mismatched_values$district\n\nmismatched_values &lt;- fstz23_sf %&gt;%\n  left_join(tz_dist, by = \"district\") %&gt;%\n  filter(is.na(shapeType)) %&gt;%\n  select(district)\nmismatched_fstz23 &lt;- mismatched_values$district\n\nlist(\n  mismatched_in_map = mismatched_tzmap,\n  number_mm1 = length(c(mismatched_tzmap)),\n  mismatched_in_survey = unique(mismatched_fstz23),\n  number_mm2 = length(c(unique(mismatched_fstz23)))\n)\n\n$mismatched_in_map\n [1] \"Arusha Urban\"                 \"Meru\"                        \n [3] \"Dodoma Urban\"                 \"Iringa Urban\"                \n [5] \"Mafinga Township Authority\"   \"Bukoba Urban\"                \n [7] \"Mpanda Urban\"                 \"Kasulu Township Authority\"   \n [9] \"Kigoma  Urban\"                \"Moshi Urban\"                 \n[11] \"Lindi Urban\"                  \"Babati UrbanBabati Urban\"    \n[13] \"Butiam\"                       \"Musoma Urban\"                \n[15] \"Mbeya Urban\"                  \"Magharibi\"                   \n[17] \"Morogoro Urban\"               \"Masasi  Township Authority\"  \n[19] \"Mtwara Urban\"                 \"Makambako Township Authority\"\n[21] \"Njombe Urban\"                 \"Kibaha Urban\"                \n[23] \"Mafia\"                        \"Sumbawanga Urban\"            \n[25] \"Songea Urban\"                 \"Kahama Township Authority\"   \n[27] \"Shinyanga Urban\"              \"Singida Urban\"               \n[29] \"Tunduma\"                      \"Tabora Urban\"                \n[31] \"Handeni Mji\"                  \"Korogwe\"                     \n[33] \"Korogwe Township Authority\"   \"Tanga Urban\"                 \n\n$number_mm1\n[1] 34\n\n$mismatched_in_survey\n [1] \"Tanganyika\"  \"Kigamboni\"   \"Arumeru\"     \"Butiama\"     \"Dodoma\"     \n [6] \"Tanga\"       \"Malinyi\"     \"Kibiti\"      \"Magharibi B\" \"Magharibi A\"\n[11] \"Ubungo\"      \"Tabora\"     \n\n$number_mm2\n[1] 12\n\n\nThe output reveals that there are 34 district names in the boundary map that are not matched, while there are 12 in the survey data that are not matched. We do not need to ensure all 34 in the first dataset is matched as there might really be areas where there are no respondents or residents. We do, however, want almost all, if not all, of the records in the second dataset to be matched as this is where our modeling data sits. It is also worth noting that all the recoding for districts will be done on fstz23_sf.\nWe will perform checks and data cleaning for the 12 unmatched values in fstz23_sf, and we will also explore some of the remaining unmatched variables in tz_dist. We see that there are values which have “Urban” at the end so there might also be an opportunity to create matches for them.\n\nTanganyika and TangaMagharibiArumeruButiamaDodomaKibitiKigamboniMalinyiTaboraUbungoSelect Urban Areas\n\n\nFor unmatched values, there might be differences in spellings between the two sources. While this doesn’t guarantee catching misspellings, we can at least check if there are other variables that share the first few letters with the unmatched value.\nTo find district values which contains “Tang” we can use str_detect() on both dataset’s columns.\n\nto_find &lt;- \"Tanga\"\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Tanga Urban\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Tanganyika\" \"Tanga\"     \n\n\nThere is only one value in tz_dist that contains “Tang” but two in fstz_23. It should be safe to assume that Tanga and Tanga Urban are one and the same. We can perform the recoding using recode() within mutate() for the district column of fstz_23.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tanga\" = \"Tanga Urban\"))\n\nFor Tanganyika, we can use the following code chunk to check how many survey records are affected using length(), and then show the wards and regions for the records that do have a district name of Tanganyika.\n\n# Count number of records which has district name Tanganyika\ncount(fstz23_sf[str_detect(fstz23_sf$district,\"Tanganyika\"),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    89\n\n# Show regions and wards for records with district name Tanganyika\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,\"Tanganyika\"),], \n       c(region, district, ward)))\n\n# A tibble: 6 × 3\n  region district   ward    \n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;   \n1 Katavi Tanganyika Mnyagala\n2 Katavi Tanganyika Ikola   \n3 Katavi Tanganyika Bulamata\n4 Katavi Tanganyika Mishamo \n5 Katavi Tanganyika Sibwesa \n6 Katavi Tanganyika Isengule\n\n\nThere are six wards that all reflect the same region name of “Katavi”. Upon research, it appears that the Tanganyika district was recently formed and was part of the rural area of the district Mpanda. As such, we will recode Tanganyika to Mpanda using the same approach with\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tanganyika\" = \"Mpanda\"))\n\n\n\nWe saw that there are two districts with the word Magharibi in the fstz23, and one in tz_dist. We can confirm this by using str_detect() to check for all district names containing “Mag” (so we also check some variation in spelling) in both datasets.\n\nto_find &lt;- \"Magha\"\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Magharibi\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Magharibi B\" \"Magharibi A\"\n\n\nFor this case, we drop the B and A by using recode() on the district column of ftsz23_sf.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Magharibi B\" = \"Magharibi\",\n                            \"Magharibi A\" = \"Magharibi\"))\n\n\n\nWe next look into the unmatched district “Arumeru” in fstz23. We will use the code chunk below for this district and most of the succeeding ones to check district names that match in both data sets, show the regions and wards in fstz23 for the district, and the number of records which reflect that district. For these, we continue using str_detect() which is included in tidyverse under the stringr package in order to find matches of a substring.\n\nto_find &lt;- \"Arumeru\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Arumeru\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   105\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 7 × 4\n  region district ward         urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;\n1 Arusha Arumeru  Poli         Urban\n2 Arusha Arumeru  Maroroni     Rural\n3 Arusha Arumeru  Olmotonyi    Rural\n4 Arusha Arumeru  Oloirien     Urban\n5 Arusha Arumeru  Maji ya Chai Rural\n6 Arusha Arumeru  Kisongo      Rural\n7 Arusha Arumeru  Nkoaranga    Rural\n\n\nThe output shows that there are no districts in tz_dist that have a name of Arumeru, but there are 105 in fstz23. Upon research, we see that the wards of Arumeru was split between Arusha and Meru. Among the wards in the dataset, the following are now part of Meru: Maroroni, Poli, Maji ya Chai, Nkoaranga. The balance 3 are Arusha: Olmotonyi, Oloirien, Kisongo.\nWe first update the records for the last three wards to reflect a district name of Arusha using the following code chunk.\n\nfstz23_sf[(fstz23_sf$ward == \"Olmotonyi\" | fstz23_sf$ward == \"Oloirien\" | fstz23_sf$ward == \"Kisongo\"),]$district = \"Arusha\"\n\nWe can then use recode() to change the remaining records that are still reflecting “Arumeru” and change them to “Meru”\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Arumeru\" = \"Meru\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Butiam”\n\nto_find &lt;- \"Butiam\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Butiam\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Butiama\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region district ward    urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;\n1 Mara   Butiama  Butiama Urban\n2 Mara   Butiama  Bukabwa Rural\n3 Mara   Butiama  Mirwa   Rural\n\n\nWe see that there is only one district from each dataset and it appears that they can only refer to the same district. We then use recode() to update “Butiama” to “Butiam”\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Butiama\" = \"Butiam\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Dodom”\n\nto_find &lt;- \"Dodom\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Dodoma Urban\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Dodoma\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   142\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 10 × 4\n   region district ward            urban\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;\n 1 Dodoma Dodoma   Msalato         Urban\n 2 Dodoma Dodoma   Mnadani         Urban\n 3 Dodoma Dodoma   Ntyuka          Urban\n 4 Dodoma Dodoma   Mbabala         Urban\n 5 Dodoma Dodoma   Nkuhungu        Urban\n 6 Dodoma Dodoma   Nzuguni         Urban\n 7 Dodoma Dodoma   Hombolo Bwawani Urban\n 8 Dodoma Dodoma   Kikuyu Kusini   Urban\n 9 Dodoma Dodoma   Mkonze          Urban\n10 Dodoma Dodoma   Uhuru           Urban\n\n\nWe again see that there is a one-to-one matching for the sole district on both dataset. We will then update “Dodoma” to “Dodoma Urban” using recode() in the chunk below\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Dodoma\" = \"Dodoma Urban\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Kibit”\n\nto_find &lt;- \"Kibit\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Kibiti\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    29\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 2 × 4\n  region district ward   urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n1 Pwani  Kibiti   Mbuchi Rural\n2 Pwani  Kibiti   Bungu  Rural\n\n\nUpon research, it appears that Kibiti is a relatively new district. The two wards that appear under it were actually part of Rufiji district, so we can change “Kibiti” to “Rufiji” using encode()\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Kibiti\" = \"Rufiji\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Kigamb”\n\nto_find &lt;- \"Kigamb\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Kigamboni\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region        district  ward      urban\n  &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;\n1 Dar es Salaam Kigamboni Kibada    Urban\n2 Dar es Salaam Kigamboni Kigamboni Urban\n3 Dar es Salaam Kigamboni Somangila Urban\n\n\nThere is no indication of the district merging or splitting recently from another, but if we check our map, the region should occupy part of the space which appears as “Temeke”. We then recode Kigamboni as Temeke using the following code chunk.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Kigamboni\" = \"Temeke\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Malin”\n\nto_find &lt;- \"Malin\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Malinyi\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    15\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 1 × 4\n  region   district ward     urban\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;\n1 Morogoro Malinyi  Usangule Rural\n\n\nIf we check the map, the location of Malinyi falls in the region of Ulanga in our map. We again use recode() to change the district names to Ulanga.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Malinyi\" = \"Ulanga\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Tabor”\n\nto_find &lt;- \"Tabor\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\n[1] \"Tabora Urban\"\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Tabora\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    45\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 3 × 4\n  region district ward     urban\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;\n1 Tabora Tabora   Mbugani  Urban\n2 Tabora Tabora   Kiloleni Urban\n3 Tabora Tabora   Mwinyi   Urban\n\n\nGiven that there is only “Tabora Urban” in tz_dist, we should be able to update the district names in fstz23 using this.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Tabora\" = \"Tabora Urban\"))\n\n\n\nWe execute the same code chunk that makes use of recode() to find records where the district name contains “Ubung”\n\nto_find &lt;- \"Ubung\"\nprint(\"Matches in tz_dist\")\n\n[1] \"Matches in tz_dist\"\n\nunique(tz_dist_poly[str_detect(tz_dist_poly$district,to_find),]$district)\n\ncharacter(0)\n\nprint(\"Matches in ftsz23\")\n\n[1] \"Matches in ftsz23\"\n\nunique(fstz23_sf[str_detect(fstz23_sf$district,to_find),]$district)\n\n[1] \"Ubungo\"\n\ncount(fstz23_sf[str_detect(fstz23_sf$district,to_find),])\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    88\n\nunique(select(fstz23_sf[str_detect(fstz23_sf$district,to_find),], \n       c(region, district, ward, urban)))\n\n# A tibble: 6 × 4\n  region        district ward    urban\n  &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;\n1 Dar es Salaam Ubungo   Mabibo  Urban\n2 Dar es Salaam Ubungo   Mbezi   Urban\n3 Dar es Salaam Ubungo   Kimara  Urban\n4 Dar es Salaam Ubungo   Msigani Urban\n5 Dar es Salaam Ubungo   Goba    Urban\n6 Dar es Salaam Ubungo   Manzese Urban\n\n\nUpon checking, the location of Ubungo appears to be within the boundaries of the district Kinondoni in our map. We recode it as such with the following code chunk.\n\nfstz23_sf &lt;- mutate(fstz23_sf, district = recode(district,\n                            \"Ubungo\" = \"Kinondoni\"))\n\n\n\nWe have removed all mismatches from fstz23 but have 28 unmatched district names in tz_dist. Among these are a few districts that are suffixed by “Urban”\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatch_urban &lt;- mismatched_values[str_detect(mismatched_values$district,\"Urban\"),]$district\nmismatch_urban\n\n [1] \"Arusha Urban\"             \"Iringa Urban\"            \n [3] \"Bukoba Urban\"             \"Mpanda Urban\"            \n [5] \"Kigoma  Urban\"            \"Moshi Urban\"             \n [7] \"Lindi Urban\"              \"Babati UrbanBabati Urban\"\n [9] \"Musoma Urban\"             \"Mbeya Urban\"             \n[11] \"Morogoro Urban\"           \"Mtwara Urban\"            \n[13] \"Njombe Urban\"             \"Kibaha Urban\"            \n[15] \"Sumbawanga Urban\"         \"Songea Urban\"            \n[17] \"Shinyanga Urban\"          \"Singida Urban\"           \n\n\nFor these, we will assume they refer to the urban region of a given district. For example, Arusha Urban will cover the urban area of the Arusha district. Based on this assumption, we can go through ftsz23 to check for records for that district and are in the urban area and then map them to the values above. We use a for loop in the code chunk below to find records in fstz23 that match these conditions and then apply the suffixed district names as replacements. We use the word() function from stringr package in order to pick the first word and treat it as the “plain” district name.\n\nfor (i in mismatch_urban)\n  {\n    dist = word(i, 1)\n    fstz23_sf[str_detect(fstz23_sf$district,dist) & fstz23_sf$urban == \"Urban\",]$district = i\n  }\n\n\n\n\nWe can check for the remaining mismatches by rerunning the earlier code.\n\nmismatched_values &lt;- tz_dist %&gt;%\n  left_join(fstz23_sf, by = \"district\") %&gt;%\n  filter(is.na(region)) %&gt;%\n  select(district)\nmismatched_tzmap &lt;- mismatched_values$district\n\nmismatched_values &lt;- fstz23_sf %&gt;%\n  left_join(tz_dist, by = \"district\") %&gt;%\n  filter(is.na(shapeType)) %&gt;%\n  select(district)\nmismatched_fstz23 &lt;- mismatched_values$district\n\nlist(\n  mismatched_in_map = mismatched_tzmap,\n  number_mm1 = length(c(mismatched_tzmap)),\n  mismatched_in_survey = unique(mismatched_fstz23),\n  number_mm2 = length(c(unique(mismatched_fstz23)))\n)\n\n$mismatched_in_map\n [1] \"Iringa Urban\"                 \"Mafinga Township Authority\"  \n [3] \"Kasulu Township Authority\"    \"Masasi  Township Authority\"  \n [5] \"Makambako Township Authority\" \"Mafia\"                       \n [7] \"Kahama Township Authority\"    \"Tunduma\"                     \n [9] \"Handeni Mji\"                  \"Korogwe\"                     \n[11] \"Korogwe Township Authority\"  \n\n$number_mm1\n[1] 11\n\n$mismatched_in_survey\ncharacter(0)\n\n$number_mm2\n[1] 0\n\n\nWe were able to remove all mismatches from fstz23 and then reduce the mismatches from tz_dist from 34 to 11. We can visualize the distribution of the (recoded) records in our map by first adding the number of records into the sf object. We use count() in the code chunk below to compute for the number of records per district. We then use left_join() to merge it with the district map, and replace any zero values (from mismatches) with zero.\n\ntz_dist_stat &lt;- tz_dist %&gt;%\n  left_join(count(fstz23_sf, district), by= \"district\") %&gt;%\n  rename(\"orig_records\" = \"n\") %&gt;%\n  replace(is.na(.),0)\n\nWe can then use tmap package to plot the distribution of respondents, We first add a layer in grey for the full map and then add a choropleth map for districts with nonzero number of respondents. This will show districts with no respondents as grey.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[tz_dist_stat$orig_records &gt; 0,]) +\n  tm_polygons(\"orig_records\", title = \"Original Records\")\n\n\n\n\n\n\n\n\nThe output shows that there are only a few districts that do not have any respondents. Most of the districts have between 0 to 50 respondents. The highest number of respondents in a single district is between 150 and 200.\n\n\nB.4.2.2 Recoding of Modelling Variables\nIf we look at the data, the modelling variables that we have in fstz23 are mostly categorical. Most of these are binary, but some have more than two values. While we can use categorical variables for EDA, these do not work well once we start modelling. For our case, we will go ahead and convert most of these variables upfront.\nWe can use the following code chunk which uses the unique() function to display the distinct values. We use lapply() to run the function on each variable in the dataframe but we exclude the region, district, ward and age as these will either not be part of the modelling, or it is already in numeric form.\n\nlapply(select(fstz23_sf,-c(region, district, ward, age)), unique)\n\n$urban\n[1] \"Rural\" \"Urban\"\n\n$female\n[1] \"Female\" \"Male\"  \n\n$maritalstatus\n[1] \"Married/living together\" \"Widowed\"                \n[3] \"Divorced/separated\"      \"Single/never married\"   \n\n$education\n [1] \"Some primary\"                             \n [2] \"No formal education\"                      \n [3] \"Primary completed\"                        \n [4] \"Some secondary\"                           \n [5] \"Some University or other higher education\"\n [6] \"University or higher education completed\" \n [7] \"Secondary competed-O level\"               \n [8] \"Post primary technical training\"          \n [9] \"Secondary completed-A level\"              \n[10] \"Don’t know\"                               \n\n$head_hh\n[1] \"Respondent is hhh\"  \"Respondent not hhh\"\n\n$visual_impaired\n[1] \"Yes\" \"No\" \n\n$hearing_impaired\n[1] \"No\"  \"Yes\"\n\n$comm_impaired\n[1] \"No\"  \"Yes\"\n\n$move_impaired\n[1] \"No\"  \"Yes\"\n\n$daily_impaired\n[1] \"No\"  \"Yes\"\n\n$cogn_impaired\n[1] \"No\"  \"Yes\"\n\n$land_own\n[1] \"You personally own the land/plot where you live\" \n[2] \"The land/plot is rented\"                         \n[3] \"You own the land/plot together with someone else\"\n[4] \"You don’t own or rent the land\"                  \n[5] \"Other A household members owns the land/plot\"    \n[6] \"Don’t know (Don’t read out)\"                     \n\n$agricultural\n[1] \"Yes\" \"No\" \n\n$source_of_funds\n [1] \"Your household sells some of its crops and uses the money\"                                              \n [2] NA                                                                                                       \n [3] \"Your household has to borrow money\"                                                                     \n [4] \"Your household has money to buy it, it uses money from wages / other regular job /  sources of income\"  \n [5] \"Help from friends/relatives/neighbors/community/Government\"                                             \n [6] \"Use savings the household has\"                                                                          \n [7] \"Your household sells non-agricultural things to get money\"                                              \n [8] \"Your household gets it from a buyer to whom it has to sell its crop, livestock or fish when it is ready\"\n [9] \"Your household does piece work/casual jobs to get money to buy it\"                                      \n[10] \"Your household sells some of its livestock and uses the money\"                                          \n[11] \"Your household gets it in exchange for work it does\"                                                    \n[12] \"Your household doesn’t have to buy because it manage with what it has\"                                  \n[13] \"Your household sells products like milk, eggs that it get from its livestock to get money to buy it\"    \n\n$mobile\n[1] \"Yes\" \"No\" \n\n$internet\n[1] \"Yes\" \"No\"  NA   \n\n$own_mobile\n[1] \"Yes\" \"2\"  \n\n$has_id\n[1] \"No\"  \"Yes\"\n\n$reg_job\n[1] \"No\"  \"Yes\"\n\n$production\n[1] \"Yes\" \"No\" \n\n$no_income\n[1] \"No\"  \"Yes\"\n\n$fi_banked\n[1] \"Not Banked\" \"Banked\"    \n\n$fi_formal\n[1] \"OVERALL_FORMAL\"     \"Not OVERALL_FORMAL\"\n\n$fi_informal\n[1] \"INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS\"\n[2] \"Not INFORMAL\"                                  \n\n$income_source\n [1] \"Farmers and fishers\"                                         \n [2] \"Piece work/casual labor\"                                     \n [3] \"Traders - non-agricultural\"                                  \n [4] \"Dependents\"                                                  \n [5] \"Service providers\"                                           \n [6] \"Traders - agricultural products\"                             \n [7] \"Welfare\"                                                     \n [8] \"Formal sector salaried\"                                      \n [9] \"Pension\"                                                     \n[10] \"Informal sector salaried\"                                    \n[11] \"Other\"                                                       \n[12] \"Rental income\"                                               \n[13] \"Gambling\"                                                    \n[14] \"Interest from savings, investments, stocks, unit trusts etc.\"\n\n\nThe output reveals that 5 of the variables have more than 2 values or levels, while the balance 20 are binary. We also see that for the internet variable there are some records that show NA. We use the code below to check if each of the records have NA for internet by using is.na(), and then counting the number of records by just adding up the TRUE values using sum()\n\nsum(is.na(fstz23_sf$internet))\n\n[1] 7\n\n\nAs there are only seven with NA values, and there is no sure way of replacing them with the right value, removing them from the dataset should not produce any big issues. We use the code chunk below to remove the na’s from the internet variable. There are a number of different ways to remove na’s, here we just use a mask based on the complement of the results of the is.na() function which returns TRUE for any invalid values. We include the count of rows before and after running the code to check that there is a difference of seven rows.\n\nnrow(fstz23_sf)\n\n[1] 9915\n\nfstz23_sf &lt;- fstz23_sf[!is.na(fstz23_sf$internet),]\nnrow(fstz23_sf)\n\n[1] 9908\n\n\nFor binary variables, we will use the code below to replace the ‘positive’ value with 1 and the negative with 0. The values for each variable may vary so we need to apply the recoding individually for these variables.\n\nfstz23_sf &lt;- mutate(fstz23_sf,\n                    urban = recode(urban, \"Rural\" = 0, \"Urban\" = 1),\n                    female = recode(female, \"Male\" = 0, \"Female\" = 1),\n                    head_hh = recode(head_hh, \"Respondent not hhh\" = 0, \"Respondent is hhh\" = 1),\n                    visual_impaired = recode(visual_impaired, \"No\" = 0, \"Yes\" = 1),\n                    hearing_impaired = recode(hearing_impaired, \"No\" = 0, \"Yes\" = 1),\n                    comm_impaired = recode(comm_impaired, \"No\" = 0, \"Yes\" = 1),\n                    move_impaired = recode(move_impaired, \"No\" = 0, \"Yes\" = 1),\n                    daily_impaired = recode(daily_impaired, \"No\" = 0, \"Yes\" = 1),\n                    cogn_impaired = recode(cogn_impaired, \"No\" = 0, \"Yes\" = 1),\n                    agricultural = recode(agricultural, \"No\" = 0, \"Yes\" = 1),\n                    mobile = recode(mobile, \"No\" = 0, \"Yes\" = 1),\n                    internet = recode(internet, \"No\" = 0, \"Yes\" = 1),\n                    own_mobile = recode(own_mobile, \"2\" = 0, \"Yes\" = 1),\n                    has_id = recode(has_id, \"No\" = 0, \"Yes\" = 1),\n                    reg_job = recode(reg_job, \"No\" = 0, \"Yes\" = 1),\n                    production = recode(production, \"No\" = 0, \"Yes\" = 1),\n                    no_income = recode(no_income, \"No\" = 0, \"Yes\" = 1),\n                    fi_banked = recode(fi_banked, \"Not Banked\" = 0, \"Banked\" = 1),\n                    fi_formal = recode(fi_formal, \"Not OVERALL_FORMAL\" = 0, \"OVERALL_FORMAL\" = 1),\n                    fi_informal = recode(fi_informal, \"Not INFORMAL\" = 0,\n                                         \"INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS\" = 1))\n\nFor the non-binary variables, we start by checking the distribution of the data across their different levels. Where there is a very small amount of data in one category, we will not be too worried merging them with another (logical) one. We use the code chunk below which uses count() to give the number of records or rows for each of the values of the indicated column. We then wrap the output in arrange() to sort the values in ascending number of records.\n\nMarital StatusEducationLand OwnershipSource of FundsPrimary Source of Income\n\n\n\narrange(count(fstz23_sf, maritalstatus),n)\n\n# A tibble: 4 × 2\n  maritalstatus               n\n  &lt;chr&gt;                   &lt;int&gt;\n1 Widowed                   949\n2 Divorced/separated        956\n3 Single/never married     1934\n4 Married/living together  6069\n\n\n\n\n\narrange(count(fstz23_sf, education),n)\n\n# A tibble: 10 × 2\n   education                                     n\n   &lt;chr&gt;                                     &lt;int&gt;\n 1 Don’t know                                    4\n 2 Secondary completed-A level                  40\n 3 Post primary technical training              55\n 4 Some University or other higher education   125\n 5 University or higher education completed    292\n 6 Some secondary                              830\n 7 Secondary competed-O level                 1273\n 8 Some primary                               1354\n 9 No formal education                        1592\n10 Primary completed                          4343\n\n\n\n\n\narrange(count(fstz23_sf, land_own),n)\n\n# A tibble: 6 × 2\n  land_own                                             n\n  &lt;chr&gt;                                            &lt;int&gt;\n1 Don’t know (Don’t read out)                         14\n2 The land/plot is rented                           1022\n3 You own the land/plot together with someone else  1544\n4 You don’t own or rent the land                    1820\n5 Other A household members owns the land/plot      1974\n6 You personally own the land/plot where you live   3534\n\n\n\n\n\narrange(count(fstz23_sf, source_of_funds),n)\n\n# A tibble: 13 × 2\n   source_of_funds                                                             n\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 Your household gets it in exchange for work it does                        30\n 2 Your household sells products like milk, eggs that it get from its liv…    32\n 3 Your household doesn’t have to buy because it manage with what it has      43\n 4 Help from friends/relatives/neighbors/community/Government                 67\n 5 Your household has to borrow money                                        101\n 6 Your household gets it from a buyer to whom it has to sell its crop, l…   146\n 7 Your household sells non-agricultural things to get money                 184\n 8 Your household sells some of its livestock and uses the money             304\n 9 Your household has money to buy it, it uses money from wages / other r…   448\n10 Use savings the household has                                             930\n11 Your household does piece work/casual jobs to get money to buy it        1164\n12 Your household sells some of its crops and uses the money                2325\n13 &lt;NA&gt;                                                                     4134\n\n\n\n\n\narrange(count(fstz23_sf, income_source),n)\n\n# A tibble: 14 × 2\n   income_source                                                    n\n   &lt;chr&gt;                                                        &lt;int&gt;\n 1 Interest from savings, investments, stocks, unit trusts etc.     2\n 2 Gambling                                                         6\n 3 Rental income                                                   46\n 4 Other                                                           67\n 5 Pension                                                         67\n 6 Welfare                                                         87\n 7 Informal sector salaried                                       209\n 8 Traders - agricultural products                                218\n 9 Service providers                                              386\n10 Formal sector salaried                                         426\n11 Traders - non-agricultural                                     643\n12 Dependents                                                    1960\n13 Piece work/casual labor                                       2559\n14 Farmers and fishers                                           3232\n\n\n\n\n\nThe output shows that there is a very large number of NAs in the source of funds. We highlight this using the is.na() function in the first code chunk below. As there are more than 40% missing values, and the income_source variable may already be holding the similar, but more complete, information, we will go ahead and drop the variable by using the select() function in the second code chunk.\n\nsum(is.na(fstz23_sf$source_of_funds))\n\n[1] 4134\n\n\n\nfstz23_sf &lt;- select(fstz23_sf, -c(source_of_funds))\n\nWhile we will be preforming a separate EDA for the variables, we will already create binary variables for certain levels of the remaining categorical variables. Common practice is to produce n-1 dummy variable for a variable with n-levels, however, we will opt to consolidate some of the variables together where it makes sense. For martial status, we can keep three levels. The first variable denotes whether the respondent is currently married, the second variable will be whether the respondent is widowed, separated or divorced. Single respondents should reflect a value of zero for both of the variables. For the code chunks, we use as.integer() to convert the logical outputs into zeros and ones.\n\nfstz23_sf$is_married &lt;- as.integer(fstz23_sf$maritalstatus == \"Married/living together\")\nfstz23_sf$was_married &lt;- as.integer(fstz23_sf$maritalstatus == \"Widowed\" | fstz23_sf$maritalstatus == \"Divorced/separated\")\n\nFor education, we will keep four levels for primary, secondary and tertiary or higher education. (with the fourth level denoting that the respondent has not completed primary)\n\nfstz23_sf$educ_primary &lt;- as.integer(fstz23_sf$education == \"Primary completed\" | fstz23_sf$education == \"Some secondary\" | fstz23_sf$education == \"Post primary technical training\")\nfstz23_sf$educ_secondary &lt;- as.integer(fstz23_sf$education == \"Secondary competed-O level\" | fstz23_sf$education == \"Secondary completed-A level\" | fstz23_sf$education == \"Some University or other higher education\")\nfstz23_sf$educ_tertiary &lt;- as.integer(fstz23_sf$education == \"University or higher education completed\")\n\nFor land ownership, we define start with four levels to denote whether the respondent personally owns the land, the land is owned by family or shared with someone, the land is rented, or the land is neither owned nor rented.\n\nfstz23_sf$land_self_own &lt;- as.integer(fstz23_sf$land_own == \"You personally own the land/plot where you live\")\nfstz23_sf$land_hh_or_shared &lt;- as.integer(fstz23_sf$land_own == \"You own the land/plot together with someone else\" | fstz23_sf$land_own == \"Other A household members owns the land/plot\")\nfstz23_sf$land_rented &lt;- as.integer(fstz23_sf$land_own == \"The land/plot is rented\")\n\nFor sources of income, we see that the largest group is “Farmers and Fishers” (3232), “Piece-work or Casual Labor” (2559) and “Dependents” (1960). We will keep these three as distinct levels. We can then define traders (861), salaried (635) and all other sources excuding welfare, gambling, pension, and service providers. We will take service providers to be very similar to casual labor as it also counts as an irregular source of funds.\n\nfstz23_sf$income_farm_and_fish &lt;- as.integer(fstz23_sf$income_source == \"Farmers and fishers\")\nfstz23_sf$income_piecework &lt;- as.integer(fstz23_sf$income_source == \"Piece work/casual labor\" | fstz23_sf$income_source == \"Service providers\")\nfstz23_sf$income_dependent &lt;- as.integer(fstz23_sf$income_source == \"Dependents\")\nfstz23_sf$income_trader &lt;- as.integer(fstz23_sf$income_source == \"Traders - non-agricultural\" | fstz23_sf$income_source == \"Traders - agricultural products\")\nfstz23_sf$income_salaried &lt;- as.integer(fstz23_sf$income_source == \"Formal sector salaried\" | fstz23_sf$income_source == \"Informal sector salaried\")\nfstz23_sf$income_other &lt;- as.integer(fstz23_sf$income_source == \"Other\" | fstz23_sf$income_source == \"Rental income\" | fstz23_sf$income_source == \"Interest from savings, investments, stocks, unit trusts etc.\")\n\nThese recoded variables will now be used for model calibrations instead of the original variables.\n\n\n\nB.4.3 Creation of overall measures\nThere are currently three different variables for financial inclusion looking at three different dimensions of financial inclusion. We can create an overall financial inclusion variable to indicate if the respondent is included in any of the three different dimensions of FI.\nWe use the following code to create a new variable which returns 1 if any of the three variables is 1, otherwise it returns zero. As the three original variables are in zero and one, we can use the logical or operator (|) to implement this operation. We then use as.integer() to convert the result from logical to a zero-one integer.\n\nfstz23_sf$fi_overall &lt;- as.integer(fstz23_sf$fi_banked | fstz23_sf$fi_formal | fstz23_sf$fi_informal)\n\nWe can also do the same for the different categories of physical impairment and create a single variable that combines it all.\n\nfstz23_sf$any_impaired &lt;- as.integer(fstz23_sf$visual_impaired | fstz23_sf$hearing_impaired | fstz23_sf$comm_impaired |\n                                       fstz23_sf$move_impaired | fstz23_sf$daily_impaired)\n\n\n\nB.4.4 Converting fstz23_sf into an sf dataframe\nThe object fstz23_sf still does not include any geospatial information and cannot be used later for geographically weighted modelling. To solve this, we can use the district centroids as the point location of the respondents. We first use left_join() to import the point geometry of the centroids, and then we use st_as_sf() in order to make sure that the new object is recognized as an sf dataframe.\n\nfstz23_sf &lt;- left_join(fstz23_sf, tz_dist_centroids, by = \"district\") %&gt;%\n  st_as_sf()\n\nWe can check if the geometries are properly mapped by plotting the respondents onto the boundary map using tmap package.\n\ntm_shape(tz_dist) +\n  tm_polygons(\"grey\") +\ntm_shape(fstz23_sf) +\n  tm_dots(\"blue\", size = 0.2) +\n  tm_layout(title = \"Respondents\",\n            title.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nThe respondents appear to be properly mapped to the district centroids, however, duplicate locations will be unacceptable for the methods we will use for geographically weighted modelling later. To solver this, we can slightly shift the points by introducing st_jitter(). For the amount argument, we use a value of 1000 which means that points will be shifted by up to 1km from their original point. This 1km should not be an issue and will not cause points to go beyond the district boundary.\n\nfstz23_sf &lt;- st_jitter(fstz23_sf, 1000)\n\nWe can doublecheck that the operation is successful and there are no duplicated locations by using duplicated() to check if a value is duplicated and then using any() to check if the function returned true for any value.\n\nany(duplicated(fstz23_sf$geometry))\n\n[1] FALSE\n\n\nThe code hase returned FALSE so we are assured that there are no duplicated point locations."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.4.2-recoding-of-variables",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#b.4.2-recoding-of-variables",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "B.4.2 Recoding of variables",
    "text": "B.4.2 Recoding of variables\nXXX"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#c.1-respondent-age",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#c.1-respondent-age",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.1 Respondent Age",
    "text": "C.1 Respondent Age\nThe age variable in the survey data is the only numeric variable retained. We can use the following code to produce a histogram to show the distribution of values of this variable. We use ggplot package to produce the chart, but we include the central measures– the mean, median, mode, as captions for additional insights.\n\nggplot(fstz23_sf, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = \"#4A90E2\", color = \"black\") +\n  labs(title = \"Age Distribution of Respondents\",\n       x = \"Respondent Age\",\n       y = \"Number of Respondents\",\n       caption = paste(\"Mean =\", round(mean(fstz23_sf$age, na.rm = TRUE), 1), \n                       \", Median =\", round(median(fstz23_sf$age, na.rm = TRUE), 1), \n                       \", Mode =\", as.numeric(names(sort(table(fstz23_sf$age), decreasing = TRUE)[1])))) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nWe can also display the summary statistics using the summary() function.\n\nsummary(fstz23_sf$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.00   27.00   37.00   39.68   50.00  100.00 \n\n\nThe outputs show that the ages range from 16 to 100 and is right skewed. The distribution has a mean of ~40yrs and a mode of 30 yrs. Given the shape of the distribution, we can consider scaled versions of the variable when we calibrate the model later."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#c.2-financial-inclusion-measures",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#c.2-financial-inclusion-measures",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.2 Financial Inclusion Measures",
    "text": "C.2 Financial Inclusion Measures\nWe currently have four variables that give an indication of whether the respondent is financially included. For this study, we want to limit to one, or at most two variables. We expect that some of the variables are highly correlated, while some will perform much better in a model than others.\nWe first check the overall distribution or proportion of respondents across these four measures. We use ggplot package to create a bar chart to show the proportion of respondents achieving financial inclusion based on each dimension. The first part of the code computes for the proportion numbers as plotting the data directly will result in counts rather than percentages.\n\n# Calculate the proportions for each variable\nproportions &lt;-  st_drop_geometry(fstz23_sf) %&gt;%\n  summarise(\n    fi_banked = mean(fi_banked, na.rm = TRUE) * 100,\n    fi_formal = mean(fi_formal, na.rm = TRUE) * 100,\n    fi_informal = mean(fi_informal, na.rm = TRUE) * 100,\n    fi_overall = mean(fi_overall, na.rm = TRUE) * 100\n  )\n\n# Convert the proportions to a long format for ggplot2\nproportions_long &lt;- proportions %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\")\n\n# Create the bar chart\nggplot(proportions_long, aes(x = variable, y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  geom_text(aes(label = round(value, 1)), vjust = -0.5, size = 4) +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(title = \"Proportion of Respondents for Financial Inclusion Variables\",\n       x = \"\",\n       y = \"Percentage\",\n       fill = \"Variable\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.y = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nThe output shows that only 20.5% of the respondents are banked, the lowest across the three main dimensions. It also shows that the difference between formal and overall financial inclusion is a difference of 7%. This means that only 7% of respondents are banked or using informal FI instruments, but are not using formal instruments.\nWe can visualize the correlation based on overlapping values across the three dimensions. Visually, overlaps can be visualized using Venn diagrams, but we can also use an upset chart from UpSetR package. This visualization is more scalable than venn diagrams, which is not really an issue since we only have three categories. This chart makes it much easier to compare intersections and non-intersections against each other.\nIn the code chunk below, we load the UpSetR package, and then prepare the data so that we only have the three variables. The preparation ensures that the variables are in 0-1 integers which is the required format for the function. We then use upset() from UpSetR package to produce the chart by passing the data and defining the variables to be plotted.\n\nlibrary(UpSetR)\n\n# Create a binary dataframe\noverlap_data &lt;- as.data.frame(st_drop_geometry(fstz23_sf)) %&gt;%\n  mutate(\n    Banked = as.integer(fi_banked == 1),\n    Formal = as.integer(fi_formal == 1),\n    Informal = as.integer(fi_informal == 1)\n  ) %&gt;%\n  select(Banked, Formal, Informal)\n\n# Create the UpSet plot\nupset(overlap_data, sets = c(\"Banked\", \"Formal\", \"Informal\"),\n      keep.order = TRUE, order.by = \"freq\", number.angles = 45,\n      main.bar.color = \"blue\", sets.bar.color = \"red\",\n      text.scale = c(1.5, 1.5, 1.5, 1, 1.5, 1.5),\n      mainbar.y.label = \"Intersection Size\", sets.x.label = \"Set Size\")\n\n\n\n\n\n\n\n\nThe resulting chart shows that:\n\nAll banked respondents are also financial included based on formal instruments. (bank is a subset or category under formal instruments)\nThere are 663 respondents (~7%) that are financially included based on informal instruments, but not based on formal instruments\nThere are 2812 respondents (~28%) that are financially included based on formal instruments, but not based on informal instruments\nThere are 3836 respondents (~38%) that are financially included based on both formal and informal instruments\n\nWe do not have enough to already exclude any of these variables, but we expect that the overall measure covers too much of the sample to be predictable. We also expect that the informal FI measure has too much overlap with formal so we would likely choose one but not the other."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#c.3-correlation-of-predictors",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#c.3-correlation-of-predictors",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "C.3 Correlation of predictors",
    "text": "C.3 Correlation of predictors\nWe can check if any of the predictors are highly correlated as the precence of autocorrelation affects the performance and interpretation of the model.\nWe can do this by producing the correlation plot of the potential predictor variables. We use ggcorrmat() of ggstatsplot package to produce this. We pass a dataframe with just the predictor variables and the code will output a diagonal matrix with the correlation coeficients between each pair of variables.\n\nggcorrmat(select(st_drop_geometry(fstz23_sf),\n                 -c(region, district, ward, maritalstatus, education, land_own, income_source,\n                    fi_banked, fi_formal, fi_informal, fi_overall)))\n\n\n\n\n\n\n\n\nThe code outputs a large matrix, but we only need to focus on pairs where the correlation coefficient is high. (i.e., &gt; 0.8) Those are:\n\nage and age_standardized = 1 - This is expected as one is just a transformation. We will keep them both for now as we want to see which one will result to a better model.\nany_impaired and visual_impaired = 0.81 - This is also expected as any_impaired is a derived variable. It looks like most of the impairment reported is visual in nature. We will then drop the derived variable\nincome_salaried and reg_job = 0.95 - This appears to be redundant variables and likely refer to the same condition. We should be able to drop the latter\nproduction and income_farm_and_fish = 0.71 - while not as high as the last pair, this pair most likely also refers to the same type of work. We will follow the same approach so we only keep the variables prefixed by income in the calibration\n\nBased on those, we can clean up our dataset by dropping the three variables mentioned above. We can also drop the variables we don’t need which include the categorical variables that we already have created new variables for. We perform this with the use of the select() function and using a “-” to exclude rather than select columns.\n\nfstz23_sf &lt;- select(fstz23_sf,-c(any_impaired, reg_job, production,\n                                 maritalstatus, education, land_own, income_source))"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#d.1-global-models-without-variable-selection",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#d.1-global-models-without-variable-selection",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.1 Global Models without Variable Selection",
    "text": "D.1 Global Models without Variable Selection\nTo calibrate the model, we use glm() which calibrates generalised linear models. As the dependent variable is binary, we need to make sure that the model used is a logistic regression model. This is done by passing the value “binomial” to the family argument.\nWe will run this with all variables for all four FI measures to see if any of them are performing very well or very poorly against the others. We will then focus on finetuning and then preparing the geographically weighted models on those variables that can be best explained with this approach.\n\nD.1.1 Global Model for Formal FI (no variable selection)\nThe code below calibrates a model with all variables with fi_formal as the dependent variable. We then use summary() to output the results. While the results display the AIC as a measure, we also compute for the reduction in variance due to the predictors. This is done by comparing the deviance and the null deviance. A higher number means that the variables had a larger contribution in predicting or explaining the value of the dependent variable.\n\nfi_formal.lr &lt;- glm(fi_formal ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_formal.lr)\n\n\nCall:\nglm(formula = fi_formal ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.909394   0.313346  -2.902 0.003705 ** \nurban                 0.632501   0.080982   7.810 5.70e-15 ***\nage                   0.005744   0.002538   2.263 0.023640 *  \nfemale                0.046569   0.074759   0.623 0.533335    \nhead_hh               0.178742   0.087275   2.048 0.040557 *  \nvisual_impaired      -0.024436   0.095515  -0.256 0.798080    \nhearing_impaired     -0.291409   0.158199  -1.842 0.065469 .  \ncomm_impaired         0.275147   0.337988   0.814 0.415603    \nmove_impaired         0.072813   0.122261   0.596 0.551477    \ndaily_impaired        0.111153   0.201332   0.552 0.580887    \ncogn_impaired         0.144262   0.159558   0.904 0.365924    \nagricultural          0.030786   0.084129   0.366 0.714411    \nmobile                0.549352   0.082138   6.688 2.26e-11 ***\ninternet              0.579452   0.081901   7.075 1.49e-12 ***\nown_mobile            1.902444   0.070026  27.168  &lt; 2e-16 ***\nhas_id               -0.675854   0.089887  -7.519 5.52e-14 ***\nno_income            -0.583239   0.122630  -4.756 1.97e-06 ***\nis_married           -0.034435   0.098156  -0.351 0.725726    \nwas_married          -0.194584   0.134307  -1.449 0.147392    \neduc_primary          0.663236   0.064346  10.307  &lt; 2e-16 ***\neduc_secondary        1.492458   0.128363  11.627  &lt; 2e-16 ***\neduc_tertiary         2.714657   0.723016   3.755 0.000174 ***\nland_self_own         0.170538   0.095351   1.789 0.073690 .  \nland_hh_or_shared     0.081153   0.085445   0.950 0.342232    \nland_rented           0.375301   0.137679   2.726 0.006412 ** \nincome_farm_and_fish -0.692323   0.268336  -2.580 0.009878 ** \nincome_piecework     -0.791856   0.269060  -2.943 0.003250 ** \nincome_dependent     -0.874045   0.275270  -3.175 0.001497 ** \nincome_trader        -0.026406   0.300939  -0.088 0.930079    \nincome_salaried      -0.157785   0.344056  -0.459 0.646518    \nincome_other         -0.660914   0.403607  -1.638 0.101522    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11074.6  on 9907  degrees of freedom\nResidual deviance:  7563.1  on 9877  degrees of freedom\nAIC: 7625.1\n\nNumber of Fisher Scoring iterations: 7\n\n1 - summary(fi_formal.lr)$deviance / summary(fi_formal.lr)$null.deviance \n\n[1] 0.3170735\n\n\n\n\nD.1.2 Global Model for Banked FI (no variable selection)\nThe code below calibrates a model with all variables with fi_banked as the dependent variable.\n\nfi_banked.lr &lt;- glm(fi_banked ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_banked.lr)\n\n\nCall:\nglm(formula = fi_banked ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -2.037948   0.289783  -7.033 2.03e-12 ***\nurban                 0.487211   0.070234   6.937 4.01e-12 ***\nage                   0.008988   0.002742   3.278 0.001046 ** \nfemale               -0.244823   0.073536  -3.329 0.000871 ***\nhead_hh               0.429123   0.084337   5.088 3.62e-07 ***\nvisual_impaired       0.220218   0.092799   2.373 0.017641 *  \nhearing_impaired     -0.545221   0.215985  -2.524 0.011592 *  \ncomm_impaired         0.168385   0.482018   0.349 0.726839    \nmove_impaired        -0.322852   0.143402  -2.251 0.024361 *  \ndaily_impaired        0.103399   0.261231   0.396 0.692242    \ncogn_impaired        -0.163345   0.189727  -0.861 0.389267    \nagricultural         -0.177993   0.076526  -2.326 0.020024 *  \nmobile               -0.054909   0.136147  -0.403 0.686723    \ninternet              0.628532   0.063778   9.855  &lt; 2e-16 ***\nown_mobile            0.856049   0.122809   6.971 3.16e-12 ***\nhas_id               -1.180586   0.146446  -8.062 7.53e-16 ***\nno_income            -0.091093   0.158109  -0.576 0.564522    \nis_married           -0.082966   0.090487  -0.917 0.359204    \nwas_married          -0.397742   0.126719  -3.139 0.001697 ** \neduc_primary          0.796899   0.086567   9.206  &lt; 2e-16 ***\neduc_secondary        1.700194   0.107407  15.829  &lt; 2e-16 ***\neduc_tertiary         3.121148   0.208886  14.942  &lt; 2e-16 ***\nland_self_own         0.272392   0.094898   2.870 0.004100 ** \nland_hh_or_shared     0.162165   0.093024   1.743 0.081288 .  \nland_rented           0.190437   0.108746   1.751 0.079909 .  \nincome_farm_and_fish -1.599462   0.207346  -7.714 1.22e-14 ***\nincome_piecework     -1.950615   0.209724  -9.301  &lt; 2e-16 ***\nincome_dependent     -1.893223   0.225727  -8.387  &lt; 2e-16 ***\nincome_trader        -1.546829   0.219310  -7.053 1.75e-12 ***\nincome_salaried      -0.644514   0.226550  -2.845 0.004442 ** \nincome_other         -1.345573   0.301468  -4.463 8.07e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10056.8  on 9907  degrees of freedom\nResidual deviance:  7550.3  on 9877  degrees of freedom\nAIC: 7612.3\n\nNumber of Fisher Scoring iterations: 6\n\n1 - summary(fi_banked.lr)$deviance / summary(fi_banked.lr)$null.deviance \n\n[1] 0.2492412\n\n\n\n\nD.1.3 Global Model for Informal FI (no variable selection)\nThe code below calibrates a model with all variables with fi_informal as the dependent variable.\n\nfi_informal.lr &lt;- glm(fi_informal ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_informal.lr)\n\n\nCall:\nglm(formula = fi_informal ~ urban + age + female + head_hh + \n    visual_impaired + hearing_impaired + comm_impaired + move_impaired + \n    daily_impaired + cogn_impaired + agricultural + mobile + \n    internet + own_mobile + has_id + no_income + is_married + \n    was_married + educ_primary + educ_secondary + educ_tertiary + \n    land_self_own + land_hh_or_shared + land_rented + income_farm_and_fish + \n    income_piecework + income_dependent + income_trader + income_salaried + \n    income_other, family = \"binomial\", data = fstz23_sf)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.85492    0.22360  -3.823 0.000132 ***\nurban                 0.06146    0.05511   1.115 0.264748    \nage                  -0.01826    0.00197  -9.270  &lt; 2e-16 ***\nfemale                0.09939    0.05470   1.817 0.069206 .  \nhead_hh               0.50994    0.06308   8.084 6.29e-16 ***\nvisual_impaired       0.17875    0.07094   2.520 0.011748 *  \nhearing_impaired      0.03574    0.13339   0.268 0.788769    \ncomm_impaired        -0.33005    0.30810  -1.071 0.284058    \nmove_impaired         0.05833    0.09647   0.605 0.545371    \ndaily_impaired       -0.19622    0.17527  -1.120 0.262915    \ncogn_impaired         0.09270    0.12797   0.724 0.468830    \nagricultural          0.10476    0.05926   1.768 0.077070 .  \nmobile                0.43162    0.07583   5.691 1.26e-08 ***\ninternet              0.40287    0.05307   7.591 3.19e-14 ***\nown_mobile            0.31775    0.06393   4.970 6.68e-07 ***\nhas_id               -0.46332    0.07398  -6.263 3.77e-10 ***\nno_income            -0.52889    0.10286  -5.142 2.72e-07 ***\nis_married            0.52809    0.06802   7.764 8.24e-15 ***\nwas_married           0.21085    0.09467   2.227 0.025933 *  \neduc_primary          0.20200    0.05155   3.919 8.90e-05 ***\neduc_secondary        0.39907    0.07977   5.003 5.64e-07 ***\neduc_tertiary         0.63482    0.16032   3.960 7.50e-05 ***\nland_self_own        -0.18105    0.06966  -2.599 0.009349 ** \nland_hh_or_shared     0.13379    0.06473   2.067 0.038751 *  \nland_rented           0.02514    0.08747   0.287 0.773773    \nincome_farm_and_fish  0.20099    0.17616   1.141 0.253888    \nincome_piecework      0.13475    0.17689   0.762 0.446201    \nincome_dependent     -0.20958    0.18466  -1.135 0.256397    \nincome_trader         0.45674    0.18825   2.426 0.015254 *  \nincome_salaried       0.42832    0.19652   2.180 0.029291 *  \nincome_other          0.32673    0.26233   1.246 0.212942    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 13683  on 9907  degrees of freedom\nResidual deviance: 12417  on 9877  degrees of freedom\nAIC: 12479\n\nNumber of Fisher Scoring iterations: 4\n\n1 - summary(fi_informal.lr)$deviance / summary(fi_informal.lr)$null.deviance \n\n[1] 0.09256485\n\n\n\n\nD.1.4 Global Model for Overall FI (no variable selection)\nThe code below calibrates a model with all variables with fi_overall as the dependent variable.\n\nfi_overall.lr &lt;- glm(fi_overall ~ urban + age + female + head_hh + visual_impaired + hearing_impaired + comm_impaired +\n                      move_impaired + daily_impaired + cogn_impaired + agricultural + mobile + internet +\n                      own_mobile + has_id + no_income + is_married + was_married+\n                      educ_primary + educ_secondary + educ_tertiary +\n                      land_self_own + land_hh_or_shared + land_rented +\n                      income_farm_and_fish + income_piecework + income_dependent + income_trader + income_salaried +\n                      income_other,\n                    family = \"binomial\",\n                    data = fstz23_sf)\nsummary(fi_overall.lr)\n\n\nCall:\nglm(formula = fi_overall ~ urban + age + female + head_hh + visual_impaired + \n    hearing_impaired + comm_impaired + move_impaired + daily_impaired + \n    cogn_impaired + agricultural + mobile + internet + own_mobile + \n    has_id + no_income + is_married + was_married + educ_primary + \n    educ_secondary + educ_tertiary + land_self_own + land_hh_or_shared + \n    land_rented + income_farm_and_fish + income_piecework + income_dependent + \n    income_trader + income_salaried + income_other, family = \"binomial\", \n    data = fstz23_sf)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.055692   0.333888  -0.167 0.867527    \nurban                 0.516681   0.086899   5.946 2.75e-09 ***\nage                  -0.001405   0.002650  -0.530 0.595996    \nfemale                0.024066   0.078557   0.306 0.759332    \nhead_hh               0.235402   0.093558   2.516 0.011866 *  \nvisual_impaired       0.150577   0.103060   1.461 0.144000    \nhearing_impaired     -0.114633   0.163713  -0.700 0.483798    \ncomm_impaired         0.033323   0.328379   0.101 0.919171    \nmove_impaired         0.007001   0.126653   0.055 0.955920    \ndaily_impaired       -0.002848   0.200755  -0.014 0.988681    \ncogn_impaired         0.204732   0.165965   1.234 0.217357    \nagricultural          0.043097   0.087983   0.490 0.624249    \nmobile                0.517871   0.081228   6.376 1.82e-10 ***\ninternet              0.554273   0.091495   6.058 1.38e-09 ***\nown_mobile            1.524108   0.074932  20.340  &lt; 2e-16 ***\nhas_id               -0.592184   0.090867  -6.517 7.17e-11 ***\nno_income            -0.620211   0.119034  -5.210 1.88e-07 ***\nis_married            0.254598   0.101106   2.518 0.011798 *  \nwas_married           0.096682   0.140398   0.689 0.491059    \neduc_primary          0.563723   0.068504   8.229  &lt; 2e-16 ***\neduc_secondary        1.393584   0.141706   9.834  &lt; 2e-16 ***\neduc_tertiary         2.338111   0.722148   3.238 0.001205 ** \nland_self_own         0.108214   0.101402   1.067 0.285892    \nland_hh_or_shared     0.210526   0.088977   2.366 0.017979 *  \nland_rented           0.319084   0.146909   2.172 0.029857 *  \nincome_farm_and_fish -0.616538   0.290245  -2.124 0.033654 *  \nincome_piecework     -0.684558   0.290886  -2.353 0.018605 *  \nincome_dependent     -1.042797   0.295604  -3.528 0.000419 ***\nincome_trader        -0.036161   0.329672  -0.110 0.912657    \nincome_salaried      -0.187959   0.374037  -0.503 0.615305    \nincome_other         -0.262658   0.473406  -0.555 0.579014    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 9339.8  on 9907  degrees of freedom\nResidual deviance: 6817.2  on 9877  degrees of freedom\nAIC: 6879.2\n\nNumber of Fisher Scoring iterations: 7\n\n1 - summary(fi_overall.lr)$deviance / summary(fi_overall.lr)$null.deviance\n\n[1] 0.2700872\n\n\n\n\nD.1.5 Choosing a dependent variable based on global model calibration\nThe different models showed that the one using fi_formal and fi_overall as the dependent variable performed better than the other two. Since fi_formal gave the best reduction in deviance, and we have seen earlier that the other fi measures overlap with fi_formal anyway, we will focus on fi_formal as the main indicator for which we will finetune the model."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#d.2-variable-selection-for-the-global-model",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#d.2-variable-selection-for-the-global-model",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.2 Variable selection for the global model",
    "text": "D.2 Variable selection for the global model\nFrom the model results, we see that not all the variables contribute in the same degree as the others. We can use the output to pick the significant variables or perform a technique like forward, backward or stepwise regression to select variables by introducing or removing them one at a time.\nForward regression can be done using ols_step_forward_p() of the olsrr package. The function takes in the full model and starts from an empty model and adds variables with the highest significance one at a time. This continues doing this as long as variables with significance less than the specified p-value can be added.\n\nfi_formal_fw_mlr &lt;- ols_step_forward_p(fi_formal.lr, p_val = 0.05, details = FALSE)\n\nWe can also display the results by calling the resulting object.\n\nfi_formal_fw_mlr\n\n\n                                      Stepwise Summary                                      \n------------------------------------------------------------------------------------------\nStep    Variable             AIC          SBC             SBIC            R2       Adj. R2 \n------------------------------------------------------------------------------------------\n 0      Base Model        11452.392    11466.795     -5643875908.953    0.00000    0.00000 \n 1      own_mobile         8001.162     8022.765    -11332019050.848    0.29427    0.29420 \n 2      has_id             7826.777     7855.582    -11742615711.514    0.30673    0.30659 \n 3      urban              7661.916     7697.921    -12144725901.394    0.31830    0.31810 \n 4      mobile             7569.414     7612.621    -12378471273.642    0.32478    0.32450 \n 5      internet           7492.544     7542.952    -12576969271.789    0.33013    0.32979 \n 6      no_income          7432.878     7490.487    -12734351633.418    0.33429    0.33388 \n 7      educ_secondary     7382.226     7447.036    -12870263096.019    0.33781    0.33735 \n 8      educ_primary       7285.271     7357.282    -13129780806.723    0.34439    0.34386 \n 9      educ_tertiary      7255.572     7334.784    -13213906836.776    0.34649    0.34589 \n 10     age                7243.236     7329.649    -13252042393.875    0.34743    0.34677 \n 11     income_trader      7234.121     7327.736    -13281646286.889    0.34817    0.34744 \n 12     head_hh            7230.229     7331.044    -13297292381.943    0.34855    0.34776 \n------------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.590       RMSE                  0.348 \nR-Squared               0.349       MSE                   0.121 \nAdj. R-Squared          0.348       Coef. Var            46.241 \nPred R-Squared          0.347       AIC                7230.229 \nMAE                     0.249       SBC                7331.044 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                Sum of                                                 \n               Squares          DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression     642.088          12         53.507    441.188    0.0000 \nResidual      1200.065        9895          0.121                      \nTotal         1842.153        9907                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n         model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-------------------------------------------------------------------------------------------\n   (Intercept)     0.236         0.017                 14.240    0.000     0.204     0.269 \n    own_mobile     0.388         0.011        0.388    36.683    0.000     0.367     0.408 \n        has_id    -0.114         0.011       -0.094    -9.947    0.000    -0.136    -0.091 \n         urban     0.061         0.008        0.067     7.537    0.000     0.045     0.077 \n        mobile     0.108         0.012        0.088     8.912    0.000     0.084     0.132 \n      internet     0.053         0.009        0.055     6.158    0.000     0.036     0.069 \n     no_income    -0.098         0.013       -0.066    -7.601    0.000    -0.123    -0.072 \neduc_secondary     0.162         0.013        0.133    12.745    0.000     0.137     0.187 \n  educ_primary     0.099         0.009        0.115    11.673    0.000     0.083     0.116 \n educ_tertiary     0.134         0.023        0.053     5.924    0.000     0.090     0.179 \n           age     0.001         0.000        0.028     2.850    0.004     0.000     0.001 \n income_trader     0.042         0.013        0.028     3.321    0.001     0.017     0.067 \n       head_hh     0.019         0.008        0.022     2.426    0.015     0.004     0.035 \n-------------------------------------------------------------------------------------------\n\n\nThe results show that the global model for fi_formal includes 12 explanatory variables which consist of variables for:\n\nMobile phone usage and ownership and internet access (3 variables) - has the highest combined weight\nEducation (3 variables)\nOther positive coefficient: urban, age, trader, head_hh\nNegative coefficients: no source of income, no id\n\nThe last variable is surprising as it implies that having an id is linked to a lower probability of being financially included. The model did not pick up gender which means that it doesn’t see a clear distinction between males and females for this dimension of financial inclusion."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#d.3-testing-for-spatial-autocorrelation",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#d.3-testing-for-spatial-autocorrelation",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "D.3 Testing for spatial autocorrelation",
    "text": "D.3 Testing for spatial autocorrelation\nBefore calibrating the geographically weighted model, we can check if there is a pattern linking global model performance and the respondent’s location. One way to do this is to plot the residuals or error and check for any patterns.\nThe first step is to export relevant output from the model as a dataframe. We just extract the residuals by referencing the model object in the results.\n\nmlr_output &lt;- as.data.frame(fi_formal_fw_mlr$model$residuals) %&gt;%\n  rename('FW_MLR_RES' = 'fi_formal_fw_mlr$model$residuals')\n\nWe then import these into fstz23_sf as a new variable MLR_RES using cbind(). This function adds the dataframe as new columns in their current order.\n\nfstz23_sf &lt;- cbind(fstz23_sf,\n                         mlr_output$FW_MLR_RES) %&gt;%\n  rename('MLR_RES' = 'mlr_output.FW_MLR_RES')\n\nWe remember that our respondent data only have the district s their location so dissplaying them as points might not be very meaningful or accurate. We can instead plot the residuals at a district level to see if there are any patterns arising from there.\nTo do this, we first compute for the average residual at a district level by using group_by() to summarise the object by district and then define the aggregate function using summarise(). We use st_drop_geometry() so that the geometry column is dropped and we only have the district name and the average residual value.\n\navg_res_df &lt;- st_drop_geometry(fstz23_sf) %&gt;%\n  group_by(district) %&gt;%\n  summarise(avg_res = mean(MLR_RES, na.rm = TRUE))\n\nhead(avg_res_df)\n\n# A tibble: 6 × 2\n  district                 avg_res\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Arusha                    0.105 \n2 Arusha Urban              0.0168\n3 Babati                   -0.0522\n4 Babati UrbanBabati Urban -0.0185\n5 Bagamoyo                  0.0395\n6 Bahi                     -0.179 \n\n\nWe then export these average residual values into the TZ boundary map by using left_join() on the district name.\n\ntz_dist_stat &lt;- tz_dist_stat %&gt;%\n  left_join(avg_res_df, by= \"district\")\n\nWe can now use tmap package to visually display the average residual value per district. We again use two layers, and then exclude any districts where there is no residuals by using a mask with !is.na()\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$avg_res),]) +\n  tm_polygons(\"avg_res\", title = \"Average Residuals\")\n\nVariable(s) \"avg_res\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe map shows some apparent clusters with positive (average) residuals and some with negative residuals. A large number of districts have relatively low average residuals between -0.1 and 0.1.\nWe can confirm this observation by running global Moran’s I test on the average residual value. In order to do this, we first need to compute for the neighbors and the weights for each of the district. We use st_knn() to derive neighbors using knn method with a parameter of 6 neighbors, and then use equal weights for neighbors using the st_weights() function. We perform these functions on the centroids object as these methods work on points rather than shapes.\n\ntz_dist_centroids &lt;- tz_dist_centroids %&gt;%\n  mutate(nb = st_knn(geometry, k = 6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nWe then run the Moran’s I test with permutations using global_moran_perm() from sfdep package. Note that we use the avg_res from the map object but have kept the neighbor list and weights in the centrodis object. The nsim argument indicates that we are running 10 simulations for this test. We also replace any na values with zero as the code will not work with any na values.\n\nset.seed(1234)\nglobal_moran_perm(replace_na(tz_dist_stat$avg_res,0),\n                  tz_dist_centroids$nb,\n                  tz_dist_centroids$wt,\n                  alternative = \"two.sided\",\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.19584, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nAs the results are significant, the test confirms that there is spatial autocorrelation for the average residual values across districts. The positive test statistic confirms our observation that the pattern is that of clustering. We should then build geographically weighted models as they are likely to produce better results as we account for the respondents’ locations."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.1-computing-a-bandwidth",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.1-computing-a-bandwidth",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.1 Computing a bandwidth",
    "text": "E.1 Computing a bandwidth\nThe first step in calibrating a geographically weighted model is determining the bandwidth to use. The choice can either be a fixed bandwidth which is based on distance, or an adaptive bandwidth which is based on the number of neighbors. For our case, we will opt for a fixed bandwidth since we have not precisely mapped the locations of the respondents, and there is a wide range of values for the number of respondents per district. A fixed bandwidth is likely to ensure that it captures most, if not all, of the points in the same district and also some in neighboring districts.\nTo compute for the optimum fixed bandwidth, we use bw.ggwr() of GWModel package. The approach argument defines the stopping rule to be used, which is cross validation in this case. Setting the adaptive argument to FALSE indicates that we are computing for the fixed bandwidth. Like the global model, we indicate binomial for the family argument to specify we are calibrating a logistic regression model.\nThe first part of the function is the formula for the model. To be sure of the details to put in here, one may use the formula() function on the model object of the forward regression output to display the final formula.\n\nbw_fixed &lt;- bw.ggwr(formula = fi_formal ~ own_mobile + has_id + urban + mobile + internet +\n                     no_income + educ_secondary + educ_primary + educ_tertiary +\n                     age + income_trader + head_hh,\n                  data = fstz23_sf,\n                  family = \"binomial\",\n                  approach = \"CV\",\n                  kernel = \"gaussian\",\n                  adaptive = FALSE,\n                  longlat = FALSE)\n\n# To display the global model's formula, you may use\n# formula(fi_formal_fw_mlr$model)\n\nThe output shows a recommended bandwidth of ~99.5km should be used. We save the output as an rds object to save our results and prevent the need to rerun the code again.\n\nwrite_rds(bw_fixed, \"data/rds/bw_fixed.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.2-deriving-the-distance-matrix",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.2-deriving-the-distance-matrix",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.2 Deriving the distance matrix",
    "text": "E.2 Deriving the distance matrix\nCalibration of the logistics regression model requires a properly set up distance matrix. We use the code below which computes for this using gw.dist() on the coordinates of the data points. The coordinates function does not work on an sf dataframe so we convert it into Spatial format first.\n\ndistMAT &lt;- gw.dist(dp.locat=\n                     coordinates(as_Spatial(fstz23_sf)))"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.3-calibrating-the-fixed-bandwidth-model",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.3-calibrating-the-fixed-bandwidth-model",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.3 Calibrating the fixed bandwidth model",
    "text": "E.3 Calibrating the fixed bandwidth model\nWe can now calibrate the geographically weighted model using the computed bandwidthby using ggwr.basic() from GWModel. The function uses mostly the same arguments as the previous one, with the exception that the bandwidth now becomes an input here.\n\ngwr_fixed &lt;- ggwr.basic(formula = fi_formal ~ own_mobile + has_id + urban + mobile + internet +\n                     no_income + educ_secondary + educ_primary + educ_tertiary +\n                     age + income_trader + head_hh,\n                     data = fstz23_sf,\n                     family = \"binomial\",\n                     kernel = \"gaussian\",\n                     bw = bw_fixed,\n                     adaptive = FALSE,\n                     longlat = FALSE,\n                     dMat = distMAT)\n\nWe again save this object into an rds file to save our work for future runs.\n\nwrite_rds(gwr_fixed, \"data/rds/gwr_fixed.rds\")\n\nWe can show the results by calling the object as in the code chunk below.\n\ngwr_fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-09 01:56:00.625357 \n   Call:\n   ggwr.basic(formula = fi_formal ~ own_mobile + has_id + urban + \n    mobile + internet + no_income + educ_secondary + educ_primary + \n    educ_tertiary + age + income_trader + head_hh, data = fstz23_sf, \n    bw = bw_fixed, family = \"binomial\", kernel = \"gaussian\", \n    adaptive = FALSE, longlat = FALSE, dMat = distMAT)\n\n   Dependent (y) variable:  fi_formal\n   Independent variables:  own_mobile has_id urban mobile internet no_income educ_secondary educ_primary educ_tertiary age income_trader head_hh\n   Number of data points: 9908\n   Used family: binomial\n   ***********************************************************************\n   *              Results of Generalized linear Regression               *\n   ***********************************************************************\n\nCall:\nNULL\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \nIntercept      -1.524234   0.125637 -12.132  &lt; 2e-16 ***\nown_mobile      1.909171   0.069174  27.600  &lt; 2e-16 ***\nhas_id         -0.713992   0.083010  -8.601  &lt; 2e-16 ***\nurban           0.634941   0.074585   8.513  &lt; 2e-16 ***\nmobile          0.544885   0.081262   6.705 2.01e-11 ***\ninternet        0.599629   0.081309   7.375 1.65e-13 ***\nno_income      -0.713969   0.097385  -7.331 2.28e-13 ***\neduc_secondary  1.504240   0.125972  11.941  &lt; 2e-16 ***\neduc_primary    0.668065   0.063948  10.447  &lt; 2e-16 ***\neduc_tertiary   2.949785   0.719341   4.101 4.12e-05 ***\nage             0.005471   0.002060   2.655  0.00792 ** \nincome_trader   0.708805   0.148055   4.787 1.69e-06 ***\nhead_hh         0.192332   0.067661   2.843  0.00447 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11074.6  on 9907  degrees of freedom\nResidual deviance:  7601.7  on 9895  degrees of freedom\nAIC: 7627.7\n\nNumber of Fisher Scoring iterations: 7\n\n\n AICc:  7627.753\n Pseudo R-square value:  0.31359\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 99550.75 \n   Regression points: the same locations as observations are used.\n   Distance metric: A distance matrix is specified for this model calibration.\n\n   ************Summary of Generalized GWR coefficient estimates:**********\n                        Min.    1st Qu.     Median    3rd Qu.   Max.\n   Intercept      -2.2553256 -1.7682889 -1.5998528 -1.3662353 0.2864\n   own_mobile      1.0280291  1.8126102  1.9268403  2.0854432 2.7884\n   has_id         -1.5130176 -0.9107411 -0.7352893 -0.6028037 0.4318\n   urban          -0.3774224  0.3679404  0.6370600  0.8034331 1.5909\n   mobile         -0.6754378  0.4583577  0.7072317  0.8739171 1.1907\n   internet       -0.4507688  0.3207454  0.6938241  0.8874637 1.2307\n   no_income      -1.6221173 -0.9028388 -0.7060301 -0.5112796 0.3790\n   educ_secondary  0.5492826  1.2938089  1.4288504  1.7448408 2.3738\n   educ_primary    0.1792122  0.4986158  0.6758896  0.8737680 1.2324\n   educ_tertiary   0.7005712  2.4681136  4.0382606  5.3483262 6.5277\n   age            -0.0341697  0.0025534  0.0057795  0.0108019 0.0220\n   income_trader   0.0135346  0.3952052  0.6642425  1.0400905 3.1913\n   head_hh        -0.3123380  0.0940548  0.1752028  0.2485868 0.4916\n   ************************Diagnostic information*************************\n   Number of data points: 9908 \n   GW Deviance: 7106.258 \n   AIC : 7548.682 \n   AICc : 7558.832 \n   Pseudo R-square value:  0.3583282 \n\n   ***********************************************************************\n   Program stops at: 2024-11-09 02:01:47.510664 \n\n\nThe output shows an improvement with the gleographically weighted model across the different performance measures. AICc improved from 7627.7 to 7558.832. The pseudo R-squared value improved from 0.31359 to 0.3583282.\nThe output also shows that the coefficient values vary widely across the local models. For some variables, there is also a change in sign across those value ranges– which implies that for some variables like mobile and internet access, they are detrimental to the respondent’s level of financial inclusion depending on their location.\nWe can analyze the gwr model further by accessing the details in the output."
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.4-importing-gwr-model-results-into-an-sf-dataframe",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.4-importing-gwr-model-results-into-an-sf-dataframe",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.4 Importing gwr model results into an sf dataframe",
    "text": "E.4 Importing gwr model results into an sf dataframe\nThe gwr output includes details for every local model in an object called SDF. We can combine this with the geospatial information in order to be able to visualize the values of coefficients, residuals and fit measures at an individual location level. We use the following code chunk to extract SDF as a dataframe and then combine it with with fstz23_sf using cbind(). The code only retains the district names and the gwr model output by using select() to specify those columns.\n\ngwr_fixed_output &lt;- as.data.frame(gwr_fixed$SDF) %&gt;%\n  select(-c(geometry))\n\ngwr_sf_fixed &lt;- cbind(fstz23_sf, gwr_fixed_output) %&gt;%\n  select(2, (ncol(fstz23_sf)):(ncol(fstz23_sf)+ncol(gwr_fixed_output))) %&gt;%\n  st_drop_geometry()\n\nAs the local models might not be relevant on their own, we can summarize all of the model values by district and then visualize these variables by district rather than by individual data point. We use the following code chunk to compute for the average of each of the variables by district name using the following code chunk. The code uses group_by() to aggregate by district. Columns with the mean for each variable are then added by using across() and everything() to compute the mean for each variable.\n\ngwr_sf_fixed_by_dist &lt;- gwr_sf_fixed %&gt;%\n  group_by(district) %&gt;%\n  summarise(across(everything(), ~ mean(.x, na.rm = TRUE)))\n\nWe then import to the district boundary map using left_join() on district.\n\ntz_dist_stat &lt;- tz_dist_stat %&gt;%\n  left_join(gwr_sf_fixed_by_dist, by= \"district\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.5-visualizing-model-coefficients-and-metrics",
    "href": "Take-home/Take-home_Ex03/Take-home_Ex03.html#e.5-visualizing-model-coefficients-and-metrics",
    "title": "Geographically Weighted Modeling of Financial Inclusion in Tanzania",
    "section": "E.5 Visualizing model coefficients and metrics",
    "text": "E.5 Visualizing model coefficients and metrics\nWe can visualize the patterns in the model coefficients or of other metrics with the updated sf dataframe.\n\nE.5.1 Visualizing model coefficients\nFor model coefficients, we focus on the following that have a wide range of values: has_id, mobile, internet, no_income, education (all levels), head_hh.\n\nWith IDMobile AccessInternet AccessLack of income sourcesEducationHeads of household\n\n\nThe code chunk below produces a choropleth map for the average coefficient of has_id in each district.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$has_id.1),]) +\n  tm_polygons(\"has_id.1\", title = \"ß has_id\")\n\nVariable(s) \"has_id.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe chart shows that the coefficient of has_id is negative across most of Tanzania. There are only a few districts where it is positive. This result seems surprising as ID’s may be treated as a way of getting access to financial services. There is likely more to this and the absence of an ID for some of the respondents might be link to another condition which is not appearing here. (e.g., those on welfare that do not have access to banking might all have IDs)\n\n\nThe code chunk below produces a choropleth map for the average coefficient of mobile in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$mobile.1),]) +\n  tm_polygons(\"mobile.1\", title = \"ß mobile\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$mobile.1) & (tz_dist_stat$mobile.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"mobile.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe output shows increased incidence of financial inclusion for respondents with access to a mobile phone. However, there is a cluster of districts on the northeast and another on the west that shows a negative correlation between mobile phone access and financial inclusion. This needs to be investigated as access is linked to access to mobile banking. These clusters might have low adoption, or limited access to such services which might be causing the results to appear as such.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of internet in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$internet.1),]) +\n  tm_polygons(\"internet.1\", title = \"ß internet\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$internet.1) & (tz_dist_stat$internet.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"internet.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWe see a similar state as mobile access. In general, probability of being financially included with internet access, but for some regions, this is not the case. There is a cluster in the north and in the west-southwest that are showing a negative correlation between financial inclusion and internet access. These also appear to be different regions districts compared to the ones for mobiel access. This needs to be investigated as to why internet access is not bringing higher levels of financial inclusion to these districts.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of no_income in each district.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$no_income.1),]) +\n  tm_polygons(\"no_income.1\", title = \"ß no_income\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$no_income.1) & (tz_dist_stat$no_income.1) &gt; 0),]) +\n  tm_borders(\"darkgreen\")\n\nVariable(s) \"no_income.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWith the exception of a cluster of five districts in the centre of Tanzania, the lack of a source of income, unsurprisingly, relates to higher probability of not being financially included. What we want to focus on are districts where the impact is higher. There is a cluster in the southwest, another in the north that appear to be more impacted. These might indicate more vulnerable populations– either a high degree of unemployment, or a much lower level (quality) of welfare or government support compared to the rest of the country.\n\n\nThe code chunk below produces a row of three choropleth maps for the average coefficient of each of the three education variables in each district. Each variable’s map is created using tmap and then stored in an object which are then displayed in a row using tmap_arrange().\n\nprim &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_primary.1),]) +\n  tm_polygons(\"educ_primary.1\", title = \"ß educ_primary\", palette = \"Greens\")\n\nsec &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_secondary.1),]) +\n  tm_polygons(\"educ_secondary.1\", title = \"ß educ_secondary\", palette = \"Greens\")\n\nter &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$educ_tertiary.1),]) +\n  tm_polygons(\"educ_tertiary.1\", title = \"ß educ_tertiary\", palette = \"Greens\")\n\ntmap_arrange(prim, sec, ter, ncol = 3)\n\n\n\n\n\n\n\n\nThe output shows that education increases the chances of being financially included. The degree of impact also increases, generally, by the level of education.\n\n\nThe code chunk below produces a choropleth map for the average coefficient of head_hh in each district. We highlight districts with negative (average) coefficients with a red border.\n\ntm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$head_hh.1),]) +\n  tm_polygons(\"head_hh.1\", title = \"ß head_hh\") +\ntm_shape(tz_dist_stat[(!is.na(tz_dist_stat$head_hh.1) & (tz_dist_stat$head_hh.1) &lt; 0),]) +\n  tm_borders(\"red\")\n\nVariable(s) \"head_hh.1\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nFor most of the districts, it appears that the head of the household is more likely to be financially included (compared to non-heads of household) However, there are districts which are showing a negative coefficient. While there may be valid reasons for this (e.g., the head of household just stays home and might not be active in managing the funds) the aim is to make sure that no resident is excluded. Further investigation is needed if this can and needs to be addressed.\n\n\n\n\n\nE.5.2 Visualizing model residuals\nWe can also visualize the model residuals using the same approach. We can use the following code chunk to display the avrage residuals beside the level of financial inclusion in the district (which is simply the proportion of the indivudals that are financially included) to see if there is any pattern in the residuals and if they are related to the level of FI in a district.\n\ny &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$y),]) +\n  tm_polygons(\"y\", title = \"Formal FI\", palette=\"-viridis\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$y) & tz_dist_stat$y &lt; 0.4,]) +\n  tm_text(\"district\")\n\nres &lt;- tm_shape(tz_dist)+\n  tm_polygons(\"grey\") +\ntm_shape(tz_dist_stat[!is.na(tz_dist_stat$residual),]) +\n  tm_polygons(\"residual\", title = \"Residual\")\n\ntmap_arrange(y, res, ncol = 2)\n\nVariable(s) \"residual\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe output shows that most of the districts that have the most negative residuals also are the ones that have low financial inclusion. (50% and below) This may indicate that the calibrated model, which was based on the global calibration, is not as appropriate for them. There may be other factors that explain the level of financial inclusion in these districts."
  }
]