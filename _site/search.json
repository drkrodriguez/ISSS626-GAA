[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "",
    "text": "Welcome to my website for my work in the course ISSS 626 - Geospatial Analytics and Applications during the August term of 2024 under Dr Kam Tin Seong.\nThe course covers theory and methods of geospatial analysis and the tools in R to implement such analyses."
  },
  {
    "objectID": "index.html#hands-on-exercises",
    "href": "index.html#hands-on-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nHands-on exercises are assigned before each lesson to give us hands-on experience in using R on the geospatial analysis methods and functions we learned. We are given step-by-step instructions and explanations (through Dr Kam’s online book) which complement our pre-class reading, and prepares us for the in-class exercises.\n\nGeospatial Data Wrangling with R (for Session 1, 26 Aug ’24)\nChoropleth Mapping with R (for Session 1, 26 Aug ’24)\n1st Order Spatial Point Patterns Analysis Methods (for Session 2, 2 Sept ’24)\n2nd Order Spatial Point Patterns Analysis Methods (for Session 2, 2 Sept ’24)\nNetwork Constrained Spatial Point Pattern Analysis (for Session 3, 9 Sept 2024)\nSpatial Weights and Applications (for Session 4, 16 Sept 2024)\nGlobal Measures of Spatial Autocorrelation (for Session 5, 23 Sept 2024)\nLocal Measures of Spatial Autocorrelation (for Session 5, 23 Sept 2024)\nGeographical Segmentation with Spatially Constrained Clustering Techniques (for Session 6, 30 Sept 2024)"
  },
  {
    "objectID": "index.html#in-class-exercises",
    "href": "index.html#in-class-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nIn each session, we go through a hands-on exercise to revise the readings for the lesson. The objective of these is to further reinforce the concepts and tools learned in the readings and in the take-home exercise. These will also go further into the analysis and interpretation of results.\n\nIntroduction to Geospatial Analytics (26 Aug ’24)\nSpatial Point Pattern Analysis - Data Load for Take-home Exercise 1 (2 Sept ’24)\nAdvanced Spatial Point Patterns Analysis (9 Sept ’24)\nSpatial Autocorrelation (23 Sept’24)"
  },
  {
    "objectID": "index.html#take-home-exercises",
    "href": "index.html#take-home-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "Take-Home Exercises",
    "text": "Take-Home Exercises\nTake-home exercises are where we students are able to apply the methods and techniques learned in class in real-world cases– using real-world data and aim to generate real insights.\nFor each of these, we are given a specific problem, objectives and a base data set. Each student will work on the problem independently, guided only by the previous exercises and lessons.\n\nGeospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region\nDiscovering the Impact of COVID-19 on Thai tourism economy"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "",
    "text": "In this exercise, we apply spatial point pattern analysis to analyse the distribution of road traffic accidents in the Bangkok Metropolitan Region. We demonstrate how kernel density estimation can be used to visualize hotspots in a network constrained and non-network constrained context. We use different approaches to display charts side by side, especially to compare hotspots across different time dimensions or different conditions. Finally, we demonstrate how K- and G-functionss can be used to support any claims on the randomness, clustering or dispersion of a spatial point distribution."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.1-background",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.1-background",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.1 Background",
    "text": "A.1 Background\nRoad traffic accidents account for 1.19million deaths and up to 50 million non-fatal injuries according to a report by the WHO last year.\nThe same report identifies major risk groups: low- and middle-income countries, (esp in Africa and Europe) the working population, and males. It also identifies some key risk factors which include human error, speeding, driving under the influence of alcohol, distracted driving, unsafe road infrastructure, unsafe vehicles, and law enforcement. Most of the factors identified are behavioral in nature but do not discount that other factors may also contribute to a higher risk of occurrence.\nWithin Southeast Asia, Thailand has ranked the highest in terms of incidence of road traffic accidents with an average number of of 20,000 deaths a year or 56 a day. The country has also seen an increase in the number of accidents from 2014 to 2021. A large 19% of these accidents occurred in national highways, and the chances of encountering an accident-prone zone was found to be 66%."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.2-objectives",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.2-objectives",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nThis study aims to take a deeper look into the road accidents in Thailand, focusing on the Bangkok Metropolitan Region (BMR) which contains the capital Bangkok, and five neighboring provinces. (Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakan, Samut Sakhon)\nAs most literature has focused on behavioral and environmental factors, the study will focus on identifying spatiotemporal factors influencing the occurrence of road accidents in BMR. At the minimum, the study deliverables include the following:\n\nVisualization of spatiotemporal dynamics of road traffic accidents in BMR\nDetailed spatial analysis of road traffic accidents in BMR\nDetailed spatiotemporal analysis of road traffic accidents in BMR\n\nThe appropriate technique must be used for these deliverables and all the analysis and visualizations will be carried out using R."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.3-data-sources",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.3-data-sources",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe study makes use of the following datasets which are publicly available online.\n\n\n\nDataset Short Name\nDescription\nDatasource\n\n\n\n\nTHRA\nThailand road accident data from 2019 to 2022\nKaggle\n\n\nTHOSM\nThailand roads open street map in shapefile format\nHDX\n\n\nTHSAB\nThailand - Subnational Administrative Boudaries shapefile\nHDX"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.4-importing-and-launching-r-packages",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.4-importing-and-launching-r-packages",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, four R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nsf - package for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - package with functions for plotting cartographic quality maps\nsPNetwork - provides functions for performing SPPA methods like KDE and K-function on a network. The package can also be used to build spatial matrices to conduct traditional spatial analyses with spatial weights based on reticular distances\nspatstat - package for plotting, EDA and simulation of spatial data\n\n\n\n\npacman::p_load(sf, spNetwork, tmap, tidyverse, spatstat)\n\n\n\n\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.1-thailand-subnational-administrative-boundary-shapefile",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.1-thailand-subnational-administrative-boundary-shapefile",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.1 Thailand Subnational Administrative Boundary, Shapefile",
    "text": "B.1 Thailand Subnational Administrative Boundary, Shapefile\nWe load the Thailand subnational administrative boundary shapefile into an R dataframe using st_read() from the sf package. The source provides the geospatial data in varying levels as indicated by their suffix: country (0), province (1), district (2), and sub-district. (3) For focusing on the BMR, which covers Bangkok and neighboring provinces, province is the most likely level of detail we will need so we will use the code chunk below to load the appropriate layer first.\n\nthsab_prov &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nWe examine the loaded data to confirm the load has been done properly and to get some initial observations of the data.\n\nCalling ObjectChecking crs information with st_crs()\n\n\n\nthsab_prov\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   Shape_Leng Shape_Area                  ADM1_EN       ADM1_TH ADM1_PCODE\n1    2.417227 0.13133873                  Bangkok  กรุงเทพมหานคร       TH10\n2    1.695100 0.07926199             Samut Prakan    สมุทรปราการ       TH11\n3    1.251111 0.05323766               Nonthaburi         นนทบุรี       TH12\n4    1.884945 0.12698345             Pathum Thani        ปทุมธานี       TH13\n5    3.041716 0.21393797 Phra Nakhon Si Ayutthaya พระนครศรีอยุธยา       TH14\n6    1.739908 0.07920961                Ang Thong        อ่างทอง       TH15\n7    5.693342 0.54578838                 Lop Buri          ลพบุรี       TH16\n8    1.778326 0.06872655                Sing Buri         สิงห์บุรี       TH17\n9    2.896316 0.20907828                 Chai Nat         ชัยนาท       TH18\n10   4.766446 0.29208711                 Saraburi         สระบุรี       TH19\n   ADM1_REF ADM1ALT1EN ADM1ALT2EN ADM1ALT1TH ADM1ALT2TH  ADM0_EN   ADM0_TH\n1      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n2      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n3      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n4      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n5      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n6      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n7      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n8      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n9      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n10     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n   ADM0_PCODE       date    validOn    validTo                       geometry\n1          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.6139 13...\n2          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.7306 13...\n3          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3415 14...\n4          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.8916 14...\n5          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.5131 14...\n6          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3332 14...\n7          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((101.3453 15...\n8          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3691 15...\n9          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.1199 15...\n10         TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((101.3994 15...\n\n\n\n\n\nst_crs(thsab_prov)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\nThe output confirms that we have a multipolygon sf object with 77 rows and 17 columns. There is a column named ADM1_EN which appears to contain the province names needed to define the BMR boundaries. It also shows that the dataset is using a coordinate reference system rather than a projected reference system.\nFirst, we reload the data to use a projected reference system and apply the correct reference system with EPSG code of 32647 using st_transform(). This transformation can be confirmed with st_crs() The tmap package is then used to visualize the object to see if it properly depicts the boundaries of Thailand and its provinces.\n\nLoad Object and Transform CRS informationChecking crs information with st_crs()Plot of thsab_prov using tmap\n\n\n\nthsab_prov &lt;- st_read(dsn=\"data/geospatial\",\n                          layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\nst_crs(thsab_prov)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n\ntm_shape(thsab_prov) +\n  tm_polygons(\"grey\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.2-filtering-thsab-for-the-bangkok-metropolitan-region",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.2-filtering-thsab-for-the-bangkok-metropolitan-region",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.2 Filtering THSAB for the Bangkok Metropolitan Region",
    "text": "B.2 Filtering THSAB for the Bangkok Metropolitan Region\nBefore further analyzing the data, we will limit the scope to only consider the Bangkok Metropolitan Region or BMR. This would encompass Bangkok, Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakan, Samut Sakhon. While it is good to get insights outside of BMR, it is out of the study scope and it is best to focus on the objectives.\nThe code chunk below checks if all the provinces in the BMR appear as is under the ADM1_EN column of thsab_prov\n\nfilter(thsab_prov, ADM1_EN %in% c(\"Bangkok\", \"Nonthaburi\",\"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\"))$ADM1_EN\n\n[1] \"Bangkok\"       \"Samut Prakan\"  \"Nonthaburi\"    \"Pathum Thani\" \n[5] \"Nakhon Pathom\" \"Samut Sakhon\" \n\n\nWith the previous code returning all 6 provinces, we have confirmation that the provinces are all present and spelled as is in the data source. We create a new object bmr_boundary to contain only the provinces in BMR. We also take this opportunity to only keep the relevant columns in the dataset using the select() function of dplyr package.\n\nCreate BMR boundary object using filter()Plot of bmr_boundary using tmap\n\n\n\nbmr_boundary &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647) %&gt;%\n  filter(ADM1_EN %in% c(\"Bangkok\", \"Nonthaburi\",\"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\")) %&gt;% dplyr::select(Shape_Leng, Shape_Area, ADM1_EN, geometry)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\ntm_shape(bmr_boundary) +\n  tm_polygons(\"grey\")\n\n\n\n\n\n\n\n\n\n\n\nThe code below creates a second object which is just a union of all the provinces. (i.e., borders between provinces are lost) This is done using the st_union() function.\n\nbmr_full = st_union(bmr_boundary)\n\n\ntm_shape(bmr_full) +\n  tm_polygons(\"grey\")\n\n\n\n\n\n\n\n\nThe code below keeps the final boundary objects into files to make loading more convenient for later analyses.\n\nwrite_rds(bmr_boundary, \"data/rds/bmr_boundary.rds\")\nwrite_rds(bmr_full, \"data/rds/bmr_full.rds\")\n\nThe code below then reloads the same objects into R:\n\nbmr_boundary = read_rds(\"data/rds/bmr_boundary.rds\")\nbmr_full = read_rds(\"data/rds/bmr_full.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.3-road-accident-data-aspatial-csv-file",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.3-road-accident-data-aspatial-csv-file",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.3 Road Accident Data, Aspatial, csv-file",
    "text": "B.3 Road Accident Data, Aspatial, csv-file\nThe road accident data is contained in a csv file. We use the code block in the first tab below to load it into the thra object with some necessary transformations that we identified upon inspecting the raw file. The second tab gives an explanation of the different nested functions used in the code\n\nCode to import and transform road accident dataExplanation of the code lines / functions used\n\n\n\nbmracc &lt;- read_csv(\"data/aspatial/thai_road_accident_2019_2022.csv\")  %&gt;%\n  filter(!is.na(longitude) & longitude != \"\", \n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;%\n  st_transform(crs = 32647)\n\nbmracc &lt;- filter(bmracc, geometry %in% st_intersection(bmr_full, bmracc)) %&gt;%\n  mutate(Year = year(incident_datetime)) %&gt;%\n  mutate(MonthNum = month(incident_datetime)) %&gt;%\n  mutate(Month = month(incident_datetime, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(DayOfWeek = wday(incident_datetime, label = TRUE, abbr = TRUE))\n\n\n\n\nread_csv() used to import a csv file into an R object\nfilter(!is.na(longitude) & longitude != \"\", !is.na(latitude) & latitude != \"\") used to exclude any records where the longitude or latitude information is missing\nst_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) used to convert the dataframe into an sf object using a reference system (WGS84) based on the coordinates\nst_transform(crs = 32647) used to apply the correct EPSG code to the sf object\nfilter(bmracc, geometry %in% st_intersection(bmr_full, bmracc)) used to leave only records which fall within the BMR boundaries\nmutate(...) these lines are used to add additional columns to quickly reference the year, month and day of the week that each accident occured as these dimensions allow for some temporal analyses\n\n\n\n\nCalling the new object shows that it has 12,989 rows across 20 fields.\n\nbmracc\n\nWe use the code chunks below to check the data and visualize the data on the BMR boundary map.\n\ntm_shape(bmr_boundary) +\n  tm_polygons(col = \"grey\") +\n  tm_shape(bmracc) +\n  tm_dots(col = \"red\", size = 0.01, alpha = 0.5)\n\n\n\n\n\n\n\n\nThe code chunk below writes the resulting accident dataset into an rds file for convenient loading.\n\nwrite_rds(bmracc, \"data/rds/bmracc.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.4-thailand-roads-open-streetmap-shapefile",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.4-thailand-roads-open-streetmap-shapefile",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.4 Thailand Roads Open StreetMap, Shapefile",
    "text": "B.4 Thailand Roads Open StreetMap, Shapefile\nThe second geospatial object is the street map shapefile. We will use the object network to contain the final road network for the study.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"hotosm_tha_roads_lines_shp\")\n\nRunning the above code confirms that the dataset is in multilinestring sf format and that it contains 2.8M records across 15 variables. It also shows that there is no CRS applied to the dataset.\nBased on these, the following steps need to be done: apply the right CRS/EPSG code of 32647 or the same as bmr_full, and, filter the network to only include BMR.\nThe code below does the first step of applying a reference system and updating the EPSG code to 32647 using st_set_crs() and st_crs() from the sf package.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"hotosm_tha_roads_lines_shp\") %&gt;%\n  st_make_valid() %&gt;% st_set_crs(4326) %&gt;% st_transform(crs = st_crs(bmr_full))\n\nThe code below then finds the network within BMR by using st_intersection() to find the overlap between the full road network and the BMR boundary. We also include write_rds() in the chunk to store this object into an rds file for easy future loading.\n\nnetwork &lt;- st_intersection(network, bmr_full)\nwrite_rds(network, \"data/rds/network.rds\")\n\nCalling the object name allows us to inspect the contents.\n\nnetwork\n\nThe size of the object has now been reduced to 585K features from the original 2.8M. This still appears a very large number if we want to visualize the data, so we need to inspect if there are any opportunities to reduce the dataset by excluding any irrelevant records.\nThe data includes a column named highway which gives information on the the type or classification of the road.\n\nggplot(network, aes(x = reorder(highway, table(highway)[highway]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Roads by Highway type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"Highway Type\", y = \"Number of Roads\")\n\nThe resulting chart shows that roads with highway type of residential and service make up 522K of the 585K roads in the dataset. We refer to the OpenStreetMap wiki page to see the definition of the different types of highways in the Thailand map and see that these two highway types are access roads for residences or specific buildings. For our objective, we should be able to limit to roads where accidents (are expected to frequently) happen, and only to roads that should be accessible by vehicles. Going through the definition of the highway types, we see that the following 6 types could be out of scope for our study:\n\nresidential - road within a residential area that gives public access to one or multiple residences\nservice - minor road that gives access to buildings or places outside a residential area (e.g., to a religious site, an attraction, part of an estate)\nfootway - pathways designed for pedestrian access\ntrack - road whose only function is to provide access to surrounding land, and is most of the time unpaved\npath - multi-purpose path intended for non-motor vehicles\nsteps\n\nWe can then use the following code chunk which uses the filter() function to remove these classifications from the current network object. We call the object name in the succeeding code chunk to check the new dataset.\n\nbmr_network &lt;- bmr_network %&gt;% \n  filter(!(highway %in% c(\"residential\", \"service\", \"footway\", \"track\", \"path\", \"steps\")))\n\n\nbmr_network\n\nWhile this looks good, it looks like the object is being identified as a GEOMETRY rather than a LINESTRING object. We can use the code below to correct it.\n\nbmr_network &lt;- st_cast(bmr_network, \"LINESTRING\")\n\n\nbmr_network\n\nThe new road network object is now reduced to 34K records or roads which is a 94% reduction in the number of records. We will use some visual inspection to see if this reduction in records will affect our analysis. The two code chunks below plot the road network within the boundaries, while the second plots the three objects together. We use the tmap function to create these maps.\n\nBMR filtered road network onlyBMR filtered road network with road accident dataset\n\n\n\ntm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\") +\n  tm_shape(bmracc) +\n  tm_dots(col = \"red\", alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nFrom these two maps, we see that:\n\nwhile we have filtered 90% of the original records, the resulting map still appears dense, especially in some central areas; and,\nthe road accident locations appear to fall along the network\n\nBased on these, we will go ahead with this version of the network for our analysis.\nThe following code writes the resulting network into an rds file for more convenient loading in the future.\n\nwrite_rds(bmr_network, \"data/rds/bmr_network.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.5-resolving-duplicate-points",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.5-resolving-duplicate-points",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.5 Resolving duplicate points",
    "text": "B.5 Resolving duplicate points\nIn this section, we perform some additional transformations to perform the required analyses.\nFirst, we check the event or accident dataset to see if there are any duplicated data points or locations as the methods require that the points are unique. The code below checks if any duplicate points exist. Note that we specify the column in the argument as we are double-checking duplicate locations rather than completely duplicate records.\n\nany(duplicated(bmracc$geometry))\n\n[1] TRUE\n\n\nAs the code returned TRUE, it confirms the presence of duplicate points, we use st_jitter() from the sf package to introduce some jitter to each point and ensure that points do not lie on the same location. Without any additional arguments, the function uses a default factor 0.002 of the bounding box diagonal as the bounds for the amount of jitter introduced. In the code below, we define an amount of 0.01 instead.\n\nbmracc_jitt &lt;- st_jitter(bmracc, 0.01)\n\nRerunning the check using duplicated() shows that there are no duplicate points anymore.\n\nany(duplicated(bmracc_jitt$geometry))\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#c.1-categories-of-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#c.1-categories-of-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "C.1 Categories of accidents",
    "text": "C.1 Categories of accidents\nWe first would like to understand the different labels we can use from the BMR road accident dataset bmracc The following columns appear to be able to give some insight about the nature of the accident:\n\nvehicle_type\npresumed_cause\naccident_type\nnumber_of_vehicles_involved\nnumber_of_fatalities\nweather_condition\n\nWe will try to be brief in analysing these variables as the main intent is to understand which ones will add the most value to the spatial analysis needed to address the main study objectives.\nNote that while we use bmracc rather than the modified bmracc_jitt in the codes below, the result will be the same as we do not concern ourselves with the geometry innformation yet.\n\nC.1.1 Vehicle Type\nThis variable is intended to give the type of vehicle involved in the accident. We use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(vehicle_type, table(vehicle_type)[vehicle_type]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Vehicle type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nThe output shows that out of 12,989 accidents, 35% are with private or passenger cars, 27% are with pickup trucks and 13% are with motorcycles. These three make up 75% of all the recorded accidents while the remaining 11 types make up the balance 25%\n\n\nC.1.2 Presumed Cause\nThis variable is intended to give the presumed cause of the accident. We again use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(presumed_cause, table(presumed_cause)[presumed_cause]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Presumed Cause\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that 10,146 or 75% of the accidents are presumed to be caused by speeding. The next largest named presumed cause only accounts for 5% of the overall data.\n\n\nC.1.3 Accident Type\nThis variable is intended to give the type or nature of the accident. We again use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(accident_type, table(accident_type)[accident_type]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that “rear-end collisions” and “rollover/fallen on straight road” are the leading causes recorded and account for 83% of the accidents.\n\n\nC.1.4 Number of Fatalities\nThis variable is intended to give the number of fatalities resulting from the accident. We again use the code block below to understand the categories under this variable using a simple histogram created through ggplot().\n\nggplot(bmracc, aes(x = number_of_fatalities)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"darkgrey\") +\n  scale_x_continuous(breaks = seq(min(bmracc$number_of_fatalities), max(bmracc$number_of_fatalities), by = 1)) +\n  labs(title = \"Accidents by Number of Fatalities\", x = \"Number of Fatalities\", y = \"Number of Accidents\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.5)\n\n\n\n\n\n\n\n\nThe plot shows that 94% of the recorded accidents are non-fatal. Only 719 were fatal. Although this is a small number, it might be worth looking at the location of such fatal accidents later. We can use the code below to introduce a new column fatal into the data for more convenient filtering later.\n\nbmracc$fatal &lt;- bmracc$number_of_fatalities &gt; 0\n\n\n\nC.1.5 Weather Condition\nThis variable is intended to indicate the weather condition when the accident was recorded. We first use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(weather_condition, table(weather_condition)[weather_condition]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Weather Condition\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that 90% of the accidents occurred during “clear” weather. Online sources suggest that Bangkok experieces a long rainy season and has 153 rainy days per year, so 10% for the occurrence of accidents appears low. The sources also say that the wettest month is September.\nWe can use the code chunk below to plot the number of accidents that were not recorded on clear weather (i.e., rainy) by month.\n\nggplot(filter(bmracc, !weather_condition == \"clear\"), aes(x = reorder(Month, table(Month)[Month]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Month during Non-clear Weather\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe plot does align with the expectation that September is the wettest month. The very low number of accidents during rainy weather is still questionable though so we will watch out for this if we will use this variable later."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.1.-converting-objects-into-spatstats-formats",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.1.-converting-objects-into-spatstats-formats",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.1. Converting Objects into spatstat’s formats",
    "text": "D.1. Converting Objects into spatstat’s formats\nThe events need to be converted into spatstat’s ppp object using as.ppp()\n\nbmracc_ppp &lt;- as.ppp(st_geometry(bmracc_jitt))\n\nWe then prepare an owin object to define the boundaries using the as.owin() function.\n\nbmr_owin &lt;- as.owin(bmr_full)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.2-kde-for-all-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.2-kde-for-all-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.2 KDE for all Accidents",
    "text": "D.2 KDE for all Accidents\nWe first combine the accidents (all years, all types) into the owin using the following code chunk\n\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n\nWe can then compute for the kde using the density() function. We use the four common methods for automatic bandwidth selection and examine them in a grid using the code chunk below.\n\npar(mfrow=c(2,2))\nplot(density(bmracc_for_kde, sigma=bw.diggle,\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Diggle Method\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Scott's Rule\")\nplot(density(bmracc_for_kde, sigma=bw.CvL(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Cronie & Van Lieshout Criterion\")\nplot(density(bmracc_for_kde, sigma=bw.ppl(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Likelihood Cross Validation\")\n\n\n\n\n\n\n\n\nAmong the four methods, it looks like Scott’s rule (using sigma=bw.scott()) is identifying hot spots unlike the others. There is a hotspot in the southwest and southeast of Bangkok. There also appears to be a high density strip (or maybe a major highway) stretching up northwards.\nWe will use this bandwidth selection method for our succeeding analysis.\n\nD.3 KDE for accidents across years\nWe then want to see if the hotspots move across the years. To do this, we effectively need to compute for the kde across years and see if there are any visible signs of shifts in the hotspots.\nFirst, let us try to understand the distribution of accidents by year. This will help us understand if the numberical values of the density will move because of change in the absolute number of accidents.\nWe use ggplot() in the code chunk below to achieve this.\n\nggplot(bmracc, aes(x = reorder(Year, table(Year)[Year]))) +\n  geom_bar() +\n  ggtitle(\"Number of Accidents by Year\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), vjust = +2) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 12),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that there were significantly more accidents in 2022 compared to the previous years. The other years are within 10% of each other. We should expect that 2022 KDE may have higher numerical values compared to the other years.\nWe can then use the following code to generate four different kde’s, one for each year.\n\npar(mfrow=c(2,2))\nfor (i in c(2019, 2020, 2021, 2022)) {\n  bmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, Year == i)))\n  bmracc_for_kde = bmracc_ppp[bmr_owin]\n  bmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n  plot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = paste(\"KDE for year\",i))\n}\n\n\n\n\n\n\n\n\nWe see that there appears to be a shift between 2020 and 2021. Before 2020, there appeared to be two separate promininet hotspots for accidents– in the central and southeastern portion of the region. However, after 2020, it seems that the accidents are more frequent in the southeastern part, and in a much wider area."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.3-kde-across-months",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.3-kde-across-months",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.3 KDE across Months",
    "text": "D.3 KDE across Months\nWe can apply a similar approach of analysing by month using the code chunk below. For now, we are aggregating accidents by month across all years, so the insights will apply to the whole period and not any particular year.\n\npar(mfrow=c(3,4))\nfor (i in 1:12) {\n  bmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, MonthNum == i)))\n  bmracc_for_kde = bmracc_ppp[bmr_owin]\n  bmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n  plot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = paste(\"KDE for month\",i))\n}\n\n\n\n\n\n\n\n\nThe output reveals no drastic shift in hotspots (using kde) across months. There are months where the intensities and relative intensities differ, but it appears like the hotspots remain in the same areas."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.4-kde-for-clear-vs-rainy-days",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.4-kde-for-clear-vs-rainy-days",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.4 KDE for Clear vs “Rainy” Days",
    "text": "D.4 KDE for Clear vs “Rainy” Days\nThe final analysis we want to perform before moving to network-constrained analysis is on clear vs non-clear days. This is indicated in the field called weather_condition in the accident dataset.\nTo produce the kde visualization, we can use the code chunk below.\n\npar(mfrow=c(1,2))\n\nbmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, weather_condition == \"clear\")))\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n             edge=TRUE,kernel=\"gaussian\"), main = \"KDE for Clear Days\")\n\nbmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, !(weather_condition == \"clear\"))))\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n             edge=TRUE,kernel=\"gaussian\"), main = \"KDE for Rainy Days\")\n\n\n\n\n\n\n\n\nThe output are very similar too some of the charts generated earlier. The hotspot in the center of BMR appears to dissipate during rainy days. (relative to the one in the southeast portion of the region."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.1-preparation-of-data-for-network-constrained-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.1-preparation-of-data-for-network-constrained-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.1 Preparation of Data for Network Constrained Analysis",
    "text": "E.1 Preparation of Data for Network Constrained Analysis\nBefore we are able to perform network-constrained NKDE, we first need to define sample points along the road network, and to do that, we can use the midpoint of the lixels of the network.\n\nE.1.1 Preparing the lixels\nTo lixelize a network, the minimum and (maximum) length of lixels need to be defined. A logical distance needs to be chosen for a given study. In our case, we might no have enough information to understand what road segment length is relevant to group accidents into. However, we can start with understanding the road lengths in the network.\nWe can use the code block below to show the distribution of the road length values using summary() to give the quartiles, and quantile() to give a wider range of view.\n\nsummary(st_length(bmr_network))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n    0.143    31.581   124.691   413.002   433.312 24745.584 \n\nquantile(st_length(bmr_network), probs = seq(.1, .9, by = .1))\n\nUnits: [m]\n       10%        20%        30%        40%        50%        60%        70% \n  13.78247   24.36581   41.45909   71.64269  124.69070  207.11305  337.56988 \n       80%        90% \n 556.85935 1074.26696 \n\n\nThe output shows that there is a very wide range of values. There is also a surprisingly large number of roads (&gt;40%) that are less than 100m– which seem to be too short for typical roads. We can first choose a min distance of 200m which would allow for at least 40% of roads to not be split. As for the maximum length, let us first set it to 600m so only a little over 20% of the roads will be split into smaller segments.\nWe implement this using lixelize_lines() in the code chunk below.\n\nlixels &lt;- lixelize_lines(bmr_network$geometry, \n                         600, \n                         mindist = 200)\n\n\n\nE.1.2 Generating sample points\nThe next step is to define sample points along the network which will be the points where the KDE function will be computed on.\nWe can create sample points on the lixel centers using lines_center() in the code below.\n\nsamples &lt;- lines_center(lixels)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.2-all-years-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.2-all-years-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.2 All Years, All Accidents, Initial Analysis",
    "text": "E.2 All Years, All Accidents, Initial Analysis\nLet’s start by looking at the highest levels. We can compute for the nkde for all accidents in the dataset (2019-2022) using the code chunk below. This uses the accidents with the jitter applied.This code uses a bandwidth of 300m, anduses “quartic” for the kernel function, and uses simple calculation method for the KDE\n\ndensities &lt;- nkde(bmr_network, \n                  events = bmracc_jitt,\n                  w = rep(1, nrow(bmracc_jitt)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe import the densities into the lixel and sample object using the code below. We use a multiple of 1000 to convert the figures from accidents per square meter to accidents per square kilometer\n\nsamples$density_all &lt;- densities*1000000\nlixels$density_all &lt;- densities*1000000\n\nWe use the code below to produce a map with just the calculated densities.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe resulting map surprisingly does not show a lot of high density road segments, which is not aligned to the earlier map with the locations of the accidents. We can check if the jitter has caused displacement of the locations and hidden high density road segments by rerunning the below code chunk which uses the accident locations without jitter applied.\n\ndensities &lt;- nkde(bmr_network, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nTo see if using the original event points will address our concern, we repeat the code chunk below to create a static map.\n\nsamples$density_all &lt;- densities*1000000\nlixels$density_all &lt;- densities*1000000\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nIt looks like the use of the original accident data does little to reveal dense locations. If we examine the interactive map and zoom in, we see one possible reason for the problem. Major roads are being split into multiple semi-parallel roads. These might denote different directions on the same highway, service roads, etc. These might cause accidents on the same “parent” road to be split across their parts.\nWe try to solve this problem by recreating our network while merging such roads into one."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.3-transformation-step---merging-of-parallel-roads",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.3-transformation-step---merging-of-parallel-roads",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.3 Transformation step - merging of parallel roads",
    "text": "E.3 Transformation step - merging of parallel roads\nOne way to merge the roads, is to first use st_buffer() to create a width dimension on the roads. We would prefer to use the actual width of the roads, plus the islands here, but there is no way to do this accurately and this would vary from road to road. (e.g., some roads could have one lane, while others could have four or more lanes) For our case, we will use a width of 2 x 15m, which is based on a ~3m lane width estimate, which means we are buffering up to the width of five lanes or five cars on each side.\nWe use the code chunk below to produce a buffered network.\n\nbmr_network_buffered &lt;- st_buffer(bmr_network, dist = 15)\nbmr_network_dissolved &lt;- st_union(bmr_network_buffered)\n\nThe next step is to convert or cast this into a linestring object, but before that we would want to make sure that the geometries are simple enough so the casting is executed properly. To do this, we fist use st_simplify() which simplifies objects by reducing vertices.\n\nbmr_network_simplified &lt;- st_simplify(bmr_network_dissolved, dTolerance = 1)\n\nWith the network simplified, we can then use st_cast() to convert the geometries back into linestrings. Note that we use two calls since we cannot cast polygons directly into linestrings.\n\nbmr_network_v2 &lt;- st_cast(bmr_network_simplified, \"MULTILINESTRING\")\nbmr_network_v2 &lt;- st_cast(bmr_network_v2, \"LINESTRING\")\n\nNote that the resulting object was a list rather than a dataframe, we can use st_as_sf() to ensure that it is in an sf dataframe format.\n\nbmr_network_v2 &lt;- st_as_sf(bmr_network_v2)\n\nWe can examine the original and new network side by side using tmap_arrange() in the code below to see that the new network has worked sufficiently.\n\norig_network &lt;- tm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\") +\n  tm_layout(title = \"Original Road Network\")\n\nnew_network &lt;- tm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network_v2) +\n  tm_lines(col = \"black\")+\n  tm_layout(title = \"Simplified Road Network\")\n\n\ntmap_arrange(orig_network, new_network, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nThe transformation seems to have worked, and we see by counting the objects using length() st_geometry() in the code below, that the number of roads has been reduced dramatically from 34K to 4.2K– even with the very similar high level map.\n\nlength(st_geometry(bmr_network))\n\n[1] 34056\n\nlength(st_geometry(bmr_network_v2))\n\n[1] 4162\n\n\nLet us examine the road lengths in the updated network using summary() and quantile() in the code chunk below.\n\nsummary(st_length(bmr_network_v2))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n      4.0     326.1    1935.9    4529.2    5826.1 1263640.1 \n\nquantile(st_length(bmr_network_v2), probs = seq(.1, .9, by = .1))\n\nUnits: [m]\n       10%        20%        30%        40%        50%        60%        70% \n  111.8646   236.2789   474.0794  1023.5185  1935.9245  3231.7745  4846.3553 \n       80%        90% \n 7074.5520 10733.3337 \n\n\nThe simplified network now has longer road segments with the median being close to 2km in length.\nWe can then repeat the preparation of data from the creation of the lixels to the creation of the sample points. we will use exactly a similar code using lixelize_lines() as in the earlier sections. Given the distribution of the lengths, we decide to use longer lixel lengths with this new network. We choose 1km and 2km for the parameters.\n\nlixels_v2 &lt;- lixelize_lines(st_geometry(bmr_network_v2), \n                         2000, \n                         mindist = 1000)\nsamples_v2 &lt;- lines_center(lixels_v2) \n\nBefore we move, let us remove the intermediate objects from memory using rm()\n\nrm(bmr_network_buffered)\n\nWarning in rm(bmr_network_buffered): object 'bmr_network_buffered' not found\n\nrm(bmr_network_dissolved)\n\nWarning in rm(bmr_network_dissolved): object 'bmr_network_dissolved' not found\n\nrm(bmr_network_simplified)\n\nWarning in rm(bmr_network_simplified): object 'bmr_network_simplified' not\nfound"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-all-years-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-all-years-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.4 All Years, All Accidents, Initial Analysis",
    "text": "E.4 All Years, All Accidents, Initial Analysis\nWe now rerun the highest level KDE with the updated network to see if we are getting more insightful output.\nWe rerun nkde() to compute the network constrained KDE on the new network and using the new sample points.\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe again transfer these densities into the lixels and sample dataframes using the code chunk below\n\nsamples_v2$density_all &lt;- densities*1000000\nlixels_v2$density_all &lt;- densities*1000000\n\nNext, we can create a static map to show the computed KDEs visually using tmap package in the code chunk below.\n\ntm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\")\n\n\n\n\n\n\n\n\nWIP"
  },
  {
    "objectID": "In-class/In-class_Ex01/data/geospatial/MPSZ-2019.html",
    "href": "In-class/In-class_Ex01/data/geospatial/MPSZ-2019.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-distribution-of-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-distribution-of-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.4 Distribution of All Accidents, Initial Analysis",
    "text": "E.4 Distribution of All Accidents, Initial Analysis\nWe now rerun the highest level KDE with the updated network to see if we are getting more insightful output.\nWe rerun nkde() to compute the network constrained KDE on the new network and using the new sample points.\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe again transfer these densities into the lixels and sample dataframes using the code chunk below\n\nsamples_v2$density_all &lt;- densities*1000000\nlixels_v2$density_all &lt;- densities*1000000\n\nNext, we can create a static map to show the computed KDEs visually using tmap package in the code chunk below.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWhile the network is simplified, it looks like we still get a very homoegenous, yellow chart. This might be due to the bottom-most bin including zeros. We can inspect the number of zeros and the range of the nonzero kde’s using summary() in the code chunk below.\n\nprint(\"Distribution of all Densities\")\n\n[1] \"Distribution of all Densities\"\n\nsummary(lixels_v2$density_all)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   1.623   0.000 214.461 \n\nprint(\"Distribution of non-zero Densities\")\n\n[1] \"Distribution of non-zero Densities\"\n\nsummary(filter(lixels_v2, density_all &gt; 0)$density_all)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n  0.00004   2.83624   4.71730  11.10733  11.22094 214.46127 \n\n\nIt looks like non-zero vlues are sparse. Less than 25% of the lixles have non-zero values. We can either use a custom palette or add a layer to grey out the zero denisty lixels. We use the latter in the code chunk below. We also add some additional elements like the provinces, and modify the formatting, in order to make the chart more information-rich and readable.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 2, title.col = \"Per sq km\") +\n  tm_shape(filter(lixels_v2, density_all == 0)) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe plot now shows clear road segments where there is higher density. It reveals the most dense segments lie within Bangkok. Pathum Thani and Samut Sakhon also show some high density segments. Meanwhile, Nakohn Pathom appears to have the least accident dense road segments."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-year",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-year",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.5 Distribution of Accidents by Year",
    "text": "E.5 Distribution of Accidents by Year\nWe then look into the distribution of accidents across years to see if there is a change or shift that has occured. To do this, we need to generate the nkde for each year using the code below. The code writes a new column for the lixels and the samples dataframes for each year’s KDE values.\n\nfor (i in 2019:2022) {\n   densities &lt;- nkde(bmr_network_v2, \n                  events = filter(bmracc, Year == i),\n                  w = rep(1, nrow(filter(bmracc, Year == i))),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n  lixels_v2[[paste(\"density_\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n  samples_v2[[paste(\"density_\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n  }\n\nWe can then use the code block below to generate the map of the four different years using tmap package.\n\ncolumns_to_map &lt;- c(\"density_2019\", \"density_2020\", \"density_2021\", \"density_2022\")\nyearly = list()\nfor (col in columns_to_map)\n{\n  yearly[[col]] &lt;- tm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=col, palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_shape(lixels_v2[lixels_v2[[col]] == 0, ]) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = paste(\"Year -\",substr(col,9,12)),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n}\n\n\ntmap_arrange(yearly[[1]], yearly[[2]], yearly[[3]], yearly[[4]], ncol = 2)\n\n\n\n\n\n\n\n\nThe output shows no significant change in the location of the hotspots across years. Before we look at another dimension, let us try to test for complete spatial randomness on the most recent year."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-test-for-csr---2022-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-test-for-csr---2022-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.6 Test for CSR - 2022 Accidents",
    "text": "E.6 Test for CSR - 2022 Accidents\nIt clearly looks like accidents are not randomly or homogeneously distributed in the network. We can verify this using tests for CSR (Complete Spatial Randomness) using the K- or G-functions.\nFor our CSR test, the test hypotheses will be:\n\n\\(H_0\\) - Road accidents in 2022 are randomly distributed along the BMR road network\n\\(H_1\\) - Road accidents in 2022 are not randomly distributed along the BMR road network\n\nThe code chunk below runs these two functions for testing CSR using kfunctions() from the spNetwork package. We specify a range of 0m (start) and 2km (end) to evaluate the function. We also specify 50 Monte Carlo simulations (nsim + 1) to draw the envelope. A confidence interval (1 - conf_int) of 95%, and intervals of 200m for the steps and the donut width. We also use an agg argument to allow consolidation of events. (as the function cannot work with duplicate points)\n\nkfun_bmracc &lt;- kfunctions(bmr_network_v2, \n                             filter(bmracc_jitt, Year == 2022),\n                             start = 0, \n                             end = 2000, \n                             step = 200, \n                             width = 200, \n                             nsim = 49, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05,\n                             agg = 100)\n\nWe can output the K-function by calling on the plotk field, and the G-function by calling on the plotg field of the resulting object\n\nkfun_bmracc$plotg\n\n\n\n\n\n\n\nkfun_bmracc$plotk\n\n\n\n\n\n\n\n\nThe envelop depicts a 95% confidence level CSR interval for each function. Both functions do not support th hypothesis of CSR except for very short intervals. (where the blue lines fall within the envelope) The K-function supports the view on clustering from a distance of around 300m-1.9km, while the G-function supports this from around 250-650m. The G-function supports a view on regular distribution beyond 750m. While these differ in the details, both tests do not support CSR for the distribution of accidents in 2022."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-month",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-month",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.5 Distribution of Accidents by Month",
    "text": "E.5 Distribution of Accidents by Month\nWe can then look into the distribution of accidents across months. Seasonal events including holidays, climate, etc can be linked to the months, so it is good to see if there are months that deviate from most of the others.\nTo do this, we need to generate the nkde for each month similar to the approach for the yearly analysis.\n\nfor (i in 1:12) {\n   densities &lt;- nkde(bmr_network_v2, \n                  events = filter(bmracc, MonthNum == i),\n                  w = rep(1, nrow(filter(bmracc, MonthNum == i))),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n   if (i &gt; 9){\n     lixels_v2[[paste(\"density_M\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n     samples_v2[[paste(\"density_M\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n   }\n   else{\n     lixels_v2[[paste(\"density_M0\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n     samples_v2[[paste(\"density_M0\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n   }\n  \n  }\n\nWe can then use the code block below to generate the map of the different months using tmap package.\n\ncolumns_to_map &lt;- c(\"density_M01\", \"density_M02\", \"density_M03\",\n                    \"density_M04\",\"density_M05\",\"density_M06\",\n                    \"density_M07\", \"density_M08\", \"density_M09\",\n                    \"density_M10\",\"density_M11\",\"density_M12\")\nmonthly = list()\nfor (col in columns_to_map)\n{\n  monthly[[col]] &lt;- tm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=col, palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_shape(lixels_v2[lixels_v2[[col]] == 0, ]) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = paste(\"Month -\",substr(col,10,11)),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n}\n\nTo be able to view the maps clearly, we display them individually in the tabs below.\n\nJanuaryFebruaryMarchAprilMayJuneJulyAugustSeptemberOctoberNovemberDecember\n\n\n\nmonthly[[1]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[2]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[3]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[4]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[5]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[6]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[7]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[8]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[9]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[10]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[11]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[12]]\n\n\n\n\n\n\n\n\n\n\n\nThe output shows a few hotspots arising on specific months. We mention the ones where there are highly dense segments outside Bangkok:\n\nPathum Thani - January, March, June\nSamut Sakhon - January"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-fatal-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-fatal-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.6 Fatal Accidents",
    "text": "E.6 Fatal Accidents\nAs mentioned in the earlier section, only a small 6% or 719 of the total number of accidents were fatal. While small, this is still a large number for the affected families. We expect that such accidents would have also caused more disruption compared to most of the non-fatal ones.\n\nE.6.1 Fatal Accidents by Year and by Province, Non-Network Constrained\nIf we look at the distribution of these accidents across years using the chart below, the annual number has ranged from 153-203, and 2020 and 2021 had 33% more accidents than the other years.\n\nggplot(filter(bmracc, number_of_fatalities &gt; 0), aes(x = reorder(Year, table(Year)[Year]))) +\n  geom_bar() +\n  ggtitle(\"Number of Fatal Accidents by Year\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), vjust = +2) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 12),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nWe can then visualize the location of these using the tmap package. The code below displays a map of the location of the fatal accidents in BMR between 2019 and 2022.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(bmr_network_v2)+\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = \"BMR Fatal Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nIt looks like fatal accidents are more frequent outside Bangkok. With the exception of Nakhon Pathom, it looks like there is a larger number of accidents, in number and in density, happening in the four other provinces.\nIt is hard to judge whether or not there are more accidents in a province using the graph above because of the presence of close or overlapping dots. One approach is to use the functions from sf package to count the accidents or events that fall within each province and then also compute for a density by computing the areas of each province.\nWe first compute for the number of fatal accidents that occur in each province by using st_intersects() function from sf package. We produce this for the total number of accidents and the accidents for each year by using the code chunk below. We create a copy of the bmr_boundary object to store these values.\n\nbmr_with_fatacc &lt;- bmr_boundary %&gt;%\n  mutate('FatAcc19-22' = lengths(st_intersects(bmr_boundary,filter(bmracc, number_of_fatalities &gt; 0)))) %&gt;%\n  mutate('FatAcc19' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2019) )))) %&gt;%\n  mutate('FatAcc20' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2020) )))) %&gt;%\n  mutate('FatAcc21' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2021) )))) %&gt;%\n  mutate('FatAcc22' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2022) ))))\n\nWe then compute for the area of each province using st_area() in the code chunk below.\n\nbmr_with_fatacc$Area &lt;- st_area(bmr_with_fatacc)\n\nFinally, we can compute for the density of accidents in each province by taking the ration of the last two measures we computed. Note that we are multiplying each by 1 million to convert the units from per meter to per square kilometer.\n\nbmr_with_fatacc$FatDensAll &lt;- bmr_with_fatacc$`FatAcc19-22` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens19 &lt;- bmr_with_fatacc$`FatAcc19` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens20 &lt;- bmr_with_fatacc$`FatAcc20` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens21 &lt;- bmr_with_fatacc$`FatAcc21` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens22 &lt;- bmr_with_fatacc$`FatAcc22` / bmr_with_fatacc$Area * 1000000\n\nWe can now compare the occurence of fatal accidents across provinces visually. First, we can produce a choropleth map for the number of accidents and the density of accidents side by side using the code below. This is done by passing a list of arguments for the different tmap elements. In the code below, we use this on the color of the polygons, the label and the chart title.\n\ntm_shape(bmr_with_fatacc)+\n  tm_polygons(col = c('FatAcc19-22', 'FatDensAll'), style = \"equal\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = c(\"Number\", \"Per Sq Km\"))+\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = c(\"Fatal Road Accidents 19-22\",\"Fatal Road Accidents Density\"),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe plot shows that Pathum Thani in the northeast and Samut Prakan in the southeast have the highest number of accidents. However, if we normalize by the area, Nonthaburi in the center and Samut Prakan, still, have the highest density of fatal accidents.\nWe can use the same approach to look at the density across the different years.\n\ntm_shape(bmr_with_fatacc)+\n  tm_polygons(col = c('FatDens19', 'FatDens20', 'FatDens21', 'FatDens22'), style = \"equal\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Per Sq Km\")+\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = c(\"2019 Fatal Accidents\",\"2020 Fatal Accidents\",\"2021 Fatal Accidents\",\"2022 Fatal Accidents\"),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nAt a province-level, Nonthaburi has been consistently the most dense with regards to fatal road accidents. In the meantime, Samut Prakan appears to have increased its density and risen in rank between 2020 and 2021.\nWe will not attempt to perform kde on this set of accidents and go straight to an nkde which considers the road network.’\n\n\nE.6.2 Fatal Accidents, Network Constrained\nThe final analyses we will perform is on the distribution of the fatal accidents along the road network. For this, we will also focus only on one year– 2022, as we recognize a shift in the hotspots over the years, at least across provinces\nTo facilitate the analysis, we create a subset of the accident dataset to only consider fatal accidents and the latest year.\n\nbmr_fatacc_2022 &lt;- filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2022) )\n\n\nbmr_fatacc_2022\n\nSimple feature collection with 159 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 610274.5 ymin: 1489002 xmax: 706582.6 ymax: 1570658\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 159 × 22\n   acc_code incident_datetime   report_datetime     province_th  province_en  \n *    &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;        &lt;chr&gt;        \n 1  5484851 2022-01-01 02:30:00 2022-01-02 02:43:00 สมุทรปราการ   Samut Prakan \n 2  5493686 2022-01-01 05:30:00 2022-01-03 10:23:00 นนทบุรี        Nonthaburi   \n 3  5493725 2022-01-01 14:00:00 2022-01-03 10:28:00 นครปฐม       Nakhon Pathom\n 4  6950193 2022-01-03 21:00:00 2022-09-21 10:53:00 นนทบุรี        Nonthaburi   \n 5  5837858 2022-01-03 23:34:00 2022-03-03 14:19:00 กรุงเทพมหานคร Bangkok      \n 6  6567040 2022-01-04 21:19:00 2022-01-05 11:27:00 สมุทรปราการ   Samut Prakan \n 7  6567055 2022-01-09 22:00:00 2022-01-11 10:56:00 ปทุมธานี       Pathum Thani \n 8  5861456 2022-01-11 18:00:00 2022-03-07 11:28:00 สมุทรปราการ   Samut Prakan \n 9  5579080 2022-01-15 00:10:00 2022-01-15 09:11:00 กรุงเทพมหานคร Bangkok      \n10  5843213 2022-01-16 11:30:00 2022-03-04 10:48:00 ปทุมธานี       Pathum Thani \n# ℹ 149 more rows\n# ℹ 17 more variables: agency &lt;chr&gt;, route &lt;chr&gt;, vehicle_type &lt;chr&gt;,\n#   presumed_cause &lt;chr&gt;, accident_type &lt;chr&gt;,\n#   number_of_vehicles_involved &lt;dbl&gt;, number_of_fatalities &lt;dbl&gt;,\n#   number_of_injuries &lt;dbl&gt;, weather_condition &lt;chr&gt;, road_description &lt;chr&gt;,\n#   slope_description &lt;chr&gt;, geometry &lt;POINT [m]&gt;, Year &lt;dbl&gt;, MonthNum &lt;dbl&gt;,\n#   Month &lt;ord&gt;, DayOfWeek &lt;ord&gt;, fatal &lt;lgl&gt;\n\n\nThere are 159 fatal accidents in the BMR in 2022, and in the new dataset– consistent with the summary in the previous section.\nWe then use the following code chunk to compute for the network-constrained KDE of the 2022 fatal accidents in our network. Note that in the code, we create a duplicate lixels and samples object to keep the original one unchanged. We take this opportunity to widen the computation range by increasing the bandwidth or bw parameter and the max_depth parameter.\n\nsamples_22 &lt;- samples_v2\nlixels_22 &lt;- lixels_v2\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmr_fatacc_2022,\n                  w = rep(1, nrow(bmr_fatacc_2022)),\n                  samples = samples_22,\n                  kernel_name = \"quartic\",\n                  bw = 10000, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 20,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nsamples_22$density_22 &lt;- densities*1000000\nlixels_22$density_22 &lt;- densities*1000000\n\nWe can then use the following codeblock to visualize the nKDE using tmap package.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(lixels_22)+\n  tm_lines(col=\"density_22\", palette = \"-inferno\", lwd = 2, title.col = \"Per sq km\") +\n  tm_shape(filter(lixels_22, density_22 == 0)) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = \"BMR Fatal Road Accident Density - 2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we learn how to compute and interpret global measures of spatial autocorrelation (GMSA) using the spdep package.\nThis exercise is based on Chapter 9 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#analytical-question",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Analytical Question",
    "text": "Analytical Question\nOne of the main development objective in spatial policy is for local governments and planners to ensure that there is equal distribution of development in the province. We then need to apply the appropriate spatial methods to verify if there is indeed even distribution of wealth geographically. If there is uneven distribution, then the next step is to identify if and where clusters are happening.\nWe continue studying the Hunan Province in China and focus on GDP per capita as the key indicator of development."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-sources",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-sources",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on the Hunan county coming from two files:\n\nHunan county boundary layer in ESRI shapefile format\nHunan local development indicators for 2012 stored in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#installing-and-launching-r-packages",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, tmap, and spdep.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspdep - functions to create spatial weights, autocorrelation statistics\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\nWe also define a random seed value for repeatability of any simulation results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-loading",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-loading",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Hunan shapefile into an R object.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe then use the code chunk below to load the csv file with the indicators into R using read_csv()\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-preparation",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-preparation",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-the-development-indicator",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-the-development-indicator",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package. We present these uas two maps using classes of equal intervals and equal quantiles.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#computing-contiguity-spatial-weights",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#computing-contiguity-spatial-weights",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nPrior to computing GMSA’s, we need t construct spatial weights of the study area. Spatial weights are used to define the neighborhood relationship between units. (i.e., neighbors or adjacent units)\nThe code chunk below uses poly2nb() of the spdep package to compute contiguity weight matrices for the study area. The function builds a neighbor list based on regions with shared boundaries. The queen argument takes TRUE (default) or FALSE as options. This instructs the function if Queen criteria should be used in defining neighbors. For the code below, we use the Queen criteria to build the contiguity matrix\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output shows that there are 88 units in the hunan dataset, The most connected unit has 11 neighbors and two units have only one neighbor."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#row-standardised-weights-matrix",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#row-standardised-weights-matrix",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Row-standardised weights matrix",
    "text": "Row-standardised weights matrix\nThe next step is assigning weights to each neighbor. For our case, we assign equal weight (using style=\"W\") to each neighboring polygon. This assigns the fraction 1/n, where n is the number of neighbors a unit has, as the weight of each unit’s neighbor. The drawback of this approach is that polygons in the edge of the study area will base their value on a smaller number of neighbors. This means that we may be potentially over- or under-estimating the true nature of spatial autocorrelation. The alternative more robust style=\"B\" can address this.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-test",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-test",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Moran’s I test",
    "text": "Moran’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of the spdep package\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nThe p-value does not support CSR for the GDPPC, while a positive statistic indicates signs of clustering. If the statistic value were below 0, or negative, then it would indicate signs of dispersion."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-morans-i",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-morans-i",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Monte Carlo Simulation for Moran’s I",
    "text": "Monte Carlo Simulation for Moran’s I\nWe use the code chunk below to perform permutation test for the statistic by using moran.mc() of spdep. The nsim argument is set so that 1000 simulations will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-morans-i",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-morans-i",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of Monte Carlo Simulation Results (Moran’s I)",
    "text": "Visualization of Monte Carlo Simulation Results (Moran’s I)\nIt is good practice to analyse and visualize the simulation results in more detail. We can do this by checking the values and distribution of the statistic numerically and graphically.\nWe can use the code chunk below to show individual statistics of the simulated value.\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\npaste(\"Standard Dev:\", var(bperm$res[1:999]))\n\n[1] \"Standard Dev: 0.00437157393477615\"\n\n\nWe can visualize graphically using hist() and abline() from R Graphics.\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-test",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-test",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Geary’s C test",
    "text": "Geary’s C test\nThe code chunk below uses geary.test() to perform Geary’s C test for spatial autocorrelation.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nGeary’s C test uses a different interpretation compared to Moran’s I. A statistic value less than one, as in the case above, indicates signs of clustering, while a value of greater than one indicates dispersion. The very low p-value means that any hypothesis of compete spatial randomness (with α &gt; 0.015%) is not supported by the observed data."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-gearys-c",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-gearys-c",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Monte Carlo Simulation for Geary’s C",
    "text": "Monte Carlo Simulation for Geary’s C\nWe use the code chunk below to perform permutation test for the statistic by using geary.mc() of spdep. The nsim argument is set so that 1000 simulations will be performed.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-gearys-c",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-gearys-c",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of Monte Carlo Simulation Results (Geary’s C)",
    "text": "Visualization of Monte Carlo Simulation Results (Geary’s C)\nIt is good practice to analyse and visualize the simulation results in more detail. We can do this by checking the values and distribution of the statistic numerically and graphically.\nWe can use the code chunk below to show individual statistics of the simulated value.\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\npaste(\"Standard Dev:\", var(bperm$res[1:999]))\n\n[1] \"Standard Dev: 0.00743649278244122\"\n\n\nWe can visualize graphically using hist() and abline() from R Graphics.\n\nhist(bperm$res,\n     freq=TRUE,\n     breaks=20,\n     xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-correlogram-and-plot",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-correlogram-and-plot",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Moran’s I Correlogram and Plot",
    "text": "Moran’s I Correlogram and Plot\nThe code chunk below uses sp.correlogram() of spdep package to compute a 6-lag (order=6) spatial correlogram of GDPPC using Moran’s I. (method=\"I\") We then plot() to produce the visualization.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\nAside from the output, we can also display the full content of the analysis using the code below. This lets us see the result for each lag in more detail.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-correlogram-and-plot",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-correlogram-and-plot",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Geary’s C Correlogram and Plot",
    "text": "Geary’s C Correlogram and Plot\nThe code chunk below uses sp.correlogram() of spdep package to compute a 6-lag (order=6) spatial correlogram of GDPPC using Geary’s C. (method=\"C\") We then plot() to produce the visualization.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\nWe can also examine the results in more detail using the code chunk below\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we learn how to compute and interpret local measures of spatial autocorrelation or local indicators of spatial association (LISA) using the spdep package.\nThis exercise is based on Chapter 10 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#analytical-question",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Analytical Question",
    "text": "Analytical Question\nOne of the main development objective in spatial policy is for local governments and planners to ensure that there is equal distribution of development in the province. We then need to apply the appropriate spatial methods to verify if there is indeed even distribution of wealth geographically. If there is uneven distribution, then the next step is to identify if and where clusters are happening.\nWe continue studying the Hunan Province in China and focus on GDP per capita as the key indicator of development."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-sources",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-sources",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on the Hunan county coming from two files:\n\nHunan county boundary layer in ESRI shapefile format\nHunan local development indicators for 2012 stored in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#installing-and-launching-r-packages",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, tmap, and spdep.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspdep - functions to create spatial weights, autocorrelation statistics\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\nWe also define a random seed value for repeatability of any simulation results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-loading",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-loading",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Hunan shapefile into an R object.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe then use the code chunk below to load the csv file with the indicators into R using read_csv()\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-preparation",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#data-preparation",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#visualization-of-the-development-indicator",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#visualization-of-the-development-indicator",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package. We present these uas two maps using classes of equal intervals and equal quantiles.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-contiguity-spatial-weights",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-contiguity-spatial-weights",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nPrior to computing LISA’s, we need t construct spatial weights of the study area. Spatial weights are used to define the neighborhood relationship between units. (i.e., neighbors or adjacent units)\nThe code chunk below uses poly2nb() of the spdep package to compute contiguity weight matrices for the study area. The function builds a neighbor list based on regions with shared boundaries. The queen argument takes TRUE (default) or FALSE as options. This instructs the function if Queen criteria should be used in defining neighbors. For the code below, we use the Queen criteria to build the contiguity matrix\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output shows that there are 88 units in the hunan dataset, The most connected unit has 11 neighbors and two units have only one neighbor."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#row-standardised-weights-matrix",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#row-standardised-weights-matrix",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Row-standardised weights matrix",
    "text": "Row-standardised weights matrix\nThe next step is assigning weights to each neighbor. For our case, we assign equal weight (using style=\"W\") to each neighboring polygon. This assigns the fraction 1/n, where n is the number of neighbors a unit has, as the weight of each unit’s neighbor. The drawback of this approach is that polygons in the edge of the study area will base their value on a smaller number of neighbors. This means that we may be potentially over- or under-estimating the true nature of spatial autocorrelation. The alternative more robust style=\"B\" can address this.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-local-morans-i",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#computing-local-morans-i",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Moran’s I",
    "text": "Computing Local Moran’s I\nWe use localmoran() of the spdep package to compute for the local Moran’s I statistic. The function computes for a set of Ii values based on a set of zi values and a listw object which provides the neighbor weight information.\nThe code chunk below computes for the local Moran’s I of the GDPPC variable.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nThe function returns a matrix with the following columns:\n\nIi - the local Moran’s statistic\nE.Ii - the expected value of the statistic under randomisation hypothesis\nV.Ii - the variance of the statistic under randomisation hypothesis\nZ.Ii - the standard deviate of the statistic\nPr(z != E(Ii)) - the p-value of the local Moran statistic\n\nThe code chunk below displays the content of the local Moran matrix by using printCoefmat()\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-the-local-morans-i",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-the-local-morans-i",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Mapping the local Moran’s I",
    "text": "Mapping the local Moran’s I\nBefore mapping, we append the local Moran’s I dataframe to the hunan SpatialPolygonDataFrame using the code chunk below.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nThe code chunk below uses the tmap package to plot the local Moran’s I statistic values.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nThe map shows evidence of positive and negative Ii values. It is good to consider the p-values for these regions. We use the code chunk below to plot the p-values\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nFor better interpretation, we should consider having these two maps side by side like in the code below.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-moran-scatterplot",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-moran-scatterplot",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting the Moran Scatterplot",
    "text": "Plotting the Moran Scatterplot\nThe Moran scatterplot illustrates the relationship between the value of a chosen attribute against the average of that value across neighbors. The code chunk below uses moran.plot() of spdep package to produce the Moran scatterplot of GDPPC\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\nThe chart is split into quadrants based on the region’s GDPPC and their neighbors’ average or their lagged GDPPC."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-moran-scatterplot-with-standardised-variables",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-moran-scatterplot-with-standardised-variables",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting Moran scatterplot with standardised variables",
    "text": "Plotting Moran scatterplot with standardised variables\nWe can use scale() to center and scale the variable as in the code chunk below. The final function in the code, as.vector(), ensures that we get a vector out of this transformation, that we can then map into our target dataframe.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nWe can then rerun the scatterplot with standardised variables using the code chunk below\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#preparing-lisa-map-classes",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#preparing-lisa-map-classes",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Preparing LISA map classes",
    "text": "Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nWe then derive the spatially lagged variable of interest, GDPPC, and center it around its mean by using the code below.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nWe then center the local Moran’s statistics around the mean\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nWe set a 5% statistical significance level for the local Moran\n\nsignif &lt;- 0.05       \n\nThe code chunk below defines the four different quadrants or categories\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, the code chunk below places the non-significant Moran in category 0 (zero)\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nThe previous steps can be rewritten into a single code chunk below\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-lisa-map",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#plotting-the-lisa-map",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Plotting the LISA map",
    "text": "Plotting the LISA map\nThe code chunk below builds the LISA map using tmap package\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nFor better and more effective interpretation, we can again plot the LISA map and the original GDPPC values side by side using the code chunk below\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#getis-and-ords-g-statistics",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#getis-and-ords-g-statistics",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Getis and Ord’s G-statistics",
    "text": "Getis and Ord’s G-statistics\nThe Getis and Ord’s G-statistics looks at neighbors based on a defined proximity to identify high (hot spots) or low (cold spots) value clusters.\nThe analysis consists of three steps:\n\nDeriving a spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#deriving-distance-based-weight-matrix",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#deriving-distance-based-weight-matrix",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Deriving distance-based weight matrix",
    "text": "Deriving distance-based weight matrix\nFor Getis-Ord, we need to define neighbors based on distance, which can be done by using fixed distance weights or adaptive distance.\n\nDeriving the Centroid\nWe need to define the centroids of each polygon. This consists of multiple steps as we cannot directly use st_centroid() directly on our object for our problem.\nWe first get the longitude values and then map the st_centroid() function on them.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for the latitude values using the code chunk below\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nWith the centroid for the longitude and latitude calculated, we can bind them into a single object using cbind()\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\nDetermining the cut-off distance\nWe then determine the upper limit for the distance bands using the following steps\n\nCreate a matrix with indices of points belonging to k-nearest neighbors using knearneigh() of the spdep package\nConvert the matrix into a neighbors list of class nb by using knn2nb()\nReturn the length of neighbor relationship edges by using nbdists()\nRemove the list structure of the returned object using unlist()\n\nThe code chunk below executes these steps\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\nComputing fixed-distance weight matrix\nWe then compute the distance matrix using dnearneigh() in the code chunk below\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nWe then convert th nb object into a spatial weights object using nb2listw() in the chunk below\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\nComputing adaptive distance matrix\nFixed distance weight matrices will result to units in densely packed areas having more neighbors than less densely packed areas.\nWe can control the numbers of neighbors directly using knn, either accepting asymmetric neighbors or imposing symmetry using the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nWe then convert the nb object into a spatial weights object using the code below\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-fixed-distance",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-fixed-distance",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Gi statistics using fixed distance",
    "text": "Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or G* values.\nThe Gi statistics are represented as Z-scores. Greater values represent grester clustering intensity while the sign indicates the high (positive) or low (negative) clusters.\nWe then join the Gi values with the corresponding units in hunan using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-gi-values-with-fixed-distance-weights",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#mapping-gi-values-with-fixed-distance-weights",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Mapping Gi values with fixed distance weights",
    "text": "Mapping Gi values with fixed distance weights\nThe code below maps the Gi values using a fixed distance weight matrix\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap_fix &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap_fix, asp=1, ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-adaptive-distance",
    "href": "Hands-on/Hands-On_Ex08/Hands-On_Ex08.html#gi-statistics-using-adaptive-distance",
    "title": "Local Measures of Spatial Autocorrelation",
    "section": "Gi statistics using adaptive distance",
    "text": "Gi statistics using adaptive distance\nThe code chunk below computes the Gi values for GDPPC by suing an adaptive distance matrix\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\nWe then visualize this (also beside the original GDPPC values) using the code below.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap_ad &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap_ad, \n             asp=1, \n             ncol=2)\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWe show the fixed and adaptive maps side-by-side using the chunk below\n\ntmap_arrange(Gimap_fix,Gimap_ad, asp=1,ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "",
    "text": "(Exercise Summary)"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.1-background",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.1-background",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.1 Background",
    "text": "A.1 Background\nWIP"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.2-objectives",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.2-objectives",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nWIP"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.3-data-sources",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.3-data-sources",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nWIP"
  },
  {
    "objectID": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.4-importing-and-launching-r-packages",
    "href": "Take-home/Take-home_Ex02/Take-home_Ex02.html#a.4-importing-and-launching-r-packages",
    "title": "Discovering Impact of COVID-19 on Thai Tourism Economy",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\n(WIP) For this study, XXX R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n{r} set.seed(1234)}"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this exercise, we are introduced to the sfdep package which is a wrapper on spdep and enables us to work directly with sf objects. It is also written in such a way to fully take advantage of the tidyverse framework."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#data-preparation",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#data-preparation",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualization-of-the-development-indicator",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualization-of-the-development-indicator",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package.\n\ntm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Hunan GDP per capita\")"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-1-computing-deriving-queens-contiguity-weights",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-1-computing-deriving-queens-contiguity-weights",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 1: Computing Deriving Queen’s Contiguity Weights",
    "text": "Step 1: Computing Deriving Queen’s Contiguity Weights\nWe use the code chunk below to compute for the contiguity weight matrix using Queen’s criterion.\n\nwm_q &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style=\"W\"),\n         .before=1)\n\nThe st_weights() function allows three arguments:\n\nnb -\nstyle -\nallow_zero -"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-2a-performing-global-morans-i-test",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-2a-performing-global-morans-i-test",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 2a: Performing Global Moran’s I Test",
    "text": "Step 2a: Performing Global Moran’s I Test\nThe Global Moran’s I test can be performed using global_moran_test() of the sfdep package.\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nAt α=0.05, the test shows that we reject a null hypothesis that the GDPPC values are randomly distributed. As the test statistic is above 0, then the data is showing signs of clustering."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#step-2b-performing-global-morans-i-permutation-test",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#step-2b-performing-global-morans-i-permutation-test",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Step 2b: Performing Global Moran’s I Permutation Test",
    "text": "Step 2b: Performing Global Moran’s I Permutation Test\nMonte Carlo simulation on the (Global Moran’s I) statistic is performed using global_moran_perm() of the sfdep package. The code chunk below performs 100 simulations (nsim + 1)\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nWe get consistent result with the one-time run, but with a lower p-value. (and higher confidence)"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-morans-i",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-morans-i",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Moran’s I",
    "text": "Computing Local Moran’s I\nWe compute for the local Moran’s I statistic for each unit by using local_moran() of sfdep package. The unnest() function expands the elements of list local_moran as separate columns in the lisa object.\n\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\nWe can examine the columns of lisa using the code chunk below.\n\nglimpse(lisa)\n\nRows: 88\nColumns: 21\n$ ii           &lt;dbl&gt; -1.468468e-03, 2.587817e-02, -1.198765e-02, 1.022468e-03,…\n$ eii          &lt;dbl&gt; 0.0017692414, 0.0064149158, -0.0374068734, -0.0000348833,…\n$ var_ii       &lt;dbl&gt; 4.179959e-04, 1.051040e-02, 1.020555e-01, 4.367565e-06, 1…\n$ z_ii         &lt;dbl&gt; -0.15836231, 0.18984794, 0.07956903, 0.50594053, 0.448752…\n$ p_ii         &lt;dbl&gt; 0.874171311, 0.849428289, 0.936580031, 0.612898396, 0.653…\n$ p_ii_sim     &lt;dbl&gt; 0.82, 0.96, 0.76, 0.64, 0.50, 0.82, 0.08, 0.08, 0.02, 0.2…\n$ p_folded_sim &lt;dbl&gt; 0.41, 0.48, 0.38, 0.32, 0.25, 0.41, 0.04, 0.04, 0.01, 0.1…\n$ skewness     &lt;dbl&gt; -0.8122108, -1.0905447, 0.8239085, 1.0401038, 1.6357304, …\n$ kurtosis     &lt;dbl&gt; 0.651875433, 1.889177462, 0.046095140, 1.613439800, 3.960…\n$ mean         &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ median       &lt;fct&gt; High-High, High-High, High-High, High-High, High-High, Hi…\n$ pysal        &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ nb           &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1,…\n$ wt           &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0…\n$ NAME_2       &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"C…\n$ ID_3         &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2…\n$ NAME_3       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ ENGTYPE_3    &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"C…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7…\n$ geometry     &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.228…\n\n\nThe local_moran() function generated 12 columns– which are the first twelve in the lisa dataframe. Key columns are:\n\nii - local Moran i statistic\np_ii_sim - p value from simulation\nFor the clustering / outlier classification, there are three options in different columns: mean, median, pysal."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-local-moran-is",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-local-moran-is",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualising Local Moran I’s",
    "text": "Visualising Local Moran I’s\nThe code chunk below prepares a choropleth map of the statistic in the ii and the p_ii_sim field\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(c(\"ii\", \"p_ii_sim\"), title = c(\"Local Moran's I\",\"P Value\")) +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"Local Moran's I and P-values\")\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#lisa-map",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#lisa-map",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "LISA map",
    "text": "LISA map\nA LISA map is a categorical map showing outliers and clusters.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii &lt; 0.05)\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-gi-statistics",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#computing-local-gi-statistics",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Local Gi* Statistics",
    "text": "Computing Local Gi* Statistics\nThe code below computes the weight matrix using inverse distance.\n\nwm_idw &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before=1)\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wts = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nWe then compute the local Gi* by using the code below.\n\nHCSA &lt;- wm_idw %&gt;%\n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0416 Low     0.0114 0.00000641  0.0493 9.61e-1  0.7          0.35    0.875\n 2 -0.333  Low     0.0106 0.00000384 -0.0941 9.25e-1  1            0.5     0.661\n 3  0.281  High    0.0126 0.00000751 -0.151  8.80e-1  0.9          0.45    0.640\n 4  0.411  High    0.0118 0.00000922  0.264  7.92e-1  0.6          0.3     0.853\n 5  0.387  High    0.0115 0.00000956  0.339  7.34e-1  0.62         0.31    1.07 \n 6 -0.368  High    0.0118 0.00000591 -0.583  5.60e-1  0.72         0.36    0.594\n 7  3.56   High    0.0151 0.00000731  2.61   9.01e-3  0.06         0.03    1.09 \n 8  2.52   High    0.0136 0.00000614  1.49   1.35e-1  0.2          0.1     1.12 \n 9  4.56   High    0.0144 0.00000584  3.53   4.17e-4  0.04         0.02    1.23 \n10  1.16   Low     0.0104 0.00000370  1.82   6.86e-2  0.12         0.06    0.416\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;"
  },
  {
    "objectID": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-gi",
    "href": "In-class/In-class_Ex04/In-class_Ex04.html#visualising-gi",
    "title": "Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualising Gi*",
    "text": "Visualising Gi*\nThe code chunk\n\ntm_shape(HCSA) +\n  tm_polygons()+\ntm_shape(filter(HCSA,p_sim &lt; 0.05)) +\n  tm_polygons(c(\"cluster\",\"p_sim\"), title=c(\"Cluster\",\"P-Value\"))"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "",
    "text": "In this hands-on exercise, we apply hierarchical cluster analysis and spatially constrained cluster analysis to delineate homogeneous regions based on geographically referenced data.\nThis exercise is based on Chapter 12 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#analytical-question",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Analytical Question",
    "text": "Analytical Question\nIn the development of spatial policy and for business, it is often important to segregate homogenous regions using multivariate data. We apply techniques in the study of Shan State in Myanmar by using various indicators."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-sources",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-sources",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on information for Myanmar and for its Shan state:\n\nMyanmar township boundary data in ESRI shapefile format (polygon)\nShan state ICT indicators for 2014 contained in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#installing-and-launching-r-packages",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of thirteen R packages:\n\nsf, rgdal, spdep - for spatial data handling\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\ncoorplot, ggpubr, heatmaply - packages for multivariate data visualization and analysis\ncluster, ClustGeo - packages for performing cluster analysis\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\nWe also define a random seed value for repeatability where of any randmoized results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Myanmar township boundary shapefile into an R object. The code chunk includes a pipeline to already filter to the Shan state and include only the relevant columns.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex09\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-boundary",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-boundary",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Data Loading - Shan state boundary",
    "text": "Data Loading - Shan state boundary\nThe code chunk below uses st_read() of the sf package to load the Myanmar township boundary shapefile into an R object. The code chunk includes a pipeline to already filter to the Shan state and include only the relevant columns.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex09\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nWe can inspect the contents of shan_sf using the code chunk below\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nThe sf dataframe conforms to the tidy framework. Given this, we can also use glimpse() to reveal the fields’ data types.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-2014-indicators-aspatial",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-loading---shan-state-2014-indicators-aspatial",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Data Loading - Shan state 2014 indicators (aspatial)",
    "text": "Data Loading - Shan state 2014 indicators (aspatial)\nThe code chunk below uses read_csv() to load the contents of the csv file into an object ict\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\nRows: 55 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): District Pcode, District Name, Township Pcode, Township Name\ndbl (7): Total households, Radio, Television, Land line phone, Mobile phone,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can use head() to check the first 6 elements of the object,\n\nhead(ict)\n\n# A tibble: 6 × 11\n  `District Pcode` `District Name` `Township Pcode` `Township Name`\n  &lt;chr&gt;            &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;          \n1 MMR014D001       Taunggyi        MMR014001        Taunggyi       \n2 MMR014D001       Taunggyi        MMR014002        Nyaungshwe     \n3 MMR014D001       Taunggyi        MMR014003        Hopong         \n4 MMR014D001       Taunggyi        MMR014004        Hsihseng       \n5 MMR014D001       Taunggyi        MMR014005        Kalaw          \n6 MMR014D001       Taunggyi        MMR014006        Pindaya        \n# ℹ 7 more variables: `Total households` &lt;dbl&gt;, Radio &lt;dbl&gt;, Television &lt;dbl&gt;,\n#   `Land line phone` &lt;dbl&gt;, `Mobile phone` &lt;dbl&gt;, Computer &lt;dbl&gt;,\n#   `Internet at home` &lt;dbl&gt;\n\n\nand summary() to display summary statistics of the numeric columns.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThe dataset contains 11 fields with 55 observations. The numeric fields give the total number of households in each township, and the number of households with the corresponding technology or appliance. (e.g., television, internet connection, etc)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#deriving-new-indicator-variables",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#deriving-new-indicator-variables",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Deriving new indicator variables",
    "text": "Deriving new indicator variables\nUsing the numeric fields directly will be highly biased as it depends on the number of households in the township. (i.e., townships with higher total households are likely to have higher values for all other columns) To overcome this problem, we can derive the penetration rates (PR) of each of the items by computing the number of households with that item per 1000 households. We accomplish this using mutate() from dplyr package in the code below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nWe can use summary() again to display summary statistics on the 6 new columns.\n\nsummary(ict_derived[c(12:17)])\n\n    RADIO_PR          TV_PR         LLPHONE_PR       MPHONE_PR     \n Min.   : 21.05   Min.   :116.0   Min.   :  2.78   Min.   : 36.42  \n 1st Qu.:138.95   1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14  \n Median :210.95   Median :517.2   Median : 37.59   Median :305.27  \n Mean   :215.68   Mean   :509.5   Mean   : 51.09   Mean   :314.05  \n 3rd Qu.:268.07   3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43  \n Max.   :484.52   Max.   :842.5   Max.   :181.49   Max.   :735.43  \n  COMPUTER_PR      INTERNET_PR     \n Min.   : 3.278   Min.   :  1.041  \n 1st Qu.:11.832   1st Qu.:  8.617  \n Median :18.970   Median : 22.829  \n Mean   :24.393   Mean   : 30.644  \n 3rd Qu.:29.897   3rd Qu.: 41.281  \n Max.   :92.402   Max.   :117.985"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#joining-spatial-and-aspatial-data",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#joining-spatial-and-aspatial-data",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Joining spatial and aspatial data",
    "text": "Joining spatial and aspatial data\nFor later map preparations, we need to combine the two datasets (geospatial shan_sf, aspatial ict_derived) into a single object. We do this using the left_join() function of the dplyr package. Both datasets have a common field TS_PCODE which will be treated as the unique identifier or joining key.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")\n\nThe code includes creation of a new rds file so we can use the following code in the future to read this joined dataset without performing all the steps above.\n\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-statistical-graphics",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-statistical-graphics",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "EDA using statistical graphics",
    "text": "EDA using statistical graphics\nWe can use histograms to visualize the overall distribution of data values– e.g., the shape or skewness. The code chunk below produces on for the field RADIO_PR.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20,  color=\"black\", fill=\"light blue\") +\n  xlab(\"Radio Penetration Rate, per K-HH\") +\n  ylab(\"No. of Townships\")\n\n\n\n\n\n\n\n\nWe can also use boxplots for identifying the median, quartiles, and outliers in the data.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")+\n  xlab(\"Radio Penetration Rate, per K-HH\")\n\n\n\n\n\n\n\n\nWe can create multiple histograms side by side by creating objects for each variable’s histogram, and then laying them out in a grid with ggarange() of the ggpubr package.\n\nCreation of Histogram objectsGrid display of multiple histograms\n\n\n\nradio &lt;- ggplot(data=ict_derived, aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20,color=\"black\", fill=\"light blue\") +\n  xlab(\"Radio PR\") +\n  ylab(\"No. of Townships\")\n\ntv &lt;- ggplot(data=ict_derived, aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"TV PR\") +\n  ylab(\"No. of Townships\")\n\nllphone &lt;- ggplot(data=ict_derived, aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Landline Phone PR\") +\n  ylab(\"No. of Townships\")\n\nmphone &lt;- ggplot(data=ict_derived, aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Mobile Phone PR\") +\n  ylab(\"No. of Townships\")\n\ncomputer &lt;- ggplot(data=ict_derived, aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Computer PR\") +\n  ylab(\"No. of Townships\")\n\ninternet &lt;- ggplot(data=ict_derived, aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  xlab(\"Internet PR\") +\n  ylab(\"No. of Townships\")\n\n\n\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, nrow = 2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-choropleth-map",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#eda-using-choropleth-map",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "EDA using choropleth map",
    "text": "EDA using choropleth map\nThe code chunk below prepares a choropleth map of the Shan state and the Radio penetration rate using qtm()\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\nThe above map is based on the derived penetration rate. We can use choropleth maps to go back to the earliest statement that using the raw variables are likely to be biased on the number of households. We can use the code chunk below to look at them side by side. We use the approach of passing multiple arguments instead of using tmap_arrange()\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"TT_HOUSEHOLDS\", \"RADIO\"),\n          n = 5,style = \"jenks\", \n          title = c(\"Total households\",\"Number Radio\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe above map shows that townships with high number of households with radios, also are towns with the high number of households. We can produce a second map to see if the penetration rate and the total number of households are correlated.\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n          n = 5,style = \"jenks\", \n          title = c(\"Total households\",\"Radio Penetration\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe second pair of maps shows no strong correlation between townships having high number of households and having high radio penetration rates.\nFinally, we can show the six derived variables visually using a similar approach in the code chunk below. The viewer needs to be mindful of the data classes. While the darker the shading means a higher value for that derived variable, the range of values are different between pairs of variables.\n\ntm_shape(shan_sf) + \n  tm_fill(col = c(\"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\",\n                  \"MPHONE_PR\", \"COMPUTER_PR\", \"INTERNET_PR\"),\n          n = 5,style = \"jenks\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"top\"), bg.color = \"grey90\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-and-extracting-cluster-variables",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#selecting-and-extracting-cluster-variables",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Selecting and extracting cluster variables",
    "text": "Selecting and extracting cluster variables\nThe code chunk below will be used to extract the clustering variables from the shan_sf dataframe. We have chosen to include COMPUTER_PR rather than INTERNET_PR for the cluster analysis\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nThe next step is to change the row names or indices to the township names rather than the row numbers\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nWe see that the row numbers have been replaced with the township names, however, the township names are now duplicated. We solve this by using the code chunk below\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-standardisation",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#data-standardisation",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Data standardisation",
    "text": "Data standardisation\nMultiple variables will usually have different range of values. If we use them as is for cluster analysis, then the clustering will be biased towards variables with larger values. It is useful to standardise the clustering variables to reduce the risk of this occuring.\n\nMin-max standardisation\nThe code chunk below uses normalize() of heatmaply package to standardise the clustering variables using min-max method. We then use summary() to show that the ranges of each variable have transformed to [0,1]\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\n\n\nZ-score standardisation\nWe can perform z-score standardisation by using scale() of Base R. We use describe() of psych package to display some statistics of the standardised columns. These show that each of the variables have been transformed to have a mean of 1 and a standard deviation of 1\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\n\n\nVisualising the standardised clustering variables\nAside from viewing the statistics of the standardised variables, it is also good practice to visualise their distribution graphically.\nThe code chunk below produces histograms to show the RADIO_PR field without and with standardisation\n\nr &lt;- ggplot(data=ict_derived, aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\nAlternatively, we can view these as density plots using the code below.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-the-proximity-matrix",
    "href": "Hands-on/Hands-On_Ex09/Hands-On_Ex09.html#computing-the-proximity-matrix",
    "title": "Geographic Segmentation wwith Spatially Constrained Clustering Techniques",
    "section": "Computing the proximity matrix",
    "text": "Computing the proximity matrix\nxx"
  }
]