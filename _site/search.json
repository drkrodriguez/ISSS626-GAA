[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "",
    "text": "Welcome to my website for my work in the course ISSS 626 - Geospatial Analytics and Applications during the August term of 2024 under Dr Kam Tin Seong.\nThe course covers theory and methods of geospatial analysis and the tools in R to implement such analyses."
  },
  {
    "objectID": "index.html#hands-on-exercises",
    "href": "index.html#hands-on-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\nHands-on exercises are assigned before each lesson to give us hands-on experience in using R on the geospatial analysis methods and functions we learned. We are given step-by-step instructions and explanations (through Dr Kam’s online book) which complement our pre-class reading, and prepares us for the in-class exercises.\n\nGeospatial Data Wrangling with R (for Session 1, 26 Aug ’24)\nChoropleth Mapping with R (for Session 1, 26 Aug ’24)\n1st Order Spatial Point Patterns Analysis Methods (for Session 2, 2 Sept ’24)\n2nd Order Spatial Point Patterns Analysis Methods (for Session 2, 2 Sept ’24)\nNetwork Constrained Spatial Point Pattern Analysis (for Session 3, 9 Sept 2024)\nSpatial Weights and Applications (for Session 4, 16 Sept 2024)\nGlobal Measures of Spatial Autocorrelation (for Session 5, 23 Sept 2024)\nLocal Measures of Spatial Autocorrelation (for Session 5, 23 Sept 2024)"
  },
  {
    "objectID": "index.html#in-class-exercises",
    "href": "index.html#in-class-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\nIn each session, we go through a hands-on exercise to revise the readings for the lesson. The objective of these is to further reinforce the concepts and tools learned in the readings and in the take-home exercise. These will also go further into the analysis and interpretation of results.\n\nIntroduction to Geospatial Analytics (26 Aug ’24)\nSpatial Point Pattern Analysis - Data Load for Take-home Exercise 1 (2 Sept ’24)\nAdvanced Spatial Point Patterns Analysis (9 Sept ’24)"
  },
  {
    "objectID": "index.html#take-home-exercises",
    "href": "index.html#take-home-exercises",
    "title": "Geospatial Analytics and Applications (in R)",
    "section": "Take-Home Exercises",
    "text": "Take-Home Exercises\nTake-home exercises are where we students are able to apply the methods and techniques learned in class in real-world cases– using real-world data and aim to generate real insights.\nFor each of these, we are given a specific problem, objectives and a base data set. Each student will work on the problem independently, guided only by the previous exercises and lessons.\n\nGeospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "",
    "text": "In this exercise, we apply spatial point pattern analysis to analyse the distribution of road traffic accidents in the Bangkok Metropolitan Region. We demonstrate how kernel density estimation can be used to visualize hotspots in a network constrained and non-network constrained context. We use different approaches to display charts side by side, especially to compare hotspots across different time dimensions or different conditions. Finally, we demonstrate how K- and G-functionss can be used to support any claims on the randomness, clustering or dispersion of a spatial point distribution."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.1-background",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.1-background",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.1 Background",
    "text": "A.1 Background\nRoad traffic accidents account for 1.19million deaths and up to 50 million non-fatal injuries according to a report by the WHO last year.\nThe same report identifies major risk groups: low- and middle-income countries, (esp in Africa and Europe) the working population, and males. It also identifies some key risk factors which include human error, speeding, driving under the influence of alcohol, distracted driving, unsafe road infrastructure, unsafe vehicles, and law enforcement. Most of the factors identified are behavioral in nature but do not discount that other factors may also contribute to a higher risk of occurrence.\nWithin Southeast Asia, Thailand has ranked the highest in terms of incidence of road traffic accidents with an average number of of 20,000 deaths a year or 56 a day. The country has also seen an increase in the number of accidents from 2014 to 2021. A large 19% of these accidents occurred in national highways, and the chances of encountering an accident-prone zone was found to be 66%."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.2-objectives",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.2-objectives",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.2 Objectives",
    "text": "A.2 Objectives\nThis study aims to take a deeper look into the road accidents in Thailand, focusing on the Bangkok Metropolitan Region (BMR) which contains the capital Bangkok, and five neighboring provinces. (Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakan, Samut Sakhon)\nAs most literature has focused on behavioral and environmental factors, the study will focus on identifying spatiotemporal factors influencing the occurrence of road accidents in BMR. At the minimum, the study deliverables include the following:\n\nVisualization of spatiotemporal dynamics of road traffic accidents in BMR\nDetailed spatial analysis of road traffic accidents in BMR\nDetailed spatiotemporal analysis of road traffic accidents in BMR\n\nThe appropriate technique must be used for these deliverables and all the analysis and visualizations will be carried out using R."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.3-data-sources",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.3-data-sources",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.3 Data Sources",
    "text": "A.3 Data Sources\nThe study makes use of the following datasets which are publicly available online.\n\n\n\nDataset Short Name\nDescription\nDatasource\n\n\n\n\nTHRA\nThailand road accident data from 2019 to 2022\nKaggle\n\n\nTHOSM\nThailand roads open street map in shapefile format\nHDX\n\n\nTHSAB\nThailand - Subnational Administrative Boudaries shapefile\nHDX"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.4-importing-and-launching-r-packages",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#a.4-importing-and-launching-r-packages",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "A.4 Importing and Launching R Packages",
    "text": "A.4 Importing and Launching R Packages\nFor this study, four R packages will be used. A description of the packages and the code, using p_load() of the pacman package, to import them is given below.\n\nPackage DescriptionImport Code\n\n\nThe loaded packages include:\n\nsf - package for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - package with functions for plotting cartographic quality maps\nsPNetwork - provides functions for performing SPPA methods like KDE and K-function on a network. The package can also be used to build spatial matrices to conduct traditional spatial analyses with spatial weights based on reticular distances\nspatstat - package for plotting, EDA and simulation of spatial data\n\n\n\n\npacman::p_load(sf, spNetwork, tmap, tidyverse, spatstat)\n\n\n\n\nAs we will be performing simulations in the analysis later, it is good practice to define a random seed to be used so that results are consistent for viewers of this report, and the results can be reproduced.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.1-thailand-subnational-administrative-boundary-shapefile",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.1-thailand-subnational-administrative-boundary-shapefile",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.1 Thailand Subnational Administrative Boundary, Shapefile",
    "text": "B.1 Thailand Subnational Administrative Boundary, Shapefile\nWe load the Thailand subnational administrative boundary shapefile into an R dataframe using st_read() from the sf package. The source provides the geospatial data in varying levels as indicated by their suffix: country (0), province (1), district (2), and sub-district. (3) For focusing on the BMR, which covers Bangkok and neighboring provinces, province is the most likely level of detail we will need so we will use the code chunk below to load the appropriate layer first.\n\nthsab_prov &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nWe examine the loaded data to confirm the load has been done properly and to get some initial observations of the data.\n\nCalling ObjectChecking crs information with st_crs()\n\n\n\nthsab_prov\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   Shape_Leng Shape_Area                  ADM1_EN       ADM1_TH ADM1_PCODE\n1    2.417227 0.13133873                  Bangkok  กรุงเทพมหานคร       TH10\n2    1.695100 0.07926199             Samut Prakan    สมุทรปราการ       TH11\n3    1.251111 0.05323766               Nonthaburi         นนทบุรี       TH12\n4    1.884945 0.12698345             Pathum Thani        ปทุมธานี       TH13\n5    3.041716 0.21393797 Phra Nakhon Si Ayutthaya พระนครศรีอยุธยา       TH14\n6    1.739908 0.07920961                Ang Thong        อ่างทอง       TH15\n7    5.693342 0.54578838                 Lop Buri          ลพบุรี       TH16\n8    1.778326 0.06872655                Sing Buri         สิงห์บุรี       TH17\n9    2.896316 0.20907828                 Chai Nat         ชัยนาท       TH18\n10   4.766446 0.29208711                 Saraburi         สระบุรี       TH19\n   ADM1_REF ADM1ALT1EN ADM1ALT2EN ADM1ALT1TH ADM1ALT2TH  ADM0_EN   ADM0_TH\n1      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n2      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n3      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n4      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n5      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n6      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n7      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n8      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n9      &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n10     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; Thailand ประเทศไทย\n   ADM0_PCODE       date    validOn    validTo                       geometry\n1          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.6139 13...\n2          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.7306 13...\n3          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3415 14...\n4          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.8916 14...\n5          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.5131 14...\n6          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3332 14...\n7          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((101.3453 15...\n8          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.3691 15...\n9          TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((100.1199 15...\n10         TH 2019-02-18 2022-01-22 -001-11-30 MULTIPOLYGON (((101.3994 15...\n\n\n\n\n\nst_crs(thsab_prov)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\nThe output confirms that we have a multipolygon sf object with 77 rows and 17 columns. There is a column named ADM1_EN which appears to contain the province names needed to define the BMR boundaries. It also shows that the dataset is using a coordinate reference system rather than a projected reference system.\nFirst, we reload the data to use a projected reference system and apply the correct reference system with EPSG code of 32647 using st_transform(). This transformation can be confirmed with st_crs() The tmap package is then used to visualize the object to see if it properly depicts the boundaries of Thailand and its provinces.\n\nLoad Object and Transform CRS informationChecking crs information with st_crs()Plot of thsab_prov using tmap\n\n\n\nthsab_prov &lt;- st_read(dsn=\"data/geospatial\",\n                          layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\nst_crs(thsab_prov)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n\ntm_shape(thsab_prov) +\n  tm_polygons(\"grey\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.2-filtering-thsab-for-the-bangkok-metropolitan-region",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.2-filtering-thsab-for-the-bangkok-metropolitan-region",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.2 Filtering THSAB for the Bangkok Metropolitan Region",
    "text": "B.2 Filtering THSAB for the Bangkok Metropolitan Region\nBefore further analyzing the data, we will limit the scope to only consider the Bangkok Metropolitan Region or BMR. This would encompass Bangkok, Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakan, Samut Sakhon. While it is good to get insights outside of BMR, it is out of the study scope and it is best to focus on the objectives.\nThe code chunk below checks if all the provinces in the BMR appear as is under the ADM1_EN column of thsab_prov\n\nfilter(thsab_prov, ADM1_EN %in% c(\"Bangkok\", \"Nonthaburi\",\"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\"))$ADM1_EN\n\n[1] \"Bangkok\"       \"Samut Prakan\"  \"Nonthaburi\"    \"Pathum Thani\" \n[5] \"Nakhon Pathom\" \"Samut Sakhon\" \n\n\nWith the previous code returning all 6 provinces, we have confirmation that the provinces are all present and spelled as is in the data source. We create a new object bmr_boundary to contain only the provinces in BMR. We also take this opportunity to only keep the relevant columns in the dataset using the select() function of dplyr package.\n\nCreate BMR boundary object using filter()Plot of bmr_boundary using tmap\n\n\n\nbmr_boundary &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647) %&gt;%\n  filter(ADM1_EN %in% c(\"Bangkok\", \"Nonthaburi\",\"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\")) %&gt;% dplyr::select(Shape_Leng, Shape_Area, ADM1_EN, geometry)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Take-home\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\n\n\ntm_shape(bmr_boundary) +\n  tm_polygons(\"grey\")\n\n\n\n\n\n\n\n\n\n\n\nThe code below creates a second object which is just a union of all the provinces. (i.e., borders between provinces are lost) This is done using the st_union() function.\n\nbmr_full = st_union(bmr_boundary)\n\n\ntm_shape(bmr_full) +\n  tm_polygons(\"grey\")\n\n\n\n\n\n\n\n\nThe code below keeps the final boundary objects into files to make loading more convenient for later analyses.\n\nwrite_rds(bmr_boundary, \"data/rds/bmr_boundary.rds\")\nwrite_rds(bmr_full, \"data/rds/bmr_full.rds\")\n\nThe code below then reloads the same objects into R:\n\nbmr_boundary = read_rds(\"data/rds/bmr_boundary.rds\")\nbmr_full = read_rds(\"data/rds/bmr_full.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.3-road-accident-data-aspatial-csv-file",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.3-road-accident-data-aspatial-csv-file",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.3 Road Accident Data, Aspatial, csv-file",
    "text": "B.3 Road Accident Data, Aspatial, csv-file\nThe road accident data is contained in a csv file. We use the code block in the first tab below to load it into the thra object with some necessary transformations that we identified upon inspecting the raw file. The second tab gives an explanation of the different nested functions used in the code\n\nCode to import and transform road accident dataExplanation of the code lines / functions used\n\n\n\nbmracc &lt;- read_csv(\"data/aspatial/thai_road_accident_2019_2022.csv\")  %&gt;%\n  filter(!is.na(longitude) & longitude != \"\", \n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;%\n  st_transform(crs = 32647)\n\nbmracc &lt;- filter(bmracc, geometry %in% st_intersection(bmr_full, bmracc)) %&gt;%\n  mutate(Year = year(incident_datetime)) %&gt;%\n  mutate(MonthNum = month(incident_datetime)) %&gt;%\n  mutate(Month = month(incident_datetime, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(DayOfWeek = wday(incident_datetime, label = TRUE, abbr = TRUE))\n\n\n\n\nread_csv() used to import a csv file into an R object\nfilter(!is.na(longitude) & longitude != \"\", !is.na(latitude) & latitude != \"\") used to exclude any records where the longitude or latitude information is missing\nst_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) used to convert the dataframe into an sf object using a reference system (WGS84) based on the coordinates\nst_transform(crs = 32647) used to apply the correct EPSG code to the sf object\nfilter(bmracc, geometry %in% st_intersection(bmr_full, bmracc)) used to leave only records which fall within the BMR boundaries\nmutate(...) these lines are used to add additional columns to quickly reference the year, month and day of the week that each accident occured as these dimensions allow for some temporal analyses\n\n\n\n\nCalling the new object shows that it has 12,989 rows across 20 fields.\n\nbmracc\n\nWe use the code chunks below to check the data and visualize the data on the BMR boundary map.\n\ntm_shape(bmr_boundary) +\n  tm_polygons(col = \"grey\") +\n  tm_shape(bmracc) +\n  tm_dots(col = \"red\", size = 0.01, alpha = 0.5)\n\n\n\n\n\n\n\n\nThe code chunk below writes the resulting accident dataset into an rds file for convenient loading.\n\nwrite_rds(bmracc, \"data/rds/bmracc.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.4-thailand-roads-open-streetmap-shapefile",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.4-thailand-roads-open-streetmap-shapefile",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.4 Thailand Roads Open StreetMap, Shapefile",
    "text": "B.4 Thailand Roads Open StreetMap, Shapefile\nThe second geospatial object is the street map shapefile. We will use the object network to contain the final road network for the study.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"hotosm_tha_roads_lines_shp\")\n\nRunning the above code confirms that the dataset is in multilinestring sf format and that it contains 2.8M records across 15 variables. It also shows that there is no CRS applied to the dataset.\nBased on these, the following steps need to be done: apply the right CRS/EPSG code of 32647 or the same as bmr_full, and, filter the network to only include BMR.\nThe code below does the first step of applying a reference system and updating the EPSG code to 32647 using st_set_crs() and st_crs() from the sf package.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\",\n                      layer=\"hotosm_tha_roads_lines_shp\") %&gt;%\n  st_make_valid() %&gt;% st_set_crs(4326) %&gt;% st_transform(crs = st_crs(bmr_full))\n\nThe code below then finds the network within BMR by using st_intersection() to find the overlap between the full road network and the BMR boundary. We also include write_rds() in the chunk to store this object into an rds file for easy future loading.\n\nnetwork &lt;- st_intersection(network, bmr_full)\nwrite_rds(network, \"data/rds/network.rds\")\n\nCalling the object name allows us to inspect the contents.\n\nnetwork\n\nThe size of the object has now been reduced to 585K features from the original 2.8M. This still appears a very large number if we want to visualize the data, so we need to inspect if there are any opportunities to reduce the dataset by excluding any irrelevant records.\nThe data includes a column named highway which gives information on the the type or classification of the road.\n\nggplot(network, aes(x = reorder(highway, table(highway)[highway]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Roads by Highway type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"Highway Type\", y = \"Number of Roads\")\n\nThe resulting chart shows that roads with highway type of residential and service make up 522K of the 585K roads in the dataset. We refer to the OpenStreetMap wiki page to see the definition of the different types of highways in the Thailand map and see that these two highway types are access roads for residences or specific buildings. For our objective, we should be able to limit to roads where accidents (are expected to frequently) happen, and only to roads that should be accessible by vehicles. Going through the definition of the highway types, we see that the following 6 types could be out of scope for our study:\n\nresidential - road within a residential area that gives public access to one or multiple residences\nservice - minor road that gives access to buildings or places outside a residential area (e.g., to a religious site, an attraction, part of an estate)\nfootway - pathways designed for pedestrian access\ntrack - road whose only function is to provide access to surrounding land, and is most of the time unpaved\npath - multi-purpose path intended for non-motor vehicles\nsteps\n\nWe can then use the following code chunk which uses the filter() function to remove these classifications from the current network object. We call the object name in the succeeding code chunk to check the new dataset.\n\nbmr_network &lt;- bmr_network %&gt;% \n  filter(!(highway %in% c(\"residential\", \"service\", \"footway\", \"track\", \"path\", \"steps\")))\n\n\nbmr_network\n\nWhile this looks good, it looks like the object is being identified as a GEOMETRY rather than a LINESTRING object. We can use the code below to correct it.\n\nbmr_network &lt;- st_cast(bmr_network, \"LINESTRING\")\n\n\nbmr_network\n\nThe new road network object is now reduced to 34K records or roads which is a 94% reduction in the number of records. We will use some visual inspection to see if this reduction in records will affect our analysis. The two code chunks below plot the road network within the boundaries, while the second plots the three objects together. We use the tmap function to create these maps.\n\nBMR filtered road network onlyBMR filtered road network with road accident dataset\n\n\n\ntm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\") +\n  tm_shape(bmracc) +\n  tm_dots(col = \"red\", alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nFrom these two maps, we see that:\n\nwhile we have filtered 90% of the original records, the resulting map still appears dense, especially in some central areas; and,\nthe road accident locations appear to fall along the network\n\nBased on these, we will go ahead with this version of the network for our analysis.\nThe following code writes the resulting network into an rds file for more convenient loading in the future.\n\nwrite_rds(bmr_network, \"data/rds/bmr_network.rds\")"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.5-resolving-duplicate-points",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#b.5-resolving-duplicate-points",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "B.5 Resolving duplicate points",
    "text": "B.5 Resolving duplicate points\nIn this section, we perform some additional transformations to perform the required analyses.\nFirst, we check the event or accident dataset to see if there are any duplicated data points or locations as the methods require that the points are unique. The code below checks if any duplicate points exist. Note that we specify the column in the argument as we are double-checking duplicate locations rather than completely duplicate records.\n\nany(duplicated(bmracc$geometry))\n\n[1] TRUE\n\n\nAs the code returned TRUE, it confirms the presence of duplicate points, we use st_jitter() from the sf package to introduce some jitter to each point and ensure that points do not lie on the same location. Without any additional arguments, the function uses a default factor 0.002 of the bounding box diagonal as the bounds for the amount of jitter introduced. In the code below, we define an amount of 0.01 instead.\n\nbmracc_jitt &lt;- st_jitter(bmracc, 0.01)\n\nRerunning the check using duplicated() shows that there are no duplicate points anymore.\n\nany(duplicated(bmracc_jitt$geometry))\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#c.1-categories-of-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#c.1-categories-of-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "C.1 Categories of accidents",
    "text": "C.1 Categories of accidents\nWe first would like to understand the different labels we can use from the BMR road accident dataset bmracc The following columns appear to be able to give some insight about the nature of the accident:\n\nvehicle_type\npresumed_cause\naccident_type\nnumber_of_vehicles_involved\nnumber_of_fatalities\nweather_condition\n\nWe will try to be brief in analysing these variables as the main intent is to understand which ones will add the most value to the spatial analysis needed to address the main study objectives.\nNote that while we use bmracc rather than the modified bmracc_jitt in the codes below, the result will be the same as we do not concern ourselves with the geometry innformation yet.\n\nC.1.1 Vehicle Type\nThis variable is intended to give the type of vehicle involved in the accident. We use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(vehicle_type, table(vehicle_type)[vehicle_type]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Vehicle type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nThe output shows that out of 12,989 accidents, 35% are with private or passenger cars, 27% are with pickup trucks and 13% are with motorcycles. These three make up 75% of all the recorded accidents while the remaining 11 types make up the balance 25%\n\n\nC.1.2 Presumed Cause\nThis variable is intended to give the presumed cause of the accident. We again use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(presumed_cause, table(presumed_cause)[presumed_cause]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Presumed Cause\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that 10,146 or 75% of the accidents are presumed to be caused by speeding. The next largest named presumed cause only accounts for 5% of the overall data.\n\n\nC.1.3 Accident Type\nThis variable is intended to give the type or nature of the accident. We again use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(accident_type, table(accident_type)[accident_type]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Type\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that “rear-end collisions” and “rollover/fallen on straight road” are the leading causes recorded and account for 83% of the accidents.\n\n\nC.1.4 Number of Fatalities\nThis variable is intended to give the number of fatalities resulting from the accident. We again use the code block below to understand the categories under this variable using a simple histogram created through ggplot().\n\nggplot(bmracc, aes(x = number_of_fatalities)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"darkgrey\") +\n  scale_x_continuous(breaks = seq(min(bmracc$number_of_fatalities), max(bmracc$number_of_fatalities), by = 1)) +\n  labs(title = \"Accidents by Number of Fatalities\", x = \"Number of Fatalities\", y = \"Number of Accidents\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.5)\n\n\n\n\n\n\n\n\nThe plot shows that 94% of the recorded accidents are non-fatal. Only 719 were fatal. Although this is a small number, it might be worth looking at the location of such fatal accidents later. We can use the code below to introduce a new column fatal into the data for more convenient filtering later.\n\nbmracc$fatal &lt;- bmracc$number_of_fatalities &gt; 0\n\n\n\nC.1.5 Weather Condition\nThis variable is intended to indicate the weather condition when the accident was recorded. We first use the code block below to understand the categories under this variable using a simple bar chart created through ggplot().\n\nggplot(bmracc, aes(x = reorder(weather_condition, table(weather_condition)[weather_condition]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Weather Condition\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that 90% of the accidents occurred during “clear” weather. Online sources suggest that Bangkok experieces a long rainy season and has 153 rainy days per year, so 10% for the occurrence of accidents appears low. The sources also say that the wettest month is September.\nWe can use the code chunk below to plot the number of accidents that were not recorded on clear weather (i.e., rainy) by month.\n\nggplot(filter(bmracc, !weather_condition == \"clear\"), aes(x = reorder(Month, table(Month)[Month]))) +\n  geom_bar() +\n  coord_flip() +\n  ggtitle(\"Number of Accidents by Month during Non-clear Weather\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), hjust=-0.3) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe plot does align with the expectation that September is the wettest month. The very low number of accidents during rainy weather is still questionable though so we will watch out for this if we will use this variable later."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.1.-converting-objects-into-spatstats-formats",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.1.-converting-objects-into-spatstats-formats",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.1. Converting Objects into spatstat’s formats",
    "text": "D.1. Converting Objects into spatstat’s formats\nThe events need to be converted into spatstat’s ppp object using as.ppp()\n\nbmracc_ppp &lt;- as.ppp(st_geometry(bmracc_jitt))\n\nWe then prepare an owin object to define the boundaries using the as.owin() function.\n\nbmr_owin &lt;- as.owin(bmr_full)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.2-kde-for-all-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.2-kde-for-all-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.2 KDE for all Accidents",
    "text": "D.2 KDE for all Accidents\nWe first combine the accidents (all years, all types) into the owin using the following code chunk\n\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n\nWe can then compute for the kde using the density() function. We use the four common methods for automatic bandwidth selection and examine them in a grid using the code chunk below.\n\npar(mfrow=c(2,2))\nplot(density(bmracc_for_kde, sigma=bw.diggle,\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Diggle Method\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Scott's Rule\")\nplot(density(bmracc_for_kde, sigma=bw.CvL(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Cronie & Van Lieshout Criterion\")\nplot(density(bmracc_for_kde, sigma=bw.ppl(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = \"KDE using Likelihood Cross Validation\")\n\n\n\n\n\n\n\n\nAmong the four methods, it looks like Scott’s rule (using sigma=bw.scott()) is identifying hot spots unlike the others. There is a hotspot in the southwest and southeast of Bangkok. There also appears to be a high density strip (or maybe a major highway) stretching up northwards.\nWe will use this bandwidth selection method for our succeeding analysis.\n\nD.3 KDE for accidents across years\nWe then want to see if the hotspots move across the years. To do this, we effectively need to compute for the kde across years and see if there are any visible signs of shifts in the hotspots.\nFirst, let us try to understand the distribution of accidents by year. This will help us understand if the numberical values of the density will move because of change in the absolute number of accidents.\nWe use ggplot() in the code chunk below to achieve this.\n\nggplot(bmracc, aes(x = reorder(Year, table(Year)[Year]))) +\n  geom_bar() +\n  ggtitle(\"Number of Accidents by Year\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), vjust = +2) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 12),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe chart shows that there were significantly more accidents in 2022 compared to the previous years. The other years are within 10% of each other. We should expect that 2022 KDE may have higher numerical values compared to the other years.\nWe can then use the following code to generate four different kde’s, one for each year.\n\npar(mfrow=c(2,2))\nfor (i in c(2019, 2020, 2021, 2022)) {\n  bmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, Year == i)))\n  bmracc_for_kde = bmracc_ppp[bmr_owin]\n  bmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n  plot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = paste(\"KDE for year\",i))\n}\n\n\n\n\n\n\n\n\nWe see that there appears to be a shift between 2020 and 2021. Before 2020, there appeared to be two separate promininet hotspots for accidents– in the central and southeastern portion of the region. However, after 2020, it seems that the accidents are more frequent in the southeastern part, and in a much wider area."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.3-kde-across-months",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.3-kde-across-months",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.3 KDE across Months",
    "text": "D.3 KDE across Months\nWe can apply a similar approach of analysing by month using the code chunk below. For now, we are aggregating accidents by month across all years, so the insights will apply to the whole period and not any particular year.\n\npar(mfrow=c(3,4))\nfor (i in 1:12) {\n  bmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, MonthNum == i)))\n  bmracc_for_kde = bmracc_ppp[bmr_owin]\n  bmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\n  plot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n                      edge=TRUE,kernel=\"gaussian\"), main = paste(\"KDE for month\",i))\n}\n\n\n\n\n\n\n\n\nThe output reveals no drastic shift in hotspots (using kde) across months. There are months where the intensities and relative intensities differ, but it appears like the hotspots remain in the same areas."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.4-kde-for-clear-vs-rainy-days",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#d.4-kde-for-clear-vs-rainy-days",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "D.4 KDE for Clear vs “Rainy” Days",
    "text": "D.4 KDE for Clear vs “Rainy” Days\nThe final analysis we want to perform before moving to network-constrained analysis is on clear vs non-clear days. This is indicated in the field called weather_condition in the accident dataset.\nTo produce the kde visualization, we can use the code chunk below.\n\npar(mfrow=c(1,2))\n\nbmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, weather_condition == \"clear\")))\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n             edge=TRUE,kernel=\"gaussian\"), main = \"KDE for Clear Days\")\n\nbmracc_ppp &lt;- as.ppp(st_geometry(filter(bmracc_jitt, !(weather_condition == \"clear\"))))\nbmracc_for_kde = bmracc_ppp[bmr_owin]\nbmracc_for_kde &lt;- rescale.ppp(bmracc_for_kde, 1000, \"km\")\nplot(density(bmracc_for_kde, sigma=bw.scott(X = bmracc_for_kde),\n             edge=TRUE,kernel=\"gaussian\"), main = \"KDE for Rainy Days\")\n\n\n\n\n\n\n\n\nThe output are very similar too some of the charts generated earlier. The hotspot in the center of BMR appears to dissipate during rainy days. (relative to the one in the southeast portion of the region."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.1-preparation-of-data-for-network-constrained-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.1-preparation-of-data-for-network-constrained-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.1 Preparation of Data for Network Constrained Analysis",
    "text": "E.1 Preparation of Data for Network Constrained Analysis\nBefore we are able to perform network-constrained NKDE, we first need to define sample points along the road network, and to do that, we can use the midpoint of the lixels of the network.\n\nE.1.1 Preparing the lixels\nTo lixelize a network, the minimum and (maximum) length of lixels need to be defined. A logical distance needs to be chosen for a given study. In our case, we might no have enough information to understand what road segment length is relevant to group accidents into. However, we can start with understanding the road lengths in the network.\nWe can use the code block below to show the distribution of the road length values using summary() to give the quartiles, and quantile() to give a wider range of view.\n\nsummary(st_length(bmr_network))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n    0.143    31.581   124.691   413.002   433.312 24745.584 \n\nquantile(st_length(bmr_network), probs = seq(.1, .9, by = .1))\n\nUnits: [m]\n       10%        20%        30%        40%        50%        60%        70% \n  13.78247   24.36581   41.45909   71.64269  124.69070  207.11305  337.56988 \n       80%        90% \n 556.85935 1074.26696 \n\n\nThe output shows that there is a very wide range of values. There is also a surprisingly large number of roads (&gt;40%) that are less than 100m– which seem to be too short for typical roads. We can first choose a min distance of 200m which would allow for at least 40% of roads to not be split. As for the maximum length, let us first set it to 600m so only a little over 20% of the roads will be split into smaller segments.\nWe implement this using lixelize_lines() in the code chunk below.\n\nlixels &lt;- lixelize_lines(bmr_network$geometry, \n                         600, \n                         mindist = 200)\n\n\n\nE.1.2 Generating sample points\nThe next step is to define sample points along the network which will be the points where the KDE function will be computed on.\nWe can create sample points on the lixel centers using lines_center() in the code below.\n\nsamples &lt;- lines_center(lixels)"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.2-all-years-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.2-all-years-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.2 All Years, All Accidents, Initial Analysis",
    "text": "E.2 All Years, All Accidents, Initial Analysis\nLet’s start by looking at the highest levels. We can compute for the nkde for all accidents in the dataset (2019-2022) using the code chunk below. This uses the accidents with the jitter applied.This code uses a bandwidth of 300m, anduses “quartic” for the kernel function, and uses simple calculation method for the KDE\n\ndensities &lt;- nkde(bmr_network, \n                  events = bmracc_jitt,\n                  w = rep(1, nrow(bmracc_jitt)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe import the densities into the lixel and sample object using the code below. We use a multiple of 1000 to convert the figures from accidents per square meter to accidents per square kilometer\n\nsamples$density_all &lt;- densities*1000000\nlixels$density_all &lt;- densities*1000000\n\nWe use the code below to produce a map with just the calculated densities.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe resulting map surprisingly does not show a lot of high density road segments, which is not aligned to the earlier map with the locations of the accidents. We can check if the jitter has caused displacement of the locations and hidden high density road segments by rerunning the below code chunk which uses the accident locations without jitter applied.\n\ndensities &lt;- nkde(bmr_network, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nTo see if using the original event points will address our concern, we repeat the code chunk below to create a static map.\n\nsamples$density_all &lt;- densities*1000000\nlixels$density_all &lt;- densities*1000000\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nIt looks like the use of the original accident data does little to reveal dense locations. If we examine the interactive map and zoom in, we see one possible reason for the problem. Major roads are being split into multiple semi-parallel roads. These might denote different directions on the same highway, service roads, etc. These might cause accidents on the same “parent” road to be split across their parts.\nWe try to solve this problem by recreating our network while merging such roads into one."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.3-transformation-step---merging-of-parallel-roads",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.3-transformation-step---merging-of-parallel-roads",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.3 Transformation step - merging of parallel roads",
    "text": "E.3 Transformation step - merging of parallel roads\nOne way to merge the roads, is to first use st_buffer() to create a width dimension on the roads. We would prefer to use the actual width of the roads, plus the islands here, but there is no way to do this accurately and this would vary from road to road. (e.g., some roads could have one lane, while others could have four or more lanes) For our case, we will use a width of 2 x 15m, which is based on a ~3m lane width estimate, which means we are buffering up to the width of five lanes or five cars on each side.\nWe use the code chunk below to produce a buffered network.\n\nbmr_network_buffered &lt;- st_buffer(bmr_network, dist = 15)\nbmr_network_dissolved &lt;- st_union(bmr_network_buffered)\n\nThe next step is to convert or cast this into a linestring object, but before that we would want to make sure that the geometries are simple enough so the casting is executed properly. To do this, we fist use st_simplify() which simplifies objects by reducing vertices.\n\nbmr_network_simplified &lt;- st_simplify(bmr_network_dissolved, dTolerance = 1)\n\nWith the network simplified, we can then use st_cast() to convert the geometries back into linestrings. Note that we use two calls since we cannot cast polygons directly into linestrings.\n\nbmr_network_v2 &lt;- st_cast(bmr_network_simplified, \"MULTILINESTRING\")\nbmr_network_v2 &lt;- st_cast(bmr_network_v2, \"LINESTRING\")\n\nNote that the resulting object was a list rather than a dataframe, we can use st_as_sf() to ensure that it is in an sf dataframe format.\n\nbmr_network_v2 &lt;- st_as_sf(bmr_network_v2)\n\nWe can examine the original and new network side by side using tmap_arrange() in the code below to see that the new network has worked sufficiently.\n\norig_network &lt;- tm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network) +\n  tm_lines(col = \"black\") +\n  tm_layout(title = \"Original Road Network\")\n\nnew_network &lt;- tm_shape(bmr_full) +\n  tm_polygons(col = \"lightgrey\") +\n  tm_shape(bmr_network_v2) +\n  tm_lines(col = \"black\")+\n  tm_layout(title = \"Simplified Road Network\")\n\n\ntmap_arrange(orig_network, new_network, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nThe transformation seems to have worked, and we see by counting the objects using length() st_geometry() in the code below, that the number of roads has been reduced dramatically from 34K to 4.2K– even with the very similar high level map.\n\nlength(st_geometry(bmr_network))\n\n[1] 34056\n\nlength(st_geometry(bmr_network_v2))\n\n[1] 4162\n\n\nLet us examine the road lengths in the updated network using summary() and quantile() in the code chunk below.\n\nsummary(st_length(bmr_network_v2))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n      4.0     326.1    1935.9    4529.2    5826.1 1263640.1 \n\nquantile(st_length(bmr_network_v2), probs = seq(.1, .9, by = .1))\n\nUnits: [m]\n       10%        20%        30%        40%        50%        60%        70% \n  111.8646   236.2789   474.0794  1023.5185  1935.9245  3231.7745  4846.3553 \n       80%        90% \n 7074.5520 10733.3337 \n\n\nThe simplified network now has longer road segments with the median being close to 2km in length.\nWe can then repeat the preparation of data from the creation of the lixels to the creation of the sample points. we will use exactly a similar code using lixelize_lines() as in the earlier sections. Given the distribution of the lengths, we decide to use longer lixel lengths with this new network. We choose 1km and 2km for the parameters.\n\nlixels_v2 &lt;- lixelize_lines(st_geometry(bmr_network_v2), \n                         2000, \n                         mindist = 1000)\nsamples_v2 &lt;- lines_center(lixels_v2) \n\nBefore we move, let us remove the intermediate objects from memory using rm()\n\nrm(bmr_network_buffered)\n\nWarning in rm(bmr_network_buffered): object 'bmr_network_buffered' not found\n\nrm(bmr_network_dissolved)\n\nWarning in rm(bmr_network_dissolved): object 'bmr_network_dissolved' not found\n\nrm(bmr_network_simplified)\n\nWarning in rm(bmr_network_simplified): object 'bmr_network_simplified' not\nfound"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-all-years-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-all-years-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.4 All Years, All Accidents, Initial Analysis",
    "text": "E.4 All Years, All Accidents, Initial Analysis\nWe now rerun the highest level KDE with the updated network to see if we are getting more insightful output.\nWe rerun nkde() to compute the network constrained KDE on the new network and using the new sample points.\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe again transfer these densities into the lixels and sample dataframes using the code chunk below\n\nsamples_v2$density_all &lt;- densities*1000000\nlixels_v2$density_all &lt;- densities*1000000\n\nNext, we can create a static map to show the computed KDEs visually using tmap package in the code chunk below.\n\ntm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\")\n\n\n\n\n\n\n\n\nWIP"
  },
  {
    "objectID": "In-class/In-class_Ex01/data/geospatial/MPSZ-2019.html",
    "href": "In-class/In-class_Ex01/data/geospatial/MPSZ-2019.html",
    "title": "Geospatial Analytics w/ Derek",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-distribution-of-all-accidents-initial-analysis",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.4-distribution-of-all-accidents-initial-analysis",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.4 Distribution of All Accidents, Initial Analysis",
    "text": "E.4 Distribution of All Accidents, Initial Analysis\nWe now rerun the highest level KDE with the updated network to see if we are getting more insightful output.\nWe rerun nkde() to compute the network constrained KDE on the new network and using the new sample points.\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmracc,\n                  w = rep(1, nrow(bmracc)),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe again transfer these densities into the lixels and sample dataframes using the code chunk below\n\nsamples_v2$density_all &lt;- densities*1000000\nlixels_v2$density_all &lt;- densities*1000000\n\nNext, we can create a static map to show the computed KDEs visually using tmap package in the code chunk below.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nWhile the network is simplified, it looks like we still get a very homoegenous, yellow chart. This might be due to the bottom-most bin including zeros. We can inspect the number of zeros and the range of the nonzero kde’s using summary() in the code chunk below.\n\nprint(\"Distribution of all Densities\")\n\n[1] \"Distribution of all Densities\"\n\nsummary(lixels_v2$density_all)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   1.623   0.000 214.461 \n\nprint(\"Distribution of non-zero Densities\")\n\n[1] \"Distribution of non-zero Densities\"\n\nsummary(filter(lixels_v2, density_all &gt; 0)$density_all)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n  0.00004   2.83624   4.71730  11.10733  11.22094 214.46127 \n\n\nIt looks like non-zero vlues are sparse. Less than 25% of the lixles have non-zero values. We can either use a custom palette or add a layer to grey out the zero denisty lixels. We use the latter in the code chunk below. We also add some additional elements like the provinces, and modify the formatting, in order to make the chart more information-rich and readable.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(lixels_v2)+\n  tm_lines(col=\"density_all\", palette = \"-inferno\", lwd = 2, title.col = \"Per sq km\") +\n  tm_shape(filter(lixels_v2, density_all == 0)) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = \"BMR Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe plot now shows clear road segments where there is higher density. It reveals the most dense segments lie within Bangkok. Pathum Thani and Samut Sakhon also show some high density segments. Meanwhile, Nakohn Pathom appears to have the least accident dense road segments."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-year",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-year",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.5 Distribution of Accidents by Year",
    "text": "E.5 Distribution of Accidents by Year\nWe then look into the distribution of accidents across years to see if there is a change or shift that has occured. To do this, we need to generate the nkde for each year using the code below. The code writes a new column for the lixels and the samples dataframes for each year’s KDE values.\n\nfor (i in 2019:2022) {\n   densities &lt;- nkde(bmr_network_v2, \n                  events = filter(bmracc, Year == i),\n                  w = rep(1, nrow(filter(bmracc, Year == i))),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n  lixels_v2[[paste(\"density_\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n  samples_v2[[paste(\"density_\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n  }\n\nWe can then use the code block below to generate the map of the four different years using tmap package.\n\ncolumns_to_map &lt;- c(\"density_2019\", \"density_2020\", \"density_2021\", \"density_2022\")\nyearly = list()\nfor (col in columns_to_map)\n{\n  yearly[[col]] &lt;- tm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=col, palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_shape(lixels_v2[lixels_v2[[col]] == 0, ]) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = paste(\"Year -\",substr(col,9,12)),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n}\n\n\ntmap_arrange(yearly[[1]], yearly[[2]], yearly[[3]], yearly[[4]], ncol = 2)\n\n\n\n\n\n\n\n\nThe output shows no significant change in the location of the hotspots across years. Before we look at another dimension, let us try to test for complete spatial randomness on the most recent year."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-test-for-csr---2022-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-test-for-csr---2022-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.6 Test for CSR - 2022 Accidents",
    "text": "E.6 Test for CSR - 2022 Accidents\nIt clearly looks like accidents are not randomly or homogeneously distributed in the network. We can verify this using tests for CSR (Complete Spatial Randomness) using the K- or G-functions.\nFor our CSR test, the test hypotheses will be:\n\n\\(H_0\\) - Road accidents in 2022 are randomly distributed along the BMR road network\n\\(H_1\\) - Road accidents in 2022 are not randomly distributed along the BMR road network\n\nThe code chunk below runs these two functions for testing CSR using kfunctions() from the spNetwork package. We specify a range of 0m (start) and 2km (end) to evaluate the function. We also specify 50 Monte Carlo simulations (nsim + 1) to draw the envelope. A confidence interval (1 - conf_int) of 95%, and intervals of 200m for the steps and the donut width. We also use an agg argument to allow consolidation of events. (as the function cannot work with duplicate points)\n\nkfun_bmracc &lt;- kfunctions(bmr_network_v2, \n                             filter(bmracc_jitt, Year == 2022),\n                             start = 0, \n                             end = 2000, \n                             step = 200, \n                             width = 200, \n                             nsim = 49, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05,\n                             agg = 100)\n\nWe can output the K-function by calling on the plotk field, and the G-function by calling on the plotg field of the resulting object\n\nkfun_bmracc$plotg\n\n\n\n\n\n\n\nkfun_bmracc$plotk\n\n\n\n\n\n\n\n\nThe envelop depicts a 95% confidence level CSR interval for each function. Both functions do not support th hypothesis of CSR except for very short intervals. (where the blue lines fall within the envelope) The K-function supports the view on clustering from a distance of around 300m-1.9km, while the G-function supports this from around 250-650m. The G-function supports a view on regular distribution beyond 750m. While these differ in the details, both tests do not support CSR for the distribution of accidents in 2022."
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-month",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.5-distribution-of-accidents-by-month",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.5 Distribution of Accidents by Month",
    "text": "E.5 Distribution of Accidents by Month\nWe can then look into the distribution of accidents across months. Seasonal events including holidays, climate, etc can be linked to the months, so it is good to see if there are months that deviate from most of the others.\nTo do this, we need to generate the nkde for each month similar to the approach for the yearly analysis.\n\nfor (i in 1:12) {\n   densities &lt;- nkde(bmr_network_v2, \n                  events = filter(bmracc, MonthNum == i),\n                  w = rep(1, nrow(filter(bmracc, MonthNum == i))),\n                  samples = samples_v2,\n                  kernel_name = \"quartic\",\n                  bw = 500, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 2,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n   if (i &gt; 9){\n     lixels_v2[[paste(\"density_M\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n     samples_v2[[paste(\"density_M\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n   }\n   else{\n     lixels_v2[[paste(\"density_M0\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n     samples_v2[[paste(\"density_M0\",as.character(i),sep=\"\")]] &lt;- densities * 100000\n   }\n  \n  }\n\nWe can then use the code block below to generate the map of the different months using tmap package.\n\ncolumns_to_map &lt;- c(\"density_M01\", \"density_M02\", \"density_M03\",\n                    \"density_M04\",\"density_M05\",\"density_M06\",\n                    \"density_M07\", \"density_M08\", \"density_M09\",\n                    \"density_M10\",\"density_M11\",\"density_M12\")\nmonthly = list()\nfor (col in columns_to_map)\n{\n  monthly[[col]] &lt;- tm_shape(bmr_boundary)+\n  tm_polygons(col = \"lightblue\", border.col = \"black\", lwd = 0.5)+\n  tm_shape(lixels_v2)+\n  tm_lines(col=col, palette = \"-inferno\", lwd = 1, title.col = \"Per sq km\") +\n  tm_shape(lixels_v2[lixels_v2[[col]] == 0, ]) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = paste(\"Month -\",substr(col,10,11)),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n}\n\nTo be able to view the maps clearly, we display them individually in the tabs below.\n\nJanuaryFebruaryMarchAprilMayJuneJulyAugustSeptemberOctoberNovemberDecember\n\n\n\nmonthly[[1]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[2]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[3]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[4]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[5]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[6]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[7]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[8]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[9]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[10]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[11]]\n\n\n\n\n\n\n\n\n\n\n\nmonthly[[12]]\n\n\n\n\n\n\n\n\n\n\n\nThe output shows a few hotspots arising on specific months. We mention the ones where there are highly dense segments outside Bangkok:\n\nPathum Thani - January, March, June\nSamut Sakhon - January"
  },
  {
    "objectID": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-fatal-accidents",
    "href": "Take-home/Take-home_Ex01/Take-home_Ex01.html#e.6-fatal-accidents",
    "title": "Geospatial Analysis for Public Good: A Data-driven Perspective on Road Traffic Accidents in the Bangkok Metropolitan Region",
    "section": "E.6 Fatal Accidents",
    "text": "E.6 Fatal Accidents\nAs mentioned in the earlier section, only a small 6% or 719 of the total number of accidents were fatal. While small, this is still a large number for the affected families. We expect that such accidents would have also caused more disruption compared to most of the non-fatal ones.\n\nE.6.1 Fatal Accidents by Year and by Province, Non-Network Constrained\nIf we look at the distribution of these accidents across years using the chart below, the annual number has ranged from 153-203, and 2020 and 2021 had 33% more accidents than the other years.\n\nggplot(filter(bmracc, number_of_fatalities &gt; 0), aes(x = reorder(Year, table(Year)[Year]))) +\n  geom_bar() +\n  ggtitle(\"Number of Fatal Accidents by Year\") +\n  theme_minimal() +\n  geom_text(stat='count', aes(label=..count..), vjust = +2) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 12),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nWe can then visualize the location of these using the tmap package. The code below displays a map of the location of the fatal accidents in BMR between 2019 and 2022.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(bmr_network_v2)+\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = \"BMR Fatal Road Accident Density - 2019-2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nIt looks like fatal accidents are more frequent outside Bangkok. With the exception of Nakhon Pathom, it looks like there is a larger number of accidents, in number and in density, happening in the four other provinces.\nIt is hard to judge whether or not there are more accidents in a province using the graph above because of the presence of close or overlapping dots. One approach is to use the functions from sf package to count the accidents or events that fall within each province and then also compute for a density by computing the areas of each province.\nWe first compute for the number of fatal accidents that occur in each province by using st_intersects() function from sf package. We produce this for the total number of accidents and the accidents for each year by using the code chunk below. We create a copy of the bmr_boundary object to store these values.\n\nbmr_with_fatacc &lt;- bmr_boundary %&gt;%\n  mutate('FatAcc19-22' = lengths(st_intersects(bmr_boundary,filter(bmracc, number_of_fatalities &gt; 0)))) %&gt;%\n  mutate('FatAcc19' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2019) )))) %&gt;%\n  mutate('FatAcc20' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2020) )))) %&gt;%\n  mutate('FatAcc21' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2021) )))) %&gt;%\n  mutate('FatAcc22' = lengths(st_intersects(bmr_boundary,filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2022) ))))\n\nWe then compute for the area of each province using st_area() in the code chunk below.\n\nbmr_with_fatacc$Area &lt;- st_area(bmr_with_fatacc)\n\nFinally, we can compute for the density of accidents in each province by taking the ration of the last two measures we computed. Note that we are multiplying each by 1 million to convert the units from per meter to per square kilometer.\n\nbmr_with_fatacc$FatDensAll &lt;- bmr_with_fatacc$`FatAcc19-22` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens19 &lt;- bmr_with_fatacc$`FatAcc19` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens20 &lt;- bmr_with_fatacc$`FatAcc20` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens21 &lt;- bmr_with_fatacc$`FatAcc21` / bmr_with_fatacc$Area * 1000000\nbmr_with_fatacc$FatDens22 &lt;- bmr_with_fatacc$`FatAcc22` / bmr_with_fatacc$Area * 1000000\n\nWe can now compare the occurence of fatal accidents across provinces visually. First, we can produce a choropleth map for the number of accidents and the density of accidents side by side using the code below. This is done by passing a list of arguments for the different tmap elements. In the code below, we use this on the color of the polygons, the label and the chart title.\n\ntm_shape(bmr_with_fatacc)+\n  tm_polygons(col = c('FatAcc19-22', 'FatDensAll'), style = \"equal\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = c(\"Number\", \"Per Sq Km\"))+\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = c(\"Fatal Road Accidents 19-22\",\"Fatal Road Accidents Density\"),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nThe plot shows that Pathum Thani in the northeast and Samut Prakan in the southeast have the highest number of accidents. However, if we normalize by the area, Nonthaburi in the center and Samut Prakan, still, have the highest density of fatal accidents.\nWe can use the same approach to look at the density across the different years.\n\ntm_shape(bmr_with_fatacc)+\n  tm_polygons(col = c('FatDens19', 'FatDens20', 'FatDens21', 'FatDens22'), style = \"equal\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Per Sq Km\")+\n  tm_shape(filter(bmracc, number_of_fatalities &gt; 0)) +\n  tm_dots(col=\"red\", size = 0.1) +\n  tm_layout(title = c(\"2019 Fatal Accidents\",\"2020 Fatal Accidents\",\"2021 Fatal Accidents\",\"2022 Fatal Accidents\"),\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")\n\n\n\n\n\n\n\n\nAt a province-level, Nonthaburi has been consistently the most dense with regards to fatal road accidents. In the meantime, Samut Prakan appears to have increased its density and risen in rank between 2020 and 2021.\nWe will not attempt to perform kde on this set of accidents and go straight to an nkde which considers the road network.’\n\n\nE.6.2 Fatal Accidents, Network Constrained\nThe final analyses we will perform is on the distribution of the fatal accidents along the road network. For this, we will also focus only on one year– 2022, as we recognize a shift in the hotspots over the years, at least across provinces\nTo facilitate the analysis, we create a subset of the accident dataset to only consider fatal accidents and the latest year.\n\nbmr_fatacc_2022 &lt;- filter(bmracc, (number_of_fatalities &gt; 0) & (Year == 2022) )\n\n\nbmr_fatacc_2022\n\nSimple feature collection with 159 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 610274.5 ymin: 1489002 xmax: 706582.6 ymax: 1570658\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 159 × 22\n   acc_code incident_datetime   report_datetime     province_th  province_en  \n *    &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;        &lt;chr&gt;        \n 1  5484851 2022-01-01 02:30:00 2022-01-02 02:43:00 สมุทรปราการ   Samut Prakan \n 2  5493686 2022-01-01 05:30:00 2022-01-03 10:23:00 นนทบุรี        Nonthaburi   \n 3  5493725 2022-01-01 14:00:00 2022-01-03 10:28:00 นครปฐม       Nakhon Pathom\n 4  6950193 2022-01-03 21:00:00 2022-09-21 10:53:00 นนทบุรี        Nonthaburi   \n 5  5837858 2022-01-03 23:34:00 2022-03-03 14:19:00 กรุงเทพมหานคร Bangkok      \n 6  6567040 2022-01-04 21:19:00 2022-01-05 11:27:00 สมุทรปราการ   Samut Prakan \n 7  6567055 2022-01-09 22:00:00 2022-01-11 10:56:00 ปทุมธานี       Pathum Thani \n 8  5861456 2022-01-11 18:00:00 2022-03-07 11:28:00 สมุทรปราการ   Samut Prakan \n 9  5579080 2022-01-15 00:10:00 2022-01-15 09:11:00 กรุงเทพมหานคร Bangkok      \n10  5843213 2022-01-16 11:30:00 2022-03-04 10:48:00 ปทุมธานี       Pathum Thani \n# ℹ 149 more rows\n# ℹ 17 more variables: agency &lt;chr&gt;, route &lt;chr&gt;, vehicle_type &lt;chr&gt;,\n#   presumed_cause &lt;chr&gt;, accident_type &lt;chr&gt;,\n#   number_of_vehicles_involved &lt;dbl&gt;, number_of_fatalities &lt;dbl&gt;,\n#   number_of_injuries &lt;dbl&gt;, weather_condition &lt;chr&gt;, road_description &lt;chr&gt;,\n#   slope_description &lt;chr&gt;, geometry &lt;POINT [m]&gt;, Year &lt;dbl&gt;, MonthNum &lt;dbl&gt;,\n#   Month &lt;ord&gt;, DayOfWeek &lt;ord&gt;, fatal &lt;lgl&gt;\n\n\nThere are 159 fatal accidents in the BMR in 2022, and in the new dataset– consistent with the summary in the previous section.\nWe then use the following code chunk to compute for the network-constrained KDE of the 2022 fatal accidents in our network. Note that in the code, we create a duplicate lixels and samples object to keep the original one unchanged. We take this opportunity to widen the computation range by increasing the bandwidth or bw parameter and the max_depth parameter.\n\nsamples_22 &lt;- samples_v2\nlixels_22 &lt;- lixels_v2\n\ndensities &lt;- nkde(bmr_network_v2, \n                  events = bmr_fatacc_2022,\n                  w = rep(1, nrow(bmr_fatacc_2022)),\n                  samples = samples_22,\n                  kernel_name = \"quartic\",\n                  bw = 10000, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(10,10), \n                  max_depth = 20,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nsamples_22$density_22 &lt;- densities*1000000\nlixels_22$density_22 &lt;- densities*1000000\n\nWe can then use the following codeblock to visualize the nKDE using tmap package.\n\ntm_shape(bmr_boundary)+\n  tm_polygons(col = \"ADM1_EN\", palette = \"Blues\", border.col = \"black\", lwd = 0.5, title = \"Province\")+\n  tm_shape(lixels_22)+\n  tm_lines(col=\"density_22\", palette = \"-inferno\", lwd = 2, title.col = \"Per sq km\") +\n  tm_shape(filter(lixels_22, density_22 == 0)) +\n  tm_lines(col=\"grey\", lwd = 2) +\n  tm_layout(title = \"BMR Fatal Road Accident Density - 2022\",\n            title.position = c(\"left\", \"top\"),\n            legend.position = c(\"left\", \"bottom\"),\n            bg.color = \"grey90\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we learn how to compute and interpret global measures of spatial autocorrelation (GMSA) using the spdep package.\nThis exercise is based on Chapter 9 of Dr Kam’s online book which can be accessed here."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#analytical-question",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#analytical-question",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Analytical Question",
    "text": "Analytical Question\nOne of the main development objective in spatial policy is for local governments and planners to ensure that there is equal distribution of development in the province. We then need to apply the appropriate spatial methods to verify if there is indeed even distribution of wealth geographically. If there is uneven distribution, then the next step is to identify if and where clusters are happening.\nWe continue studying the Hunan Province in China and focus on GDP per capita as the key indicator of development."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-sources",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-sources",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Sources",
    "text": "Data Sources\nData for this exercise are based on the Hunan county coming from two files:\n\nHunan county boundary layer in ESRI shapefile format\nHunan local development indicators for 2012 stored in a csv file"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#installing-and-launching-r-packages",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#installing-and-launching-r-packages",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Installing and launching R packages",
    "text": "Installing and launching R packages\nThis exercise will make use of five R packages: sf, tidyverse, tmap, and spdep.\n\nsf - for importing, managing and processing vector-based geospatial data\ntidyverse - collection of packages for performing data importation, wrangling and visualization\ntmap - for plotting cartographic quality maps\nspdep - functions to create spatial weights, autocorrelation statistics\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. It installs them first if they are not. It then loads them into R.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\nWe also define a random seed value for repeatability of any simulation results.\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-loading",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-loading",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Loading",
    "text": "Data Loading\nThe code chunk below uses st_read() of the sf package to load the Hunan shapefile into an R object.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\drkrodriguez\\ISSS626-GAA\\Hands-on\\Hands-On_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe then use the code chunk below to load the csv file with the indicators into R using read_csv()\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-preparation",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#data-preparation",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe then update the first object, which is of sf type, by adding in the economic indicators from the second object using left_join() as in the code chunk below\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\nIf we check the contents of hunan using head(), we see that it now includes a column GDDPPC\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-the-development-indicator",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-the-development-indicator",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of the Development Indicator",
    "text": "Visualization of the Development Indicator\nBefore we move to the main analyses, we can visualize the distribution of GCPPC by using tmap package. We present these uas two maps using classes of equal intervals and equal quantiles.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#computing-contiguity-spatial-weights",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#computing-contiguity-spatial-weights",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nPrior to computing GMSA’s, we need t construct spatial weights of the study area. Spatial weights are used to define the neighborhood relationship between units. (i.e., neighbors or adjacent units)\nThe code chunk below uses poly2nb() of the spdep package to compute contiguity weight matrices for the study area. The function builds a neighbor list based on regions with shared boundaries. The queen argument takes TRUE (default) or FALSE as options. This instructs the function if Queen criteria should be used in defining neighbors. For the code below, we use the Queen criteria to build the contiguity matrix\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output shows that there are 88 units in the hunan dataset, The most connected unit has 11 neighbors and two units have only one neighbor."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#row-standardised-weights-matrix",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#row-standardised-weights-matrix",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Row-standardised weights matrix",
    "text": "Row-standardised weights matrix\nThe next step is assigning weights to each neighbor. For our case, we assign equal weight (using style=\"W\") to each neighboring polygon. This assigns the fraction 1/n, where n is the number of neighbors a unit has, as the weight of each unit’s neighbor. The drawback of this approach is that polygons in the edge of the study area will base their value on a smaller number of neighbors. This means that we may be potentially over- or under-estimating the true nature of spatial autocorrelation. The alternative more robust style=\"B\" can address this.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-test",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-test",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Moran’s I test",
    "text": "Moran’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of the spdep package\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nThe p-value does not support CSR for the GDPPC, while a positive statistic indicates signs of clustering. If the statistic value were below 0, or negative, then it would indicate signs of dispersion."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-morans-i",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-morans-i",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Monte Carlo Simulation for Moran’s I",
    "text": "Monte Carlo Simulation for Moran’s I\nWe use the code chunk below to perform permutation test for the statistic by using moran.mc() of spdep. The nsim argument is set so that 1000 simulations will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-morans-i",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-morans-i",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of Monte Carlo Simulation Results (Moran’s I)",
    "text": "Visualization of Monte Carlo Simulation Results (Moran’s I)\nIt is good practice to analyse and visualize the simulation results in more detail. We can do this by checking the values and distribution of the statistic numerically and graphically.\nWe can use the code chunk below to show individual statistics of the simulated value.\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\npaste(\"Standard Dev:\", var(bperm$res[1:999]))\n\n[1] \"Standard Dev: 0.00437157393477615\"\n\n\nWe can visualize graphically using hist() and abline() from R Graphics.\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-test",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-test",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Geary’s C test",
    "text": "Geary’s C test\nThe code chunk below uses geary.test() to perform Geary’s C test for spatial autocorrelation.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nGeary’s C test uses a different interpretation compared to Moran’s I. A statistic value less than one, as in the case above, indicates signs of clustering, while a value of greater than one indicates dispersion. The very low p-value means that any hypothesis of compete spatial randomness (with α &gt; 0.015%) is not supported by the observed data."
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-gearys-c",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#monte-carlo-simulation-for-gearys-c",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Monte Carlo Simulation for Geary’s C",
    "text": "Monte Carlo Simulation for Geary’s C\nWe use the code chunk below to perform permutation test for the statistic by using geary.mc() of spdep. The nsim argument is set so that 1000 simulations will be performed.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-gearys-c",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#visualization-of-monte-carlo-simulation-results-gearys-c",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Visualization of Monte Carlo Simulation Results (Geary’s C)",
    "text": "Visualization of Monte Carlo Simulation Results (Geary’s C)\nIt is good practice to analyse and visualize the simulation results in more detail. We can do this by checking the values and distribution of the statistic numerically and graphically.\nWe can use the code chunk below to show individual statistics of the simulated value.\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\npaste(\"Standard Dev:\", var(bperm$res[1:999]))\n\n[1] \"Standard Dev: 0.00743649278244122\"\n\n\nWe can visualize graphically using hist() and abline() from R Graphics.\n\nhist(bperm$res,\n     freq=TRUE,\n     breaks=20,\n     xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-correlogram-and-plot",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#morans-i-correlogram-and-plot",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Moran’s I Correlogram and Plot",
    "text": "Moran’s I Correlogram and Plot\nThe code chunk below uses sp.correlogram() of spdep package to compute a 6-lag (order=6) spatial correlogram of GDPPC using Moran’s I. (method=\"I\") We then plot() to produce the visualization.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\nAside from the output, we can also display the full content of the analysis using the code below. This lets us see the result for each lag in more detail.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-correlogram-and-plot",
    "href": "Hands-on/Hands-On_Ex07/Hands-On_Ex07.html#gearys-c-correlogram-and-plot",
    "title": "Global Measures of Spatial Autocorrelation",
    "section": "Geary’s C Correlogram and Plot",
    "text": "Geary’s C Correlogram and Plot\nThe code chunk below uses sp.correlogram() of spdep package to compute a 6-lag (order=6) spatial correlogram of GDPPC using Geary’s C. (method=\"C\") We then plot() to produce the visualization.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\nWe can also examine the results in more detail using the code chunk below\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  }
]